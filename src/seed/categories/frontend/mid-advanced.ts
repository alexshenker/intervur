import { Category, Level, ValidTag } from "../../../db";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const midAdvanced: QuestionForCategoryAndLevel<
    typeof Category.enum.frontend,
    typeof Level.enum["mid-advanced"]
>[] = [
    // JavaScript
    {
        text: "What are WeakMap and WeakSet and when would you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.weakmap, ValidTag.enum.weakset],
        answers: ["WeakMap and WeakSet are collections that hold weak references to their keys or values, which means they don't prevent garbage collection. The key difference from regular Map and Set is that WeakMap keys must be objects, and if there are no other references to that object, it can be garbage collected. I typically use WeakMap for storing metadata about objects without preventing those objects from being cleaned up - like caching DOM nodes or storing private data for class instances. WeakSet is useful for marking objects, like tracking which objects have been processed. The main limitation is you can't iterate over them or check their size, since the contents can change at any time due to garbage collection."],
    },
    {
        text: "What is tail call optimization?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.performance],
        answers: ["Tail call optimization is a performance feature where the JavaScript engine can reuse stack frames for function calls that are in tail position - meaning the last thing a function does is return another function call. This prevents stack overflow errors in recursive functions by keeping the call stack size constant. For example, if you have a recursive function where the final operation is calling itself, the engine can optimize away the stack frame. However, it's worth noting that TCO is part of ES6 spec but isn't widely implemented - Safari supports it but Chrome and Firefox don't. In practice, I usually convert deeply recursive functions to iterative ones or use trampolining instead of relying on TCO."],
    },
    {
        text: "What is the Temporal Dead Zone?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.hoisting, ValidTag.enum.scope],
        answers: ["The Temporal Dead Zone is the period between entering a scope and the actual declaration of a let or const variable, where the variable exists but can't be accessed. If you try to access the variable during this time, you'll get a ReferenceError. This is different from var, which gets hoisted and initialized with undefined. For example, if you have a let statement in the middle of a function, you can't access that variable anywhere before that line, even though the variable is technically hoisted. This behavior actually helps catch bugs by making it clear when you're trying to use a variable before it's ready. It's called temporal because it's about the time between entering the scope and the declaration, not the physical location in the code."],
    },

    // TypeScript
    {
        text: "What are conditional types and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["conditional-types"], ValidTag.enum.types],
        answers: ["Conditional types let you express type logic using a ternary-like syntax - basically 'if this type extends that type, then return this, else return that'. The syntax is T extends U ? X : Y. I use them all the time for creating flexible, type-safe utilities. For example, you might create a type that returns different shapes based on the input type, or extract certain properties based on conditions. They're particularly powerful when combined with infer for extracting types from generic parameters. A common use case is creating a type that unwraps Promise types, where you'd check if a type extends Promise and extract the inner type. They enable really sophisticated type manipulation that would be impossible with simpler type features."],
    },
    {
        text: "What are mapped types?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["mapped-types"], ValidTag.enum.types],
        answers: ["Mapped types let you transform one type into another by iterating over its properties. The syntax looks like { [K in keyof T]: SomeType }. They're incredibly useful for creating variations of existing types - like making all properties optional, readonly, or nullable. Built-in utility types like Partial, Required, and Pick are all implemented using mapped types. I often use them to create derived types from API responses or to transform database models into form types. You can also use key remapping with the 'as' clause to rename properties or filter them out. They're one of TypeScript's most powerful features for reducing type duplication and maintaining type safety when you need similar but slightly different type shapes."],
    },
    {
        text: "What is the infer keyword and how do you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["conditional-types"]],
        answers: ["The infer keyword lets you extract and store types within conditional types for later use. It's like pattern matching for types - you're saying 'if this type matches this pattern, extract this part and let me use it'. The classic example is extracting the return type of a function: if T extends a function that returns infer R, then R is the return type. You can only use infer within the extends clause of a conditional type. I commonly use it for unwrapping types - like getting the element type from an array, the resolved value from a Promise, or parameters from a function signature. It's essential for building advanced utility types that need to extract and manipulate parts of complex type structures."],
    },
    {
        text: "What are template literal types?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Template literal types let you build string types by combining literal strings with other string types, using the same backtick syntax as template literals in JavaScript. They're incredibly powerful for creating type-safe string patterns. For example, you can create a type that represents CSS properties like 'padding-top' or 'margin-left' by combining 'padding' | 'margin' with 'top' | 'left'. They support all the manipulation you'd expect - concatenation, unions, and even intrinsic string manipulation with Uppercase, Lowercase, Capitalize, and Uncapitalize. I often use them for things like event names, CSS class names, or API endpoints where you want type safety for string patterns. They're especially useful when combined with mapped types to generate variations of string-based keys."],
    },
    {
        text: "What is type widening and how do you prevent it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Type widening is when TypeScript automatically expands a specific type to a more general one. For example, when you declare a variable with let and initialize it with a string literal like 'hello', TypeScript widens the type from the literal 'hello' to the general type string. This is usually what you want for mutable variables, but it can cause issues when you need precise types. You can prevent widening in several ways - using const instead of let keeps literal types, adding 'as const' to the end of an expression makes everything deeply readonly and literal, or you can explicitly type annotate the variable. I commonly use 'as const' when defining configuration objects or lookup tables where I want the exact literal values preserved in the type system."],
    },
    {
        text: "What are function overloads in TypeScript?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Function overloads let you define multiple type signatures for a single function implementation, which is useful when a function behaves differently based on its input types. You write multiple function signature declarations above the actual implementation. For example, you might have a function that takes either a string or a number and returns different types accordingly. The key thing to understand is that the overload signatures are what callers see - they define the public API - while the implementation signature must be general enough to handle all the overload cases. TypeScript matches overloads from top to bottom, so you should order them from most specific to most general. I use overloads when the relationship between parameters and return types is complex and can't be easily expressed with union types or generics alone."],
    },
    {
        text: "What is the satisfies operator and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["The satisfies operator, introduced in TypeScript 4.9, lets you validate that a value matches a type without actually changing the inferred type of that value. It's like a type assertion but in reverse - instead of forcing the type, you're checking that what you have satisfies certain constraints while keeping the more specific type. This is super useful when you want to ensure an object has certain properties but still want to maintain literal types or more specific shapes. For example, if you have a color palette object, you can use satisfies to ensure all values are valid colors while keeping the specific color strings as literal types. The main benefit over type annotations is that satisfies preserves the narrowest possible type, giving you better autocomplete and type checking."],
    },
    {
        text: "What are branded types and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Branded types, also called nominal typing or opaque types, let you create distinct types from the same underlying primitive type so they can't be accidentally mixed up. You implement them by adding a phantom property that only exists at compile time. For example, you might have UserId and ProductId both as numbers, but you don't want to accidentally pass a ProductId where a UserId is expected. You create a brand by intersecting the primitive type with an object containing a unique symbol or literal type. The key is that this brand property is never actually present at runtime - it's purely for TypeScript's type checking. I use branded types for things like IDs, validated strings, or measurements where mixing them up would be a logical error even though they're the same underlying type."],
    },

    // CSS
    {
        text: "What are the tradeoffs between CSS Modules vs Tailwind vs CSS-in-JS?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum["css-modules"], ValidTag.enum.tailwind, ValidTag.enum["css-in-js"]],
        answers: ["Each approach has distinct tradeoffs. CSS Modules gives you scoped CSS with familiar syntax and great caching since styles are static files, but you still write and maintain separate CSS files. Tailwind offers incredible developer velocity with utility classes and built-in consistency, but can lead to verbose className strings and has a learning curve for the utility names. CSS-in-JS like styled-components gives you dynamic styling with JavaScript, automatic critical CSS, and true component encapsulation, but adds runtime overhead and can cause styling to block rendering. I generally prefer Tailwind for most projects because of the speed and consistency, CSS Modules when I need traditional CSS workflow or don't want the Tailwind bundle, and CSS-in-JS when I need heavily dynamic styles based on props or themes. The choice really depends on your team's preferences and the project's requirements."],
    },
    {
        text: "How do you handle dark mode in Tailwind?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum.css],
        answers: ["Tailwind has built-in dark mode support that you enable in your config file. You can use either 'media' strategy which respects the user's system preferences, or 'class' strategy which lets you manually toggle dark mode with a class on a parent element. With the class strategy, you add a 'dark' class to your html or body element, then use the dark: variant prefix on any utility class - like 'dark:bg-gray-900' or 'dark:text-white'. I usually prefer the class strategy because it gives users control and lets you persist their preference. You typically store the preference in localStorage and sync it with the class on mount. The dark: variant works with all Tailwind utilities, so you can completely customize your dark mode styling. It's really elegant once you get used to the pattern."],
    },
    {
        text: "What are Tailwind plugins and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind],
        answers: ["Tailwind plugins let you extend Tailwind's functionality by adding new utilities, components, variants, or even modifying the default theme. You register them in your tailwind.config.js file's plugins array. Plugins are JavaScript functions that receive the plugin API, which gives you access to methods like addUtilities, addComponents, and addVariant. For example, you might create a plugin to add custom utilities for text shadows or gradients that aren't in the base framework. There are also official plugins like @tailwindcss/forms and @tailwindcss/typography that provide pre-built utilities for common use cases. I often use plugins to add project-specific utilities or to integrate design system tokens, which helps keep the utility classes consistent across the team while avoiding one-off custom CSS."],
    },
    {
        text: "How do you handle animations in Tailwind?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum.animations],
        answers: ["Tailwind includes some default animations like animate-spin, animate-pulse, and animate-bounce that you can use right away. For custom animations, you extend the theme in your config file by adding keyframes and animation utilities. You define the keyframes in theme.extend.keyframes and then reference them in theme.extend.animation. For more complex animations or transitions, you can combine Tailwind's transition utilities with hover or focus states. I often use the transition-all or transition-transform utilities along with duration and ease classes. For really complex animations, I might reach for a library like Framer Motion and use Tailwind just for the basic styling. The key is that Tailwind handles the simple cases really well with utilities, but doesn't try to replace JavaScript animation libraries for complex choreography."],
    },
    {
        text: "What are CSS custom properties and how do they differ from Sass variables?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.sass],
        answers: ["CSS custom properties, or CSS variables, are native CSS features that cascade and can be changed at runtime, while Sass variables are compile-time only and get replaced during the build. The key difference is that custom properties are part of the DOM and can be manipulated with JavaScript, inherited through the cascade, and scoped to elements. For example, you can change a custom property value in a media query or with JavaScript, and all references update automatically. Sass variables are more powerful for complex preprocessing logic and calculations, but they're static once compiled. I use custom properties for theming, responsive values, and anything that needs to change dynamically. For complex calculations or build-time logic, Sass variables are still useful. In modern projects, I often use both - Sass for build-time organization and custom properties for runtime flexibility."],
    },

    // React Hooks Advanced
    {
        text: "What's the difference between useCallback and useMemo? When would you use each?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.performance, ValidTag.enum.memoization, ValidTag.enum.useCallback, ValidTag.enum.useMemo],
        answers: ["Both are memoization hooks, but useCallback memoizes a function itself while useMemo memoizes the result of calling a function. useCallback returns the same function reference between renders unless dependencies change, which is useful for preventing child components from re-rendering when you pass callbacks as props. useMemo caches the computed value and only recalculates when dependencies change, which is useful for expensive calculations. A common mistake is overusing these - you should only use useCallback when passing functions to memoized child components or when the function is used as a dependency in other hooks. useMemo is good for expensive computations like filtering or sorting large arrays. The performance cost of the memoization itself can outweigh the benefits if you're not careful, so I only reach for them when I've identified an actual performance issue."],
    },
    {
        text: "What is useImperativeHandle and when is it appropriate?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["useImperativeHandle lets you customize the instance value that's exposed to parent components when using forwardRef. Instead of exposing the entire DOM element, you can expose only specific methods or values. For example, if you have a custom input component, you might expose only a focus method and hide everything else. You use it together with forwardRef to control what the ref actually gives access to. This is useful for component libraries where you want to provide a controlled API surface rather than direct DOM access. However, it breaks the typical React data flow, so I only use it when building reusable components that need imperative APIs - like form inputs, modals, or video players - where parent components need to trigger actions that don't fit naturally into props."],
    },
    {
        text: "What is useDeferredValue and how does it differ from debouncing?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.performance],
        answers: ["useDeferredValue is a React 18 hook that lets you defer updating a part of the UI, keeping a stale value during urgent updates and updating it later. Unlike debouncing which delays the update by a fixed time, useDeferredValue is adaptive - React decides when to update based on available resources and tries to update as soon as possible without blocking urgent updates. For example, if you have a search input and an expensive list, you can defer the list's value so typing stays responsive. The key difference from debouncing is that there's no fixed delay, it integrates with concurrent rendering, and React can interrupt the deferred update if new input comes in. I use useDeferredValue when I want to keep the UI responsive during expensive renders, while debouncing is better for reducing the frequency of operations like API calls."],
    },
    {
        text: "What is useTransition and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum["concurrent-mode"]],
        answers: ["useTransition is a React 18 hook that lets you mark certain state updates as non-urgent transitions, allowing React to keep the UI responsive by interrupting those updates if needed. It returns an isPending flag and a startTransition function. When you wrap state updates in startTransition, React knows it can interrupt them to handle more urgent updates like user input. This is perfect for situations where you're updating something expensive in response to user input, like filtering a large list or changing tabs. The isPending flag lets you show loading states during the transition. The main difference from useDeferredValue is that useTransition marks the state update itself as low priority, while useDeferredValue defers consuming a value. I use it whenever I have state updates that cause expensive renders and I want to keep the UI feeling snappy."],
    },
    {
        text: "What is useSyncExternalStore and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["useSyncExternalStore is a hook for safely subscribing to external data sources in a way that's compatible with concurrent rendering. It solves tearing issues where different parts of the UI might see inconsistent data during concurrent renders. You provide a subscribe function and a getSnapshot function that returns the current value. This is mainly needed when integrating with external stores like Redux, Zustand, or browser APIs like window.matchMedia. Most developers won't need to use this directly because state management libraries handle it internally. However, if you're building a custom store or subscribing to external data sources, this is the safe way to do it. Before React 18, you could get away with simpler subscription patterns, but concurrent rendering requires this approach to prevent visual inconsistencies."],
    },

    // React Advanced
    {
        text: "What is React's reconciliation algorithm and how does it decide what to re-render?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.reconciliation, ValidTag.enum["virtual-dom"], ValidTag.enum.performance],
        answers: ["React's reconciliation is the diffing algorithm that compares the new virtual DOM tree with the previous one to determine what actually needs to update in the real DOM. React makes several assumptions to optimize this - elements of different types will produce different trees, and you can hint which children are stable using keys. When comparing trees, if the root elements are different types, React tears down the old tree and builds a new one. For same-type elements, React keeps the same DOM node and only updates changed attributes. Component updates trigger a re-render of that component and its children by default. Keys are crucial for lists because they help React identify which items changed, were added, or removed. The algorithm is O(n) instead of O(nÂ³) because of these heuristics. Understanding this helps you optimize by avoiding unnecessary type changes and using keys properly."],
    },
    {
        text: "What causes infinite re-render loops and how do you debug them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.debugging],
        answers: ["Infinite re-render loops typically happen when you update state during render, creating a cycle. Common causes include calling setState directly in the component body, or having a useEffect with missing dependencies that updates state and triggers itself again. Another common issue is creating new object or function references during render and using them as dependencies or props. To debug them, first check the error message - React often tells you 'too many re-renders' and points to the component. Add console.logs to see what's triggering renders. Check your useEffect dependencies - the eslint plugin for hooks is invaluable here. Look for setState calls outside of event handlers or effects. Use React DevTools profiler to see what's causing renders. The fix is usually adding proper dependencies, moving state updates to event handlers, or memoizing values with useMemo or useCallback."],
    },
    {
        text: "What is StrictMode and why does it cause double-rendering?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["StrictMode is a development-only tool that helps find potential problems in your application by intentionally double-invoking certain functions like function components, useState initializers, and useEffect callbacks. This double-rendering only happens in development and helps catch side effects that aren't properly cleaned up or impure functions that might cause bugs. The idea is that if your component renders twice with the same props and produces different results, you probably have an impure component that's relying on external state incorrectly. It also highlights deprecated APIs and unsafe lifecycle methods. The double-rendering can be confusing at first, but it's actually helping you write more resilient code. Any side effects you see duplicated should be moved to useEffect or event handlers. In production builds, StrictMode does nothing, so there's no performance impact."],
    },
    {
        text: "What are portals and when would you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.portals],
        answers: ["Portals let you render children into a DOM node that exists outside the hierarchy of the parent component. You create them with ReactDOM.createPortal, passing the JSX and the target DOM node. This is super useful for modals, tooltips, dropdowns, or popovers where you need the component to visually break out of the parent container, especially when dealing with overflow: hidden or z-index stacking contexts. Even though the component renders outside the parent DOM hierarchy, it still behaves like a normal React child - events bubble up through the React tree, not the DOM tree, and context still works. I use portals whenever I need something to appear above everything else or escape CSS constraints. The classic use case is a modal that needs to render at the document body level to avoid z-index issues, but still receive props and state from deep in the component tree."],
    },
    {
        text: "What is React.memo and when does it actually improve performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.performance, ValidTag.enum.memoization],
        answers: ["React.memo is a higher-order component that memoizes the component and only re-renders when props change. It does a shallow comparison of props by default, or you can provide a custom comparison function. However, it only improves performance in specific scenarios - when the component renders often with the same props, when the component is expensive to render, or when it's a pure component that doesn't depend on context or state. It won't help if props are new objects or functions every render, which is why you often need to combine it with useCallback and useMemo. I don't use React.memo by default everywhere - that actually adds overhead. I only add it after profiling and identifying components that re-render unnecessarily. It's most useful for components deep in the tree that receive stable props from higher up, or for items in large lists."],
    },
    {
        text: "What are render props and when would you use them over hooks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Render props is a pattern where a component takes a function as a prop and calls it to determine what to render, allowing you to share code between components. Before hooks, this was a primary way to share stateful logic. The component handles the logic and state, then calls the render prop function with that data. While hooks have mostly replaced render props for sharing logic, render props are still useful in specific cases - when you need more control over rendering, when building library components that need maximum flexibility, or when the component needs to render multiple times with different data. For example, a virtualized list might use render props to let consumers control how each item renders. Hooks are generally simpler and more composable, but render props can be more explicit about data flow and give consumers fine-grained control over rendering."],
    },
    {
        text: "What are compound components and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["design-patterns"]],
        answers: ["Compound components is a pattern where multiple components work together to form a complete UI, sharing implicit state through context. Think of HTML select and option elements - they work together as a unit. You implement this by creating a parent component that manages state and provides it via context, and child components that consume that context. For example, a Tabs component might have Tabs.List, Tabs.Tab, and Tabs.Panel subcomponents that all share selection state. The benefit is a flexible, expressive API - users can compose the components however they want while the internal state stays synchronized. Libraries like Radix UI and Headless UI use this pattern extensively. I implement compound components when building complex UI elements that need flexibility in structure but shared behavior. The key is using context to share state implicitly rather than requiring users to wire everything together with props."],
    },
    {
        text: "What is the children prop and how do you manipulate it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["The children prop is a special prop in React that represents the content between component's opening and closing tags. It can be anything - a string, an element, an array of elements, or even a function. React provides React.Children utilities to work with children safely since children's structure can vary. React.Children.map lets you iterate and transform children, React.Children.count counts them, React.Children.only ensures there's exactly one child, and React.Children.toArray converts children to a flat array with keys. You can also clone children with React.cloneElement to add props. Common use cases include wrapping children with extra elements, filtering certain children, or injecting props into children. However, I try to avoid heavy children manipulation because it can make components harder to understand. When I need complex composition, I often use render props or compound components instead."],
    },
    {
        text: "What is React.lazy and how do you use it with Suspense?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.suspense, ValidTag.enum["lazy-loading"], ValidTag.enum.performance],
        answers: ["React.lazy lets you dynamically import components, enabling code splitting so you only load components when they're needed. You wrap a dynamic import in React.lazy, and it returns a component that can be rendered. You must wrap lazy components in a Suspense boundary that provides a fallback UI while the component loads. For example, React.lazy(() => import('./HeavyComponent')) will create a separate bundle for that component. Suspense shows the fallback until the component loads, then renders the actual component. This is great for reducing initial bundle size, especially for routes or features that aren't immediately needed. I use it for route-based code splitting and for heavy components like charts or editors that not all users will need. The key is finding the right granularity - too much splitting can actually hurt performance with too many network requests."],
    },
    {
        text: "What is forwardRef and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["forwardRef is a React API that lets function components accept a ref and forward it to a child element or component. Normally, function components can't receive refs because they're not instances. forwardRef solves this by wrapping your component and giving it a second parameter for the ref. This is essential when building reusable component libraries where consumers need direct access to DOM elements - like focusing an input, measuring an element, or integrating with third-party libraries. For example, a custom Input component would use forwardRef so parent components can call .focus() on it. You often use it with useImperativeHandle to control exactly what the ref exposes. I use forwardRef whenever I'm building a component that wraps a native element and consumers might need DOM access, or when building component libraries where ref forwarding is expected."],
    },
    {
        text: "What are custom hooks and what makes a good custom hook?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum["custom-hooks"]],
        answers: ["Custom hooks are JavaScript functions that start with 'use' and can call other hooks, letting you extract and reuse stateful logic across components. A good custom hook has a single, clear purpose and returns exactly what consumers need - nothing more, nothing less. It should be composable, follow the rules of hooks, and have a clear API. Good examples include useLocalStorage for syncing state with localStorage, useDebounce for debouncing values, or useMediaQuery for responsive logic. A custom hook should abstract complexity while remaining flexible. I avoid hooks that are too specific to one component or that return too much - if consumers only use half of what it returns, it's probably doing too much. The best custom hooks feel obvious in hindsight and make the consuming code cleaner and more readable. They should handle their own cleanup and edge cases so consumers don't have to worry about implementation details."],
    },
    {
        text: "What is the difference between optimistic and pessimistic UI updates?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Optimistic UI updates assume the operation will succeed and update the UI immediately before getting server confirmation, while pessimistic updates wait for the server response before updating. Optimistic updates make the app feel instant and responsive, but you need to handle rollback if the operation fails. Pessimistic updates are safer and simpler since you only update on success, but they feel slower to users. For example, with an optimistic like button, you'd increment the count immediately and show the filled heart, then rollback if the request fails. With pessimistic, you'd wait for the server, leaving the UI unchanged until success. I use optimistic updates for actions that rarely fail and have clear rollback states - like likes, simple form submissions, or reordering items. For critical operations like payments or deletes, I use pessimistic updates. Libraries like TanStack Query make optimistic updates easier with built-in rollback support."],
    },
    {
        text: "What is hydration and what causes hydration mismatches?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.ssr, ValidTag.enum.nextjs],
        answers: ["Hydration is when React attaches event listeners and makes the server-rendered HTML interactive on the client. React expects the client-rendered output to match the server-rendered HTML exactly. Hydration mismatches occur when they differ, causing React to warn or even re-render from scratch. Common causes include using browser-only APIs like window during render, different data between server and client, random values or timestamps, or content that depends on client state like localStorage. Third-party scripts that modify the DOM can also cause mismatches. To fix them, use useEffect for browser-only code, ensure data is consistent, use suppressHydrationWarning for intentionally different content like timestamps, or use two-pass rendering where you render a placeholder on the server and the real content after hydration. Hydration errors can hurt performance since React might have to throw away the server HTML and re-render everything."],
    },
    {
        text: "What are concurrent features in React 18?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["concurrent-mode"]],
        answers: ["Concurrent features in React 18 allow React to work on multiple versions of the UI at the same time, pausing and resuming work as needed. The key features are useTransition for marking updates as non-urgent, useDeferredValue for deferring less important updates, Suspense for loading states, and automatic batching. Unlike the old synchronous rendering where React had to finish what it started, concurrent rendering can interrupt work to handle higher priority updates. This makes apps feel more responsive because urgent updates like typing or clicking aren't blocked by expensive renders. The mental model is that React can keep a low-priority render in memory while handling a high-priority update, then resume or discard the low-priority work. You opt into concurrent features by using createRoot instead of render and using the new hooks. It's backward compatible - existing code works fine, you just don't get the benefits until you adopt the new APIs."],
    },
    {
        text: "What is automatic batching in React 18?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.performance],
        answers: ["Automatic batching is a React 18 feature that groups multiple state updates into a single re-render for better performance. Before React 18, batching only happened in event handlers, but now it happens everywhere - in promises, setTimeout, native event handlers, and any other context. This means if you have multiple setState calls in an async function or callback, React will batch them automatically instead of re-rendering for each one. For example, if you fetch data and update multiple pieces of state, React batches those updates into one render. The performance benefit is significant since you avoid unnecessary renders. In rare cases where you need synchronous updates, you can use ReactDOM.flushSync to opt out of batching. I generally don't have to think about this - it just makes React faster by default. The only time you might notice is if you had code relying on the old synchronous behavior, which is rare."],
    },

    // State Management
    {
        text: "When would you choose Redux vs React Context vs Zustand vs TanStack Query?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum.zustand, ValidTag.enum["context-api"], ValidTag.enum["tanstack-query"], ValidTag.enum.react],
        answers: ["Each has different use cases. TanStack Query is specifically for server state - data fetching, caching, and synchronization - so I reach for it first when dealing with API data. React Context is great for simple state that doesn't change often, like themes or auth, but causes re-renders of all consumers. Zustand is my go-to for complex client state - it's simpler than Redux with less boilerplate, good performance, and works outside React. Redux with Redux Toolkit is best for very large apps that need time-travel debugging, strict patterns, or have complex state interactions that benefit from the Redux DevTools. I typically use TanStack Query for all server data, Zustand for global client state like UI preferences or app-wide modals, Context for dependency injection or truly infrequent changes, and Redux only when specifically needed. Often I combine them - TanStack Query for server state and Zustand for everything else works really well."],
    },
    {
        text: "What are Redux Toolkit slices and how do they simplify Redux?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum["redux-toolkit"]],
        answers: ["Redux Toolkit slices are a way to define a piece of Redux state along with its reducers and actions in one place using createSlice. Instead of writing action types, action creators, and reducers separately, you define them together with reducer functions that can 'mutate' state directly - RTK uses Immer under the hood to make that safe. For example, you create a slice with a name, initial state, and reducers object, and it automatically generates action creators and action types. This dramatically reduces boilerplate and makes Redux code much more maintainable. Slices also encourage organizing your state by feature rather than by type of code. Redux Toolkit also includes createAsyncThunk for async logic and configureStore that sets up good defaults. I find Redux Toolkit makes Redux actually pleasant to use, whereas classic Redux had so much ceremony it was often not worth it."],
    },
    {
        text: "What is Redux middleware and how does Redux Thunk differ from Redux Saga?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum["redux-toolkit"]],
        answers: ["Redux middleware intercepts actions before they reach reducers, letting you add side effects, logging, async logic, or modify actions. Redux Thunk is the simplest middleware - it lets action creators return functions instead of actions, and those functions receive dispatch and getState. It's great for simple async logic like API calls. Redux Saga uses generator functions and an effects system to handle complex async flows. Sagas are more powerful for things like debouncing, retries, race conditions, or coordinating multiple actions, but they have a steeper learning curve. Thunk is imperative - you write normal async/await code. Saga is declarative - you yield effects that describe what should happen. I use Thunk for most cases since it's simpler and Redux Toolkit includes it by default. Saga makes sense for complex workflows like multi-step forms or when you need to cancel operations, but that's rare in my experience."],
    },
    {
        text: "What is the difference between normalized and denormalized state?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Normalized state stores entities by ID in a flat structure, avoiding duplication and making updates easier. Denormalized state nests data naturally, matching the API response structure. For example, normalized state might store users and posts separately in objects keyed by ID, while denormalized keeps posts nested within users. Normalization prevents inconsistencies when the same entity appears in multiple places - you update it once and it's updated everywhere. It also makes it easier to update, delete, or find specific entities. The tradeoff is that you need to do more work to select related data, often using selectors. Denormalized state is simpler to work with initially but can lead to bugs when you update one copy and forget others. I normalize state when entities are referenced in multiple places or when I need to update them frequently. For read-heavy, simple data structures, denormalized is fine. Redux Toolkit has createEntityAdapter to help with normalization."],
    },
    {
        text: "What are selectors and why are they important for performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum.performance],
        answers: ["Selectors are functions that extract specific pieces of derived state from the store. They encapsulate state shape knowledge, making it easier to refactor state structure. More importantly, memoized selectors with libraries like Reselect prevent unnecessary recalculations and re-renders. A memoized selector only recomputes when its inputs change, so if you have an expensive transformation like filtering or sorting a large array, it won't run on every render. Without memoization, you'd recreate that computed value every time, potentially causing performance issues. Selectors also let you keep your state minimal by deriving values rather than storing them. For example, instead of storing filtered and sorted data, you store the raw data and selection criteria, then derive the filtered list with a selector. I use Reselect or RTK's createSelector for any computed state that's used in components, especially when the computation is expensive or when the component renders frequently."],
    },
    {
        text: "What is state colocation and why does it matter?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["State colocation means keeping state as close as possible to where it's used, rather than lifting it to a global store or high up in the tree. This improves performance because fewer components re-render when that state changes, makes code easier to understand since the state is near its usage, and makes components more reusable since they don't depend on global state shape. The general principle is to start with local state and only lift it up when multiple components actually need it. Too often developers put everything in Redux or global state when most state could be local. For example, form state, UI state like modal open/closed, or local filters should usually be component state. I only lift state when it's genuinely shared or needs to persist across unmounts. Kent C. Dodds has a great article on this - the idea is that global state has real costs in terms of complexity and performance, so you should only pay that cost when necessary."],
    },

    // TanStack Query Advanced
    {
        text: "What is optimistic updates and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Optimistic updates in TanStack Query let you update the UI immediately before the mutation completes, then rollback if it fails. You implement them using the onMutate callback to save a snapshot of current data and update the cache, onError to rollback using that snapshot, and onSettled to invalidate and refetch to ensure consistency. In onMutate, you cancel outgoing queries with queryClient.cancelQueries, save the previous value with getQueryData, and set the optimistic value with setQueryData. If the mutation fails, onError restores the previous value. This pattern makes the UI feel instant while maintaining data integrity. The key is properly handling the rollback case - users should clearly see if an action failed. I use optimistic updates for high-frequency actions like likes, votes, or toggling states where the success rate is high and the user expects immediate feedback."],
    },
    {
        text: "What is infinite queries and how do you implement pagination?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Infinite queries in TanStack Query handle paginated data where you load more as the user scrolls, like an infinite scroll feed. You use useInfiniteQuery instead of useQuery and provide a getNextPageParam function that determines the next page parameter from the last page of data. The hook returns pages as a flat array, plus fetchNextPage and hasNextPage helpers. Each page is cached independently, and TanStack Query manages loading more pages. For cursor-based pagination, getNextPageParam returns the next cursor from the response. For offset-based, it calculates the next offset. You can also implement getPreviousPageParam for bidirectional scrolling. I typically combine this with an intersection observer to trigger fetchNextPage when the user nears the bottom. The benefit over manual pagination is that TanStack Query handles caching, refetching, and state management automatically."],
    },
    {
        text: "What is prefetching and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.performance, ValidTag.enum.prefetching],
        answers: ["Prefetching in TanStack Query means loading data before it's needed so it's available instantly when requested. You use queryClient.prefetchQuery to load data without subscribing to it. Common use cases include prefetching on hover for things like tooltips or detail views, prefetching the next page of paginated data, or prefetching data for routes the user is likely to visit. The prefetched data goes into the cache with the same cache time rules, so it can become stale. Prefetching is different from initial data - prefetching makes a real request while initial data provides temporary data. I use prefetching for predictable user flows, like hovering over a product card to prefetch its details, or automatically prefetching the next page when displaying a list. The key is not to prefetch too aggressively, which wastes bandwidth and server resources."],
    },
    {
        text: "What is the difference between refetch and invalidate?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Refetch and invalidate serve different purposes in TanStack Query. Refetch immediately re-runs queries and always makes a network request, regardless of whether data is stale. You call it with query.refetch() or queryClient.refetchQueries(). Invalidate marks queries as stale so they'll refetch on the next render or when components mount, but doesn't immediately fetch unless there are active observers. You use queryClient.invalidateQueries(). Invalidate is generally better because it's more efficient - it only refetches queries that are currently being used. Refetch is useful when you need to force an immediate update, like after a user action or in response to a websocket message. I use invalidateQueries after mutations to mark related data as stale, and refetch for explicit user-triggered refreshes like a pull-to-refresh gesture."],
    },
    {
        text: "How does garbage collection work in TanStack Query?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.performance],
        answers: ["TanStack Query automatically cleans up unused query data through garbage collection. When a query has no active observers - meaning no components are using it - it becomes inactive. After the cacheTime duration (default 5 minutes), the query data is garbage collected and removed from memory. This prevents memory leaks in long-running applications. You can configure cacheTime per query - setting it to Infinity keeps data forever, while 0 removes it immediately when inactive. Stale time is separate from cache time - staleTime determines when to refetch, while cacheTime determines when to remove from memory. I usually keep the default cache time, but might increase it for data that's expensive to fetch and doesn't change often, or decrease it for sensitive data that shouldn't stay in memory."],
    },
    {
        text: "What is placeholder data vs initial data?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Placeholder data and initial data are both ways to provide data before a query loads, but they work differently. Initial data is treated as real data - it goes into the cache at the specified dataUpdatedAt timestamp and follows normal stale/cache rules. It won't refetch unless it's stale. Placeholder data is temporary fake data that's shown while the real query runs - it doesn't go into the cache and the query still runs in the background. Initial data is useful when you have real data from SSR, another query, or localStorage. Placeholder data is for showing skeleton structures or default values while loading. For example, you might use placeholder data to show empty arrays for lists while loading, or initial data to reuse data from a list view when showing a detail view. I use initialData when I have actual data to provide, and placeholderData when I just want to avoid loading states."],
    },
    {
        text: "How do you handle offline support?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["TanStack Query has built-in offline support through its caching and retry mechanisms. Queries automatically use cached data when offline, and mutations can be paused and resumed when connectivity returns. You configure this with networkMode option - 'online' only runs when online, 'always' runs regardless, and 'offlineFirst' tries the request but falls back to cache. For mutations, you can use mutation.persist() to store them in localStorage and retry when back online. The library also integrates with the browser's online/offline events to pause and resume operations. For a fully offline-first app, I typically combine TanStack Query with a service worker for offline caching, use optimistic updates for mutations, and configure appropriate cache times. The onlineManager lets you customize online detection logic. The key is deciding whether each operation should fail, queue, or use cached data when offline."],
    },

    // Next.js Advanced
    {
        text: "What is the _document.js file and when would you customize it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"]],
        answers: ["The _document.js file in Next.js lets you customize the HTML document structure that wraps your application. It only runs on the server and allows you to modify the html and body tags, add meta tags that should be on every page, or inject third-party scripts. You'd customize it to add a lang attribute to the html tag, include fonts from external CDNs, add analytics scripts that need to be in the head, or integrate with CSS-in-JS libraries that need server-side setup. Unlike _app.js which runs on every page navigation, _document only runs during the initial server render. You extend the Document class and can customize the Html, Head, Main, and NextScript components. I typically only customize _document when I need to modify the base HTML structure or add global scripts - for most styling and layout, _app.js is the better choice."],
    },
    {
        text: "How do you handle redirects and rewrites?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs],
        answers: ["Next.js handles redirects and rewrites in next.config.js. Redirects send users to a different URL and change the URL in the browser, while rewrites proxy to a different URL but keep the original URL visible. Both support pattern matching with parameters and wildcards. Redirects are useful for moved content, forcing www or https, or redirecting old URLs to new ones. Rewrites are great for API proxying to avoid CORS, implementing i18n routing, or A/B testing. You can also do redirects in getServerSideProps or middleware for dynamic logic. Permanent redirects return 308, temporary ones return 307. I use redirects for SEO-friendly URL changes and rewrites when I want to hide the implementation details, like proxying API requests through the Next.js server to hide API keys or avoid CORS issues."],
    },
    {
        text: "What is middleware in Next.js and what are valid use cases?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.middleware],
        answers: ["Next.js middleware runs before a request completes, letting you modify the response, redirect, rewrite, or set headers. It runs at the edge, before cached content or routes are matched, making it very fast. You create a middleware.js file and export a middleware function that receives a request and can return a response. Valid use cases include authentication checks, A/B testing by rewriting to different pages, internationalization routing, bot detection and blocking, adding security headers, logging and analytics, or redirecting based on geolocation. The key limitation is that middleware runs in a restricted runtime - you can't use Node.js APIs or large dependencies. I use middleware for authentication gates, setting security headers, and simple redirects. For complex logic, I'd use getServerSideProps or API routes instead since they have full Node.js access."],
    },
    {
        text: "How do you debug large Next.js bundle sizes?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.debugging, ValidTag.enum.bundling],
        answers: ["To debug bundle sizes in Next.js, I start by running next build which shows bundle sizes for each page. For deeper analysis, I use @next/bundle-analyzer which generates an interactive treemap showing what's in each bundle. Common culprits are large libraries like moment.js or lodash being fully imported instead of tree-shaken, duplicate dependencies from different versions, or forgetting to dynamically import heavy components. I check for client-side imports of server-only code, ensure next/dynamic is used for heavy components, and verify tree-shaking is working by using named imports. The webpack Bundle Analyzer helps identify which specific packages are large. I also check if moment.js can be replaced with date-fns or day.js, if lodash can be replaced with lodash-es for better tree-shaking, and if any dependencies can be removed entirely. Setting the source map to 'hidden-source-map' in production also reduces bundle size."],
    },
    {
        text: "What is the difference between fallback: false, fallback: true, and fallback: 'blocking'?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.ssg],
        answers: ["These options in getStaticPaths determine how Next.js handles paths not returned at build time. fallback: false returns 404 for any path not in the paths array - use this when you have all possible paths at build time. fallback: true immediately serves a fallback page while generating the real page in the background, then swaps it in - you need to handle the loading state with router.isFallback. fallback: 'blocking' waits to serve the page until it's fully generated, like getServerSideProps but only runs once then caches - no fallback UI needed. I use fallback: false for small, finite sets like a few marketing pages, fallback: true for large sets where I want instant navigation with a loading state, and fallback: 'blocking' when I can't show a fallback UI or when SEO is critical since the crawler gets the full page immediately."],
    },
    {
        text: "What is getInitialProps and why is it generally discouraged?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"]],
        answers: ["getInitialProps is the legacy data fetching method in Next.js that runs on both server and client side. It's discouraged because it disables automatic static optimization for the entire app - even pages without getInitialProps become server-rendered. It also has a confusing execution model where you need to check if you're on the server or client. getStaticProps and getServerSideProps are better because they're explicit about where they run, enable automatic static optimization, and have better TypeScript support. getInitialProps is only needed for older Next.js apps or when you specifically need data fetching in _app.js, though even that's often better handled differently now. If I inherit code with getInitialProps, I migrate it to getStaticProps for static pages or getServerSideProps for dynamic ones. The only time I'd use it in new code is for maintaining legacy patterns in very old Next.js versions."],
    },
    {
        text: "How do you handle authentication in the Pages Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.auth],
        answers: ["Authentication in the Pages Router can happen at different layers. For client-side protection, I check auth state in useEffect and redirect if not authenticated - this is simple but shows a flash of protected content. For server-side, I use getServerSideProps to check cookies or tokens and redirect to login if missing - this prevents any flash but runs on every request. Middleware is great for protecting multiple routes at once by checking auth before the page loads. For API routes, I verify tokens in each route handler. NextAuth.js is the most popular solution - it handles sessions with JWTs or database sessions, supports many providers, and works well with the Pages Router. I typically combine approaches: middleware for route protection, NextAuth for session management, and getServerSideProps when I need to fetch user-specific data. The key is never trusting client-side auth checks alone - always verify on the server or in API routes."],
    },
    {
        text: "How does the App Router differ from the Pages Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["pages-router"]],
        answers: ["The App Router is a fundamental redesign built on React Server Components. The key differences are file-based routing in an app directory with folders instead of files determining routes, Server Components by default instead of client components, built-in layouts and templates, streaming and Suspense support, and new data fetching with async components instead of getServerSideProps. The App Router has colocation - you can put components, tests, and styles in route folders. Loading and error states are file-based with loading.js and error.js. Metadata is handled with a new API instead of Head components. Caching is more aggressive and automatic. The mental model is different - components are server-rendered by default and you opt into client rendering with 'use client'. I use the App Router for new projects to get better performance through streaming and less JavaScript shipped to the client, but the Pages Router is still fully supported and fine for existing apps."],
    },
    {
        text: "What are Server Components vs Client Components?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["server-components"], ValidTag.enum.rsc],
        answers: ["Server Components render on the server and send HTML to the client, while Client Components are the traditional React components that hydrate and run in the browser. Server Components can access databases directly, don't add to the client bundle, and can use server-only packages. They can't use hooks, browser APIs, or event handlers. Client Components can do all of those but add to the bundle size. The default in the App Router is Server Components - you add 'use client' to make something a Client Component. You can pass Server Components as children to Client Components, which is a powerful pattern. I use Server Components for layouts, static content, and data fetching, and Client Components only when I need interactivity, hooks, or browser APIs. The benefit is less JavaScript shipped to users and faster initial page loads since more work happens on the server."],
    },
    {
        text: "What is the use client directive and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["server-components"]],
        answers: ["The 'use client' directive marks a boundary between Server and Client Components. You put it at the top of a file to indicate that component and everything it imports should be Client Components. You need it when using React hooks like useState or useEffect, when handling browser events like onClick, when using browser-only APIs like localStorage or window, when using libraries that depend on browser APIs, or when using React features like Context or createContext. You don't need it for components that just render props and children. The key insight is to push 'use client' as far down the tree as possible - make leaf interactive components Client Components while keeping layouts and wrappers as Server Components. This minimizes the client bundle. I typically create small Client Components for interactive pieces like buttons or forms, and keep everything else as Server Components by default."],
    },
    {
        text: "What are loading.js and error.js files?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"]],
        answers: ["loading.js and error.js are special files in the App Router that handle loading and error states automatically. loading.js wraps the page in a Suspense boundary and shows while the page is loading - it enables instant loading states and streaming. error.js wraps the page in an Error Boundary and catches errors during rendering, making error handling declarative. Both are scoped to their route segment and nested routes inherit them. This means you can have different loading and error UIs for different parts of your app without manually wrapping everything in Suspense or Error Boundaries. loading.js receives no props, while error.js receives the error and a reset function. I use loading.js to show skeletons while data loads, and error.js to show user-friendly error messages with retry buttons. This pattern makes error handling and loading states much more manageable than manually adding them everywhere."],
    },
    {
        text: "How does the App Router handle layouts and nested layouts?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"]],
        answers: ["Layouts in the App Router are components that wrap pages and persist across navigation. Each route segment can have a layout.js file that wraps that segment and all nested segments. Layouts are nested automatically - the root layout wraps the entire app, then each nested layout wraps its children. This is different from the Pages Router where you had to manually compose layouts. Layouts preserve state across navigation and don't re-render, making them perfect for navigation, sidebars, or shared UI. The root layout must include html and body tags and is required. You can also use template.js which is like layout but creates a new instance on navigation. I use layouts for persistent navigation and sidebars, and templates when I need components to remount on navigation, like for animations or resetting scroll position."],
    },
    {
        text: "What is the metadata API in the App Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum.seo],
        answers: ["The metadata API in the App Router provides a declarative way to set meta tags for SEO and social sharing. You export a metadata object or generateMetadata function from pages or layouts. The metadata object can include title, description, openGraph, twitter, robots, and more. Metadata merges down the tree, so you can set defaults in the root layout and override in specific pages. generateMetadata is async, letting you fetch data for dynamic metadata like blog post titles. There's also a metadataBase for resolving relative URLs, and special files like opengraph-image.js for generating images. This is much better than the Pages Router's Head component because metadata is type-safe, can be async, and Next.js handles deduplication automatically. I set global defaults in the root layout and use generateMetadata in dynamic pages to create proper meta tags from data."],
    },
    {
        text: "How does caching work in the App Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum.caching, ValidTag.enum.performance],
        answers: ["The App Router has multiple caching layers that are more aggressive than the Pages Router. Fetch requests are cached by default indefinitely - you opt out with {cache: 'no-store'} or set revalidation with {next: {revalidate: 60}}. React has a Request Memoization layer that deduplicates fetch calls during a single render. There's a Full Route Cache for static routes at build time, and a Router Cache on the client that persists across navigations. This multi-layer caching is powerful but can be confusing - data might be cached when you don't expect it. To opt out, use dynamic functions like cookies() or headers(), set dynamic = 'force-dynamic', or use no-store on fetches. For ISR-like behavior, use revalidatePath or revalidateTag in Server Actions. I find the caching great for performance but you need to understand it to avoid stale data issues. The key is being explicit about your caching strategy."],
    },

    // React Native Advanced
    {
        text: "What are native modules and when would you create one?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["Native modules are bridges between JavaScript and native platform code, letting you access platform-specific APIs that React Native doesn't expose. You create one when you need functionality not available in React Native's core - like accessing device sensors, integrating with native SDKs, optimizing performance-critical code in native languages, or accessing platform-specific features. You write native code in Java/Kotlin for Android and Objective-C/Swift for iOS, then expose methods to JavaScript. The new architecture uses TurboModules which are faster and type-safe. Creating a native module requires platform-specific knowledge and complicates builds, so I only do it when necessary. Common use cases include payment SDKs, biometric authentication, or complex native UI components. For most apps, community packages cover common needs, but custom native modules are essential when you need something unique or performance-critical."],
    },
    {
        text: "What is the new architecture in React Native (Fabric, TurboModules, JSI)?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["The new React Native architecture consists of three main parts: JSI (JavaScript Interface) which replaces the asynchronous bridge with synchronous native calls, Fabric which is the new rendering system allowing better interoperability between JavaScript and native UI, and TurboModules which lazy-load native modules and provide type safety. The old architecture used a bridge for all JS-to-native communication, creating serialization overhead and making synchronous calls impossible. The new architecture enables synchronous method calls, better startup time through lazy loading, improved type safety with codegen, and true concurrent rendering support. Fabric specifically allows for more sophisticated UI interactions and better performance since it can prioritize high-priority updates. The migration requires updating native modules and careful testing, but the performance improvements are significant. I'd adopt it in new projects now that it's stable, but migration for existing apps requires planning."],
    },
    {
        text: "What are performance considerations specific to React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.performance],
        answers: ["React Native has unique performance considerations because of the JavaScript-to-native bridge. Key issues include bridge congestion from too many messages, slow list rendering without proper virtualization, dropped frames from running heavy computations on the UI thread, and large bundle sizes affecting startup time. I optimize by using FlatList or SectionList with proper keyExtractor and item separators, avoiding inline functions and object creation in render, using the useNativeDriver option for animations to run them on the native thread, and keeping images optimized and properly sized. The new Hermes JavaScript engine improves startup time. For navigation, React Navigation performs better than alternatives. I profile with the built-in performance monitor, Flipper, or React DevTools profiler. Memory leaks from event listeners or subscriptions are also common - always clean up in useEffect. The key is understanding what crosses the bridge and minimizing those operations."],
    },
    {
        text: "How do you handle deep linking in React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.routing],
        answers: ["Deep linking in React Native involves configuring URL schemes for your app and handling incoming URLs. You set up URL schemes in native config files - Info.plist for iOS and AndroidManifest.xml for Android. For universal links (iOS) and App Links (Android), you also need to configure associated domains and host an apple-app-site-association or assetlinks.json file. React Navigation has built-in deep linking support through the linking config, which maps URL paths to screens. You handle URLs with the Linking API to detect incoming links and navigate accordingly. The tricky part is handling cold starts versus warm starts - the app might not be running when the link opens. I configure the navigation linking prop with prefixes and screen mappings, handle authentication flows before navigating, and test thoroughly with both custom schemes and universal links. Deep links are essential for features like email verification, password resets, or marketing campaigns."],
    },
    {
        text: "What is CodePush and how does over-the-air updates work?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["CodePush is a Microsoft service that enables over-the-air updates for React Native apps, letting you push JavaScript and asset changes without going through app store review. It works by uploading a new bundle to CodePush, then the app checks for updates on launch or resume and downloads the new bundle in the background. Updates can be mandatory or optional, and you control when they're applied - immediately, on next resume, or on next restart. This is incredibly useful for bug fixes, content updates, or A/B testing. However, you can only update JavaScript and assets - native code changes still require an app store release. I use CodePush for rapid iteration and hot fixes, with staged rollouts to catch issues early. The key is proper versioning and targeting - make sure updates match the native app version. It's also important to have rollback capabilities in case an update causes issues."],
    },
    {
        text: "How do you handle gestures and animations in React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.animations],
        answers: ["For animations, I use the Animated API with useNativeDriver: true to run animations on the native thread for 60fps performance. For complex gestures, React Native Gesture Handler provides native gesture recognizers that feel more responsive than the built-in PanResponder. React Native Reanimated is the most powerful option, combining gestures and animations with a worklet-based API that runs entirely on the UI thread. For simple animations like fading or sliding, Animated is sufficient. For interactive gestures like swipe-to-delete or draggable cards, I use Gesture Handler with Reanimated. The key difference from web is that you want animations running on the native thread to avoid jank - the JavaScript thread can be busy but animations stay smooth. I avoid animating properties that don't support native driver, like layout properties. For layout animations, LayoutAnimation provides simple spring-based animations. The ecosystem around Reanimated 2+ is really mature now with great developer experience."],
    },

    // Accessibility Advanced
    {
        text: "What is the accessibility tree?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.a11y],
        answers: ["The accessibility tree is a simplified version of the DOM tree that assistive technologies like screen readers use to navigate and understand a page. Browsers create it from the DOM by including only semantically meaningful elements and their relationships, roles, states, and properties. Elements like divs without semantic meaning are excluded, while buttons, headings, and links are included with their accessible names and roles. Proper HTML semantics automatically create a good accessibility tree - using button instead of div with onClick, nav for navigation, and proper heading hierarchy. ARIA attributes let you enhance or override the default tree when semantic HTML isn't enough. I check the accessibility tree using browser DevTools to ensure interactive elements are properly exposed and have correct roles and names. Understanding the accessibility tree helps debug why a screen reader isn't working as expected - often it's because elements aren't in the tree or have the wrong information."],
    },
    {
        text: "How do you make a custom dropdown accessible?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.aria],
        answers: ["Making a custom dropdown accessible requires proper ARIA roles, keyboard navigation, and focus management. The button should have aria-expanded to indicate state, aria-haspopup=\"listbox\", and aria-controls pointing to the dropdown ID. The dropdown itself should have role=\"listbox\" and each option role=\"option\" with aria-selected state. Implement keyboard navigation - Space/Enter to open, Escape to close, Arrow keys to move between options, and Home/End for first/last. Focus management is crucial - focus should move into the dropdown when opened and return to the trigger when closed. The selected option should be indicated visually and with aria-selected. I also add aria-label or aria-labelledby for screen reader users. Honestly, building fully accessible custom dropdowns is complex, so I often use libraries like Radix UI or Headless UI that handle this correctly. If building from scratch, I follow the ARIA Authoring Practices Guide patterns exactly."],
    },
    {
        text: "What are skip links?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility],
        answers: ["Skip links are hidden links at the very top of a page that let keyboard users jump directly to main content, bypassing repetitive navigation. They're typically invisible until focused, then appear at the top of the page. This is essential for keyboard and screen reader users who otherwise have to tab through dozens of navigation links on every page. You implement them with an anchor link that points to the main content's ID, styled to be visually hidden until focused. When clicked, focus should move to the target element, which might need tabindex=\"-1\" if it's not naturally focusable. I make sure skip links are the first focusable element on the page and are visible when focused. Common skip links include 'Skip to main content', 'Skip to navigation', and 'Skip to footer'. This is a simple addition that dramatically improves keyboard navigation experience."],
    },
    {
        text: "How do you test for accessibility?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.testing],
        answers: ["I use a combination of automated and manual testing. Automated tools like axe DevTools, Lighthouse, or eslint-plugin-jsx-a11y catch obvious issues like missing alt text or color contrast problems, but they only catch about 30-40% of issues. Manual testing is essential - I navigate the entire site with only keyboard, use a screen reader like NVDA or VoiceOver to verify the experience, and test with browser zoom at 200%. I check that all interactive elements are reachable and operable with keyboard, focus indicators are visible, form errors are announced, and dynamic content updates are communicated. For automated testing in CI, I use jest-axe or pa11y. I also test with real users who use assistive tech when possible. The WCAG guidelines are my reference - I aim for AA compliance minimum. Color contrast, keyboard navigation, and screen reader announcements are the areas where I find the most issues."],
    },
    {
        text: "What is keyboard navigation and how do you implement it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum["keyboard-navigation"]],
        answers: ["Keyboard navigation means users can operate your entire site using only the keyboard, without a mouse. The basics are Tab to move forward, Shift+Tab to move backward, Enter/Space to activate, and Arrow keys for component navigation. Implementation starts with using semantic HTML - buttons, links, and form elements are naturally keyboard accessible. For custom interactive elements, add tabindex=\"0\" to make them focusable and handle keyboard events. Focus indicators must be visible - never remove outline without providing an alternative. Focus order should follow visual order and make logical sense. For complex widgets like tabs or menus, implement roving tabindex so only one item is in the tab order and arrow keys move between items. Trapped focus in modals is important - focus should stay within the modal until it closes. I test by unplugging my mouse and trying to complete all tasks. The Web AIM keyboard testing guide is a great resource."],
    },
    {
        text: "What are ARIA live regions?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.aria],
        answers: ["ARIA live regions announce dynamic content changes to screen reader users. You mark an element with aria-live=\"polite\" or aria-live=\"assertive\" to indicate updates should be announced. Polite waits for the user to finish what they're doing, while assertive interrupts immediately - use assertive sparingly for critical alerts. There's also aria-live=\"off\" which is the default. Related attributes include aria-atomic to announce the entire region versus just changes, and aria-relevant to specify what types of changes to announce. Common use cases are form validation errors, loading states, notification toasts, or updating counts. The live region must exist in the DOM before content changes for announcements to work - you can't add both the region and content simultaneously. I typically have a live region in my layout that I update with messages as needed. Role=\"status\" and role=\"alert\" are shortcuts for common live region patterns."],
    },
    {
        text: "What is color contrast and how do you check it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.wcag],
        answers: ["Color contrast is the difference in luminance between text and background, ensuring text is readable for people with visual impairments. WCAG AA requires a ratio of at least 4.5:1 for normal text and 3:1 for large text. AAA level requires 7:1 and 4.5:1 respectively. This also applies to interactive elements and graphics. I check contrast using browser DevTools which show the ratio when inspecting elements, or dedicated tools like the WebAIM contrast checker or Colour Contrast Analyser. Design systems should define color palettes with accessible combinations. Common mistakes are gray text on white backgrounds, light text on colorful backgrounds, or relying only on color to convey information. I make sure links are distinguishable from body text without relying solely on color - underlines or other visual indicators help. Automated tools flag contrast issues, but I also visually review to ensure readability in real-world conditions."],
    },

    // Testing Advanced
    {
        text: "How do you test components that fetch data?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["I mock the data fetching mechanism and test the component's behavior with different data states. For components using fetch directly, I mock global fetch with jest.spyOn or a library like MSW (Mock Service Worker) which intercepts network requests. For components using TanStack Query, I wrap them in a QueryClientProvider with a test QueryClient. The test pattern is: render the component, wait for loading state to resolve with waitFor or findBy queries, assert the correct data is displayed, and test error states by mocking failed requests. MSW is my preferred approach because it mocks at the network level, making tests more realistic and not coupled to implementation details. I also test loading states, error states, and empty states. The key is making tests resilient to timing issues with proper async utilities like waitFor, and not testing implementation details like whether useState was called - focus on what the user sees."],
    },
    {
        text: "What is act() and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["act() is a test utility that ensures all updates related to state changes, effects, and rendering are processed before making assertions. React Testing Library wraps most operations in act() automatically - things like render, fireEvent, and userEvent - so you rarely need it directly. You might need act() when using asynchronous utilities outside of RTL's helpers, or when manually triggering state updates in tests. If you see act() warnings, it usually means there are state updates happening after your test completes - often from useEffect or timers. The fix is typically to wait for those updates with waitFor or to clean up side effects properly. Modern RTL with async utilities like findBy queries handles most act() requirements automatically. I avoid wrapping things in act() unless I'm getting warnings, and when I am, I first try to fix the underlying issue rather than silencing the warning."],
    },
    {
        text: "How do you test custom hooks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["I use @testing-library/react-hooks or the newer renderHook utility from React Testing Library. renderHook wraps the hook in a test component and provides a result object to access the hook's return value. You can test the initial state, call returned functions to test state updates, and test effects by waiting for changes. For hooks with dependencies like context or providers, you pass a wrapper option. The pattern is: call renderHook, access result.current for the hook's value, call rerender to test prop changes, and use waitFor for async updates. For example, testing a useToggle hook: render it, assert initial state is false, call toggle function from result.current, assert state is now true. For hooks using TanStack Query or other providers, wrap in the necessary providers. I test the hook's API and behavior, not implementation details - if the hook returns the right values and functions work correctly, implementation can change."],
    },
    {
        text: "How do you test error boundaries?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["Testing error boundaries requires making a component throw an error and asserting the error UI renders. I create a component that throws when a prop is true, wrap it in the error boundary, and render with that prop. Console errors during testing are noisy, so I mock console.error to suppress them. The pattern is: spy on console.error to suppress output, render the error boundary wrapping a component that will throw, assert the fallback UI is displayed, and restore console.error after the test. You might need to use act() or waitFor if the error boundary updates asynchronously. React Testing Library has a suppressConsoleWarnings option. I also test that the error boundary doesn't show for components that don't throw, and test the reset functionality if the error boundary provides one. Error boundaries are a good case for integration testing since they test React's error handling mechanism."],
    },
    {
        text: "What is screen and why should you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl],
        answers: ["screen is an object from React Testing Library that provides query methods scoped to the entire document body. Instead of destructuring queries from render, you use screen.getByRole, screen.findByText, etc. The benefit is you don't need to keep track of the render result, tests are more concise, and it encourages querying from the user's perspective of the entire page. It also makes tests more maintainable since you're not constantly destructuring and managing query functions. The pattern is: render the component, then use screen.getByRole('button') instead of const { getByRole } = render() and getByRole('button'). This is the recommended approach in modern RTL. I use screen for all queries unless I specifically need container or other values from render. It makes tests cleaner and aligns with the library's philosophy of testing from the user's perspective of the whole document."],
    },
    {
        text: "How do you debug failing tests?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.debugging],
        answers: ["I start with screen.debug() to see what's actually rendered - this often reveals the component isn't in the state I expected. For specific elements, screen.logTestingPlaygroundURL() gives an interactive playground to find the right queries. I check if I'm using the right query type - getBy throws immediately, queryBy returns null, findBy is async. If timing issues, I add waitFor or switch to findBy queries. I run the single failing test with .only to isolate it, and use --watch mode to iterate quickly. For async issues, I increase the timeout or add better waiting. Common issues are: querying for text that's not actually rendered, not waiting for async operations, querying too specifically and being brittle, or having multiple elements matching the query. The RTL error messages are usually helpful - they show what was found and suggest alternatives. I also check test coverage to ensure I'm actually testing the paths I think I am."],
    },

    // Git Advanced
    {
        text: "What is interactive rebase?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.rebase],
        answers: ["Interactive rebase lets you rewrite commit history by editing, reordering, squashing, or dropping commits. You run git rebase -i HEAD~n to rebase the last n commits, which opens an editor with commands for each commit. Commands include pick (keep), reword (change message), edit (amend commit), squash (combine with previous), fixup (squash but discard message), and drop (remove). This is incredibly useful for cleaning up commits before merging - you can combine WIP commits, fix typos in messages, or reorder commits logically. I use it to maintain a clean, meaningful commit history on feature branches. The golden rule is never rebase commits that have been pushed to shared branches - only rebase local commits or commits on your personal feature branch. After rebasing, you'll need to force push, which is why it's dangerous on shared branches."],
    },
    {
        text: "What is git bisect?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.debugging],
        answers: ["git bisect is a binary search tool for finding which commit introduced a bug. You start with git bisect start, mark the current bad commit with git bisect bad, and mark a known good commit with git bisect good <commit>. Git then checks out commits in the middle of that range, you test each one and mark it good or bad, and Git narrows down until it finds the first bad commit. You can automate this with git bisect run <script> that returns 0 for good, 1 for bad. This is incredibly powerful when you know something broke but don't know when or why. It's much faster than manually checking commits. I've used it to find performance regressions or broken tests when the commit history is long. Once bisect finds the culprit commit, you can examine what changed and why it broke."],
    },
    {
        text: "What are Git submodules?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git submodules let you include one Git repository inside another as a subdirectory, keeping them as separate repos with independent histories. You add one with git submodule add <url>, which creates a .gitmodules file and checks out the submodule at a specific commit. The parent repo stores a pointer to the submodule's commit, not the submodule's content. When cloning a repo with submodules, you need git clone --recursive or git submodule update --init. Submodules are tricky because they're pinned to specific commits - updating requires explicitly pulling in the submodule and committing the new pointer. They're useful for shared libraries or dependencies you want version-controlled separately. However, they're complex and I avoid them when possible - modern package managers and monorepo tools like pnpm workspaces are usually better solutions. If I do use them, I document the workflow clearly."],
    },
    {
        text: "What is force push and when is it safe to use?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Force push overwrites the remote branch with your local branch, discarding any commits that exist remotely but not locally. It's needed after rewriting history with rebase, amend, or reset. The danger is that it can delete other people's work if they've pushed to the same branch. It's safe when: you're the only one working on the branch, you've coordinated with your team, or you're fixing your own feature branch. Use --force-with-lease instead of --force - it only succeeds if the remote hasn't been updated since you last fetched, preventing you from overwriting someone else's work. Never force push to main, master, or shared branches unless there's an emergency and you've coordinated with the team. I use it regularly on feature branches after cleaning up commits with interactive rebase, but I'm very careful and always use --force-with-lease."],
    },
    {
        text: "What is squashing commits and when would you do it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Squashing combines multiple commits into a single commit, cleaning up history. You can squash with interactive rebase (using squash or fixup commands) or when merging with git merge --squash. Squashing is useful for combining WIP commits, cleanup commits, or multiple commits that represent a single logical change. It makes the main branch history cleaner and easier to understand. However, it loses the granular history, so you lose the ability to understand the detailed progression of changes or to cherry-pick specific parts. I squash commits on feature branches before merging to main to keep one commit per feature or fix. Some teams prefer preserving all commits for full history. The right approach depends on your team's workflow - some want atomic commits in main, others want full history. GitHub and GitLab both support squash merging, which makes this easy."],
    },

    // Performance Advanced
    {
        text: "What is the Performance panel and how do you identify bottlenecks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["The Performance panel in Chrome DevTools records runtime performance by capturing all activity during a session. You start a recording, interact with the page, stop recording, and analyze the timeline. The flame graph shows which functions took the longest - wider bars mean longer execution. Look for long tasks (over 50ms) that block the main thread, excessive layout recalculations from forced reflows, or long paint times. The Summary tab shows time breakdown by activity type - scripting, rendering, painting. I look for unnecessary re-renders in React, expensive operations in loops, or layout thrashing from reading and writing layout properties. The Bottom-Up and Call Tree tabs help identify which functions are most expensive. Common bottlenecks include large DOM operations, unoptimized images, inefficient selectors, or heavy JavaScript execution. The key is recording realistic user interactions, not just page load."],
    },
    {
        text: "What is the Memory panel and how do you detect memory leaks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["The Memory panel takes heap snapshots showing what's using memory in your app. To detect leaks, I take a snapshot, perform actions that should clean up (like navigating away), force garbage collection, take another snapshot, and compare them. If memory keeps growing, there's likely a leak. The Comparison view shows what increased between snapshots. Common culprits are event listeners not removed, timers not cleared, DOM references kept after elements removed, or closures holding onto large objects. The Retainers section shows why an object is still in memory. I also use the Allocation Timeline to see memory allocation over time - steady growth indicates leaks. In React, leaks often come from useEffect without cleanup, subscriptions not unsubscribed, or listeners not removed. The DevTools highlight detached DOM nodes which are leaked elements. Fix leaks by ensuring cleanup in useEffect, clearing timers, and removing event listeners."],
    },
    {
        text: "What are React DevTools and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.debugging],
        answers: ["React DevTools is a browser extension with Components and Profiler tabs. The Components tab shows the React component tree, lets you inspect props and state, edit values in real-time, and see which component rendered which DOM elements. You can also view hooks and their values. The Profiler tab records rendering performance - which components rendered, why they rendered, and how long they took. I use it to identify unnecessary re-renders by looking at why each component rendered (props changed, parent rendered, etc.). The flame graph shows expensive renders, and the ranked chart shows which components took longest. You can highlight renders in the browser to visually see what's re-rendering. I also use the 'rendered by' feature to trace where components come from. For debugging, I inspect component props and state to verify they match expectations. The ability to edit state live is incredibly useful for testing different scenarios without changing code."],
    },
    {
        text: "What are some debugging tools you use?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging],
        answers: ["I use a combination of tools depending on the issue. Chrome DevTools is essential - Console for logging, Sources for breakpoints and stepping through code, Network for API issues, Performance for runtime bottlenecks, and Application for storage and service workers. React DevTools for component inspection and performance profiling. For network issues, I use the Network panel or tools like Postman for API testing. For build issues, I check webpack stats or use bundle analyzers. For state management, Redux DevTools or Zustand DevTools. VS Code debugger for stepping through Next.js server-side code. console.log is still useful but I try to use debugger statements and breakpoints instead. For production issues, error tracking with Sentry or LogRocket that includes session replay. The key is using the right tool for the problem - network issues need Network tab, rendering issues need Performance panel, logic issues need the debugger."],
    },
    {
        text: "What is the difference between LCP, FID/INP, and CLS?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum["web-vitals"], ValidTag.enum.lcp, ValidTag.enum.fid, ValidTag.enum.cls],
        answers: ["These are Core Web Vitals that measure user experience. LCP (Largest Contentful Paint) measures loading performance - when the largest content element becomes visible. Good LCP is under 2.5 seconds. Improve it by optimizing images, server response times, and render-blocking resources. FID (First Input Delay) measures interactivity - the delay between user interaction and browser response. It's being replaced by INP (Interaction to Next Paint) which measures overall responsiveness. Good FID is under 100ms, INP under 200ms. Improve by reducing JavaScript execution time and breaking up long tasks. CLS (Cumulative Layout Shift) measures visual stability - unexpected layout shifts. Good CLS is under 0.1. Improve by setting dimensions on images and embeds, avoiding inserting content above existing content, and using transform animations instead of layout properties. I monitor these with Lighthouse, Chrome UX Report, or real user monitoring. They directly impact SEO and user experience."],
    },
    {
        text: "What are source maps and how do they work?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging],
        answers: ["Source maps are files that map minified/compiled code back to the original source code, making debugging production code possible. When your code is bundled and minified, error stack traces point to meaningless minified code. Source maps let the browser show you the original file, line, and column numbers. They work through a special comment in the compiled code pointing to the .map file, which contains a mapping between transformed and original code. In development, you want inline source maps for fast debugging. In production, I use hidden source maps that aren't deployed publicly but can be uploaded to error tracking services like Sentry. This keeps source code private while still enabling debugging. Different source map types have tradeoffs between build speed and quality - 'eval-source-map' is fast but large, 'source-map' is slower but production-ready. Most tools like webpack handle source map generation automatically."],
    },
    {
        text: "How do you profile JavaScript performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["I use the Performance panel in DevTools to record and analyze JavaScript execution. Start recording, perform the action to profile, stop recording, and examine the flame chart. Look for long tasks over 50ms, excessive function calls, or expensive operations. The Bottom-Up view shows which functions consumed the most time. For detailed function-level profiling, console.time/timeEnd or performance.mark/measure work well. React Profiler specifically shows component render times. I look for unnecessary calculations, inefficient algorithms, or operations in tight loops. Common issues include excessive object creation, unoptimized loops, synchronous operations blocking the thread, or missing memoization. The key is testing with realistic data sizes - performance problems often only appear at scale. For production monitoring, tools like Sentry track long tasks and slow renders. The goal is keeping the main thread free so the UI stays responsive - break up long tasks, use web workers for heavy computation, or defer non-critical work."],
    },
    {
        text: "What is the difference between client-side and server-side profiling?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["Client-side profiling measures performance in the user's browser - render times, JavaScript execution, network requests. Tools are browser DevTools, Lighthouse, or Web Vitals. You see real user device constraints and network conditions. Server-side profiling measures backend performance - database queries, API response times, server CPU/memory. Tools are APMs like New Relic, Datadog, or built-in Node profilers. With SSR frameworks like Next.js, you need both - server-side for getServerSideProps execution and database queries, client-side for hydration and interactivity. The challenges are different - client-side is affected by device capabilities and network, server-side by database performance and concurrent requests. I use client-side profiling to optimize bundle size, rendering, and user interactions. Server-side to optimize API response times and database queries. Real User Monitoring combines both to see the full picture of how fast the app feels to actual users."],
    },
    {
        text: "How do you identify and fix memory leaks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["I use the Memory panel to take heap snapshots before and after actions. The workflow is: take snapshot, perform action that might leak (like opening/closing a modal repeatedly), force garbage collection, take another snapshot, compare. Growing memory between snapshots indicates a leak. The Comparison view shows what increased. Common React leaks are useEffect without cleanup returning functions, event listeners added but not removed, intervals not cleared, or subscriptions not unsubscribed. Detached DOM nodes are elements removed from the DOM but still referenced in memory. The Retainers view shows why an object can't be garbage collected. To fix: ensure useEffect cleanup functions remove listeners and clear timers, unsubscribe from observables, remove event listeners with the same function reference used to add them. WeakMap can help for caching without preventing garbage collection. I also watch for growing arrays or objects that should be bounded. The key is reproducing the leak in a controlled way so you can snapshot before and after."],
    },
    {
        text: "What causes render blocking and how do you avoid it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance],
        answers: ["Render blocking happens when resources prevent the browser from displaying content. CSS is render-blocking by default because the browser needs styles before painting. JavaScript blocks HTML parsing unless it's async or defer. Large synchronous scripts in the head are the worst offenders. To avoid: load critical CSS inline or as high priority, defer non-critical CSS with media queries or rel=\"preload\", use async or defer on scripts, split code to only load what's needed, move scripts to the end of body. For CSS, extract critical above-the-fold styles and inline them, lazy-load the rest. Next.js and modern frameworks handle a lot of this automatically with code splitting and optimized loading. Font loading can also block - use font-display: swap to show fallback fonts immediately. The goal is getting content visible as fast as possible, even if not fully styled or interactive. Lighthouse flags render-blocking resources and suggests optimizations."],
    },
    {
        text: "What is the critical rendering path?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance],
        answers: ["The critical rendering path is the sequence of steps the browser takes to convert HTML, CSS, and JavaScript into pixels on screen. The steps are: parse HTML into DOM, parse CSS into CSSOM, combine DOM and CSSOM into render tree, calculate layout (where everything goes), and paint pixels. JavaScript can block this process - when the browser hits a script tag, it stops parsing HTML to execute the script. CSS blocks JavaScript execution because scripts might query styles. This means CSS indirectly blocks HTML parsing. Optimizing the critical path means minimizing render-blocking resources, deferring non-critical resources, reducing file sizes, and minimizing critical CSS. I inline critical CSS, defer non-critical CSS, async or defer JavaScript, and minimize the number of critical resources. Understanding this helps explain why putting scripts in the head slows page load, or why large CSS files delay rendering even if images are optimized."],
    },

    // Bundle Sizing
    {
        text: "How would you debug large bundle sizes?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["I start with a bundle analyzer to visualize what's taking up space. webpack-bundle-analyzer or Next.js's @next/bundle-analyzer show a treemap of your bundle. Common issues include: importing entire libraries instead of specific functions (lodash instead of lodash/debounce), multiple versions of the same package, large dependencies that could be replaced, source maps or development code in production builds, or duplicate code across chunks. I check the import statements - using named imports from barrel files can prevent tree shaking. I look for moment.js which is huge and can be replaced with date-fns or day.js. I verify production mode is enabled and minification is working. I also check for accidentally imported dev dependencies. The bundle analyzer quickly shows the biggest contributors. Once identified, solutions include using dynamic imports for large features, replacing heavy libraries, fixing duplicate dependencies in package.json, or splitting vendor code into separate chunks."],
    },
    {
        text: "What tools do you use to analyze bundle size?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance, ValidTag.enum["bundle-analysis"]],
        answers: ["webpack-bundle-analyzer is my go-to for visualizing bundle composition as an interactive treemap. For Next.js, @next/bundle-analyzer integrates seamlessly. bundlephobia.com lets me check the size of packages before installing them. Source-map-explorer analyzes the final bundle using source maps. For ongoing monitoring, I use bundlesize or size-limit in CI to fail builds that exceed thresholds. VS Code extensions like Import Cost show package sizes inline. In the browser, the Coverage tab in DevTools shows unused code in each file. Lighthouse reports bundle size issues. For build output, most bundlers show gzipped sizes which is what actually matters over the network. I also check the Network tab to see actual transferred sizes. The combination of visualization tools for investigation and automated checks in CI for prevention works well. The key is making bundle size visible and tracked, not just a one-time check."],
    },
    {
        text: "What is webpack-bundle-analyzer?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.webpack, ValidTag.enum.bundling, ValidTag.enum["bundle-analysis"]],
        answers: ["webpack-bundle-analyzer is a plugin that generates an interactive treemap visualization of your webpack bundle contents. Each rectangle represents a module, sized by its actual size in the bundle. You can see what's taking up space, drill down into dependencies, and identify optimization opportunities. It shows both stat size (original), parsed size (output), and gzipped size (what's actually transferred). I add it to webpack config or use it as a one-off with the CLI. The visualization makes it immediately obvious if you're accidentally including a massive library or have duplicate code. You can hover over modules to see their full path and size. It's invaluable for optimization - I've found things like entire icon libraries included when only a few icons were needed, or dev dependencies accidentally bundled. For Next.js there's @next/bundle-analyzer which wraps this. I run it periodically, especially after adding new dependencies."],
    },
    {
        text: "What is the difference between named exports and default exports for tree shaking?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tree-shaking"], ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["Named exports generally tree-shake better than default exports because bundlers can statically analyze exactly what's imported. With named exports, if you import { specific } from 'library', the bundler knows you only need 'specific' and can remove everything else. With default exports, especially when re-exporting an object with many properties, it's harder for bundlers to determine what's actually used. However, the real issue is barrel files - index files that re-export everything. Even with named imports, if the barrel file imports everything first, you might get the whole library. The best practice for tree-shakeable libraries is named exports with separate entry points, not barrel files. For example, lodash-es tree shakes better than lodash because it uses ES modules. I use named exports for library code and avoid deep barrel files. The sideEffects field in package.json also helps bundlers know it's safe to tree shake."],
    },
    {
        text: "How do you optimize third-party library imports?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["I start by importing only what I need using specific paths rather than barrel files. For lodash, import debounce from 'lodash/debounce' instead of import { debounce } from 'lodash'. For icon libraries, use individual icon imports or optimized packages. I check if there's a lighter alternative - replace moment.js with date-fns or day.js, use preact instead of react for smaller bundles if possible. For UI libraries like Material-UI, configure babel plugins for automatic tree shaking. I avoid libraries that don't support tree shaking or are unnecessarily large. Before adding dependencies, I check bundlephobia for their impact. For already included libraries, I look for CDN options or dynamic imports to defer loading. I also check for duplicate dependencies with npm dedupe or pnpm. Using the browser's native APIs instead of libraries is ideal - intersection observer instead of scroll libraries, native date formatting instead of moment."],
    },
    {
        text: "What is dynamic imports and how do they help bundle size?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum["code-splitting"], ValidTag.enum.performance],
        answers: ["Dynamic imports use import() syntax to load modules asynchronously at runtime instead of bundling them in the main bundle. They return a promise that resolves to the module. This creates separate chunks that are only loaded when needed, reducing the initial bundle size. For example, import('./HeavyComponent').then(mod => ...) loads HeavyComponent only when that code path executes. In React, React.lazy wraps dynamic imports for component code splitting. I use dynamic imports for routes - each route becomes a separate chunk loaded on navigation. Also for features not immediately needed like modals, charts, or admin sections. The tradeoff is a network request when the code is needed, so there's a slight delay. But the improved initial load time usually outweighs this. Next.js handles this automatically with dynamic imports. The key is identifying code that isn't needed on initial render and splitting it into separate chunks."],
    },
    {
        text: "How do you measure and track bundle size over time?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance, ValidTag.enum["bundle-analysis"]],
        answers: ["I use automated tools in CI to prevent regressions. bundlesize or size-limit run on every PR and fail if bundles exceed defined thresholds. They can comment on PRs with size changes. bundlewatch tracks size over time and integrates with GitHub status checks. For more detailed tracking, I export webpack stats JSON and use tools like bundle-stats or relative-ci to compare builds. These show historical trends and highlight what changed. In the build output itself, most bundlers show compressed sizes - I track these numbers in release notes. For production monitoring, tools like SpeedCurve or Calibre track real bundle sizes from actual users. I set alerts for significant increases. The workflow is: define size budgets based on performance goals, enforce them in CI, review changes in PRs, and monitor production. The key is making bundle size a team concern, not just something one person checks occasionally. Automation prevents accidental bloat."],
    },
];

