import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const midAdvanced: QuestionForCategoryAndLevel<
    typeof Category.enum.frontend,
    typeof Level.enum["mid-advanced"]
>[] = [
    // JavaScript
    {
        text: "What are WeakMap and WeakSet and when would you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.weakmap, ValidTag.enum.weakset],
        answers: ["WeakMap and WeakSet are collections that hold weak references to their keys or values, which means they don't prevent garbage collection. The key difference from regular Map and Set is that WeakMap keys must be objects, and if there are no other references to that object, it can be garbage collected. I typically use WeakMap for storing metadata about objects without preventing those objects from being cleaned up - like caching DOM nodes or storing private data for class instances. WeakSet is useful for marking objects, like tracking which objects have been processed. The main limitation is you can't iterate over them or check their size, since the contents can change at any time due to garbage collection.",
            "WeakMap and WeakSet hold weak references to object keys, allowing garbage collection when there are no other references. Unlike Map and Set, WeakMap keys must be objects, and you can't iterate or check size since entries can disappear anytime. WeakMap is great for attaching metadata to objects without preventing cleanup, like caching computed values for DOM nodes or implementing private class fields. WeakSet is useful for marking objects as processed without holding references. I use WeakMap for caching expensive computations keyed by objects that might be garbage collected. The weak reference aspect is the key differentiator since regular Map/Set would create memory leaks in those scenarios."],
    },
    {
        text: "What is the Temporal Dead Zone?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.hoisting, ValidTag.enum.scope],
        answers: ["The Temporal Dead Zone is the period between entering a scope and the actual declaration of a let or const variable, where the variable exists but can't be accessed. If you try to access the variable during this time, you'll get a ReferenceError. This is different from var, which gets hoisted and initialized with undefined. For example, if you have a let statement in the middle of a function, you can't access that variable anywhere before that line, even though the variable is technically hoisted. This behavior actually helps catch bugs by making it clear when you're trying to use a variable before it's ready. It's called temporal because it's about the time between entering the scope and the declaration, not the physical location in the code.",
            "The TDZ is the period from entering a scope until the let or const declaration is reached, during which the variable cannot be accessed. Accessing it throws a ReferenceError. Unlike var which hoists and initializes to undefined, let and const are hoisted but remain uninitialized in this dead zone. This is a safety feature that catches bugs from using variables before they're ready. The name refers to the temporal aspect since it's about time in execution, not position in code. I find it helpful because it makes code more predictable and prevents subtle bugs that var's hoisting behavior can cause."],
    },

    // TypeScript
    {
        text: "What are conditional types and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["conditional-types"], ValidTag.enum.types],
        answers: ["Conditional types let you express type logic using a ternary-like syntax - basically 'if this type extends that type, then return this, else return that'. The syntax is T extends U ? X : Y. I use them all the time for creating flexible, type-safe utilities. For example, you might create a type that returns different shapes based on the input type, or extract certain properties based on conditions. They're particularly powerful when combined with infer for extracting types from generic parameters. A common use case is creating a type that unwraps Promise types, where you'd check if a type extends Promise and extract the inner type. They enable really sophisticated type manipulation that would be impossible with simpler type features.",
            "Conditional types use ternary syntax for type-level logic: T extends U ? X : Y. If T is assignable to U, the type is X, otherwise Y. They're powerful for creating flexible utility types that respond differently based on input. Combined with infer, you can extract parts of types, like getting the return type of a function or unwrapping a Promise. They distribute over unions by default, applying the condition to each union member. I use them for building type-safe utilities like extracting array element types or creating conditional prop requirements based on other prop values. They enable sophisticated type transformations that make APIs much more type-safe."],
    },
    {
        text: "What are mapped types?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["mapped-types"], ValidTag.enum.types],
        answers: ["Mapped types let you transform one type into another by iterating over its properties. The syntax looks like { [K in keyof T]: SomeType }. They're incredibly useful for creating variations of existing types - like making all properties optional, readonly, or nullable. Built-in utility types like Partial, Required, and Pick are all implemented using mapped types. I often use them to create derived types from API responses or to transform database models into form types. You can also use key remapping with the 'as' clause to rename properties or filter them out. They're one of TypeScript's most powerful features for reducing type duplication and maintaining type safety when you need similar but slightly different type shapes.",
            "Mapped types iterate over properties to create new types. The syntax { [K in keyof T]: NewType } loops over each key in T and creates a new property with the transformed type. Built-ins like Partial, Required, Readonly, and Pick all use mapped types. Key remapping with the 'as' clause lets you rename or filter properties: { [K in keyof T as K extends 'id' ? never : K]: T[K] } excludes the id property. Modifiers like readonly and ? can be added or removed with + and -. I use mapped types to transform API types to form types, create getter/setter interfaces, or derive related types while keeping them synchronized."],
    },
    {
        text: "What is the infer keyword and how do you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["conditional-types"]],
        answers: ["The infer keyword lets you extract and store types within conditional types for later use. It's like pattern matching for types - you're saying 'if this type matches this pattern, extract this part and let me use it'. The classic example is extracting the return type of a function: if T extends a function that returns infer R, then R is the return type. You can only use infer within the extends clause of a conditional type. I commonly use it for unwrapping types - like getting the element type from an array, the resolved value from a Promise, or parameters from a function signature. It's essential for building advanced utility types that need to extract and manipulate parts of complex type structures.",
            "The infer keyword captures types within conditional type patterns. In T extends Array<infer U> ? U : never, if T is an array, U becomes the element type. It only works in the extends clause of conditional types. Classic uses include extracting function return types, Promise resolved values, or array element types. The built-in ReturnType, Parameters, and Awaited utility types all use infer. You can have multiple infers in one pattern to extract several parts. I use it for unwrapping wrapper types, extracting generics from library types, or building utilities that need to decompose complex types."],
    },
    {
        text: "What are template literal types?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Template literal types let you build string types by combining literal strings with other string types, using the same backtick syntax as template literals in JavaScript. They're incredibly powerful for creating type-safe string patterns. For example, you can create a type that represents CSS properties like 'padding-top' or 'margin-left' by combining 'padding' | 'margin' with 'top' | 'left'. They support all the manipulation you'd expect - concatenation, unions, and even intrinsic string manipulation with Uppercase, Lowercase, Capitalize, and Uncapitalize. I often use them for things like event names, CSS class names, or API endpoints where you want type safety for string patterns. They're especially useful when combined with mapped types to generate variations of string-based keys.",
            "Template literal types create string types using backtick syntax like JavaScript template literals. You can combine string literals with unions to generate all combinations: `${'get' | 'set'}${Capitalize<'name' | 'age'>}` produces 'getName' | 'setName' | 'getAge' | 'setAge'. Built-in intrinsics like Uppercase, Lowercase, Capitalize, and Uncapitalize transform string types. They're great for event names, CSS class patterns, or API route types. Combined with mapped types, you can generate entire type-safe APIs from string patterns. I use them for defining route patterns, event handler names, or ensuring string arguments match expected formats."],
    },
    {
        text: "What is type widening and how do you prevent it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Type widening is when TypeScript automatically expands a specific type to a more general one. For example, when you declare a variable with let and initialize it with a string literal like 'hello', TypeScript widens the type from the literal 'hello' to the general type string. This is usually what you want for mutable variables, but it can cause issues when you need precise types. You can prevent widening in several ways - using const instead of let keeps literal types, adding 'as const' to the end of an expression makes everything deeply readonly and literal, or you can explicitly type annotate the variable. I commonly use 'as const' when defining configuration objects or lookup tables where I want the exact literal values preserved in the type system.",
            "Type widening happens when TypeScript infers a broader type than the literal value provided. With let x = 'hello', the type becomes string, not 'hello'. This makes sense for mutable variables since the value might change. To preserve literal types, use const declarations, 'as const' assertions, or explicit type annotations. 'as const' is especially useful for objects and arrays since it makes the entire structure readonly with literal types. I use 'as const' for configuration objects, lookup tables, or when passing objects to functions that need exact literal types for inference to work correctly."],
    },
    {
        text: "What are function overloads in TypeScript?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Function overloads let you define multiple type signatures for a single function implementation, which is useful when a function behaves differently based on its input types. You write multiple function signature declarations above the actual implementation. For example, you might have a function that takes either a string or a number and returns different types accordingly. The key thing to understand is that the overload signatures are what callers see - they define the public API - while the implementation signature must be general enough to handle all the overload cases. TypeScript matches overloads from top to bottom, so you should order them from most specific to most general. I use overloads when the relationship between parameters and return types is complex and can't be easily expressed with union types or generics alone.",
            "Function overloads provide multiple call signatures for one function. You list the overload signatures above the implementation, and TypeScript uses them for type checking. The implementation signature must handle all overload cases but isn't visible to callers. Overloads are matched top to bottom, so put specific signatures before general ones. They're useful when return types depend on input types in ways that generics or unions can't express cleanly. For example, a function returning string when given a string, and number when given a number. I use overloads sparingly since conditional types with generics often work better, but they're essential for some library APIs."],
    },
    {
        text: "What is the satisfies operator and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["The satisfies operator, introduced in TypeScript 4.9, lets you validate that a value matches a type without actually changing the inferred type of that value. It's like a type assertion but in reverse - instead of forcing the type, you're checking that what you have satisfies certain constraints while keeping the more specific type. This is super useful when you want to ensure an object has certain properties but still want to maintain literal types or more specific shapes. For example, if you have a color palette object, you can use satisfies to ensure all values are valid colors while keeping the specific color strings as literal types. The main benefit over type annotations is that satisfies preserves the narrowest possible type, giving you better autocomplete and type checking.",
            "The satisfies operator validates a value matches a type while preserving the inferred type. Unlike type annotations which widen to the declared type, satisfies keeps the narrowest type. const config = { port: 3000 } satisfies { port: number } keeps port typed as 3000, not number, while still ensuring the object matches the constraint. This gives you type checking for constraints plus autocomplete for the specific values. I use it for configuration objects where I want to enforce structure but keep literal types, or for objects that must match an interface while maintaining more precise types than the interface specifies."],
    },
    {
        text: "What are branded types and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Branded types, also called nominal typing or opaque types, let you create distinct types from the same underlying primitive type so they can't be accidentally mixed up. You implement them by adding a phantom property that only exists at compile time. For example, you might have UserId and ProductId both as numbers, but you don't want to accidentally pass a ProductId where a UserId is expected. You create a brand by intersecting the primitive type with an object containing a unique symbol or literal type. The key is that this brand property is never actually present at runtime - it's purely for TypeScript's type checking. I use branded types for things like IDs, validated strings, or measurements where mixing them up would be a logical error even though they're the same underlying type.",
            "Branded types add nominal typing to TypeScript's structural type system. You create distinct types from the same primitive by intersecting with a phantom brand: type UserId = number & { readonly brand: unique symbol }. At runtime it's just a number, but TypeScript treats UserId and ProductId as incompatible. Create branded values through validator functions that assert the brand. This prevents logical errors like passing a ProductId where UserId is expected. I use branded types for IDs, validated inputs like email addresses, or units where mixing them up would cause bugs that structural typing alone can't catch."],
    },

    // CSS
    {
        text: "What are the tradeoffs between CSS Modules vs Tailwind vs CSS-in-JS?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum["css-modules"], ValidTag.enum.tailwind, ValidTag.enum["css-in-js"]],
        answers: ["Each approach has distinct tradeoffs. CSS Modules gives you scoped CSS with familiar syntax and great caching since styles are static files, but you still write and maintain separate CSS files. Tailwind offers incredible developer velocity with utility classes and built-in consistency, but can lead to verbose className strings and has a learning curve for the utility names. CSS-in-JS like styled-components gives you dynamic styling with JavaScript, automatic critical CSS, and true component encapsulation, but adds runtime overhead and can cause styling to block rendering. I generally prefer Tailwind for most projects because of the speed and consistency, CSS Modules when I need traditional CSS workflow or don't want the Tailwind bundle, and CSS-in-JS when I need heavily dynamic styles based on props or themes. The choice really depends on your team's preferences and the project's requirements.",
            "Each has distinct tradeoffs. CSS Modules scopes styles locally with standard CSS syntax and static file caching, but you maintain separate CSS files. Tailwind provides utility-first speed and design consistency but creates verbose classNames. CSS-in-JS enables dynamic styles and component encapsulation but adds runtime cost. For performance, CSS Modules and Tailwind are static and cache well. CSS-in-JS can block rendering. I prefer Tailwind for most work due to velocity and consistency. CSS Modules for traditional CSS workflows or avoiding Tailwind's bundle. CSS-in-JS when styles must be highly dynamic based on props. The choice depends on team preference, performance needs, and how dynamic your styling is."],
    },
    {
        text: "How do you handle dark mode in Tailwind?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum.css],
        answers: ["Tailwind has built-in dark mode support that you enable in your config file. You can use either 'media' strategy which respects the user's system preferences, or 'class' strategy which lets you manually toggle dark mode with a class on a parent element. With the class strategy, you add a 'dark' class to your html or body element, then use the dark: variant prefix on any utility class - like 'dark:bg-gray-900' or 'dark:text-white'. I usually prefer the class strategy because it gives users control and lets you persist their preference. You typically store the preference in localStorage and sync it with the class on mount. The dark: variant works with all Tailwind utilities, so you can completely customize your dark mode styling. It's really elegant once you get used to the pattern.",
            "Tailwind has two dark mode strategies. 'media' respects system preferences automatically using prefers-color-scheme. 'class' lets you control dark mode via a 'dark' class on a parent element. I prefer 'class' since it allows user preference with localStorage persistence. With the dark: variant, you prefix any utility: dark:bg-gray-900, dark:text-white. This applies only when dark mode is active. Set it up in tailwind.config.js with darkMode: 'class'. In your app, toggle the 'dark' class on the html element based on user preference. The pattern is clean once set up, and you can theme your entire app systematically."],
    },
    {
        text: "What are Tailwind plugins and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind],
        answers: ["Tailwind plugins let you extend Tailwind's functionality by adding new utilities, components, variants, or even modifying the default theme. You register them in your tailwind.config.js file's plugins array. Plugins are JavaScript functions that receive the plugin API, which gives you access to methods like addUtilities, addComponents, and addVariant. For example, you might create a plugin to add custom utilities for text shadows or gradients that aren't in the base framework. There are also official plugins like @tailwindcss/forms and @tailwindcss/typography that provide pre-built utilities for common use cases. I often use plugins to add project-specific utilities or to integrate design system tokens, which helps keep the utility classes consistent across the team while avoiding one-off custom CSS.",
            "Tailwind plugins extend the framework with custom utilities, components, or variants. Register them in tailwind.config.js plugins array. Plugin functions receive the API with addUtilities, addComponents, addVariant methods. Official plugins include @tailwindcss/forms for form styling and @tailwindcss/typography for prose content. Create custom plugins for project-specific utilities like text shadows, custom gradients, or design system integration. I use plugins to add utilities that match our design tokens, ensuring consistency without one-off CSS. Plugins can also add responsive and state variants to new utilities automatically."],
    },
    {
        text: "How do you handle animations in Tailwind?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum.animations],
        answers: ["Tailwind includes some default animations like animate-spin, animate-pulse, and animate-bounce that you can use right away. For custom animations, you extend the theme in your config file by adding keyframes and animation utilities. You define the keyframes in theme.extend.keyframes and then reference them in theme.extend.animation. For more complex animations or transitions, you can combine Tailwind's transition utilities with hover or focus states. I often use the transition-all or transition-transform utilities along with duration and ease classes. For really complex animations, I might reach for a library like Framer Motion and use Tailwind just for the basic styling. The key is that Tailwind handles the simple cases really well with utilities, but doesn't try to replace JavaScript animation libraries for complex choreography.",
            "Tailwind provides built-in animations like animate-spin, animate-pulse, and animate-bounce. For custom animations, extend the config with keyframes and animation definitions in theme.extend. Combine transition utilities with duration and ease classes for hover effects. For simple animations, Tailwind's utilities work great: transition-transform, duration-300, ease-in-out. For complex choreographed animations, I combine Tailwind with libraries like Framer Motion. Tailwind handles styling while the library handles animation orchestration. The key is recognizing when utilities are enough versus when you need programmatic control."],
    },
    {
        text: "What are CSS custom properties and how do they differ from Sass variables?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.sass],
        answers: ["CSS custom properties, or CSS variables, are native CSS features that cascade and can be changed at runtime, while Sass variables are compile-time only and get replaced during the build. The key difference is that custom properties are part of the DOM and can be manipulated with JavaScript, inherited through the cascade, and scoped to elements. For example, you can change a custom property value in a media query or with JavaScript, and all references update automatically. Sass variables are more powerful for complex preprocessing logic and calculations, but they're static once compiled. I use custom properties for theming, responsive values, and anything that needs to change dynamically. For complex calculations or build-time logic, Sass variables are still useful. In modern projects, I often use both - Sass for build-time organization and custom properties for runtime flexibility.",
            "CSS custom properties exist in the DOM and can change at runtime, inheriting through the cascade and updating all references automatically. Sass variables are compile-time only, replaced with static values during build. Custom properties can be changed with JavaScript, respond to media queries, and be scoped to elements. Sass variables enable complex preprocessing logic and compile-time calculations. I use custom properties for theming since changing one variable updates the whole theme. For build-time calculations or complex logic, Sass works well. Modern projects often combine both: Sass for organization and preprocessing, custom properties for runtime theming and dynamic values."],
    },

    // React Hooks Advanced
    {
        text: "What is useImperativeHandle and when is it appropriate?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["useImperativeHandle lets you customize the instance value that's exposed to parent components when using forwardRef. Instead of exposing the entire DOM element, you can expose only specific methods or values. For example, if you have a custom input component, you might expose only a focus method and hide everything else. You use it together with forwardRef to control what the ref actually gives access to. This is useful for component libraries where you want to provide a controlled API surface rather than direct DOM access. However, it breaks the typical React data flow, so I only use it when building reusable components that need imperative APIs - like form inputs, modals, or video players - where parent components need to trigger actions that don't fit naturally into props.",
            "useImperativeHandle customizes the value exposed to parents via refs. Instead of exposing the whole DOM node, you expose specific methods like focus() or scrollTo(). Use it with forwardRef to create controlled imperative APIs. For a custom Input component, you might expose focus and clear methods while hiding the DOM element. This is useful for component libraries providing clean APIs. I only use it when imperative actions are truly needed since it breaks declarative data flow. Common cases are form inputs needing focus, video players needing play/pause, or modals needing open/close methods."],
    },
    {
        text: "What is useDeferredValue and how does it differ from debouncing?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.performance],
        answers: ["useDeferredValue is a React 18 hook that lets you defer updating a part of the UI, keeping a stale value during urgent updates and updating it later. Unlike debouncing which delays the update by a fixed time, useDeferredValue is adaptive - React decides when to update based on available resources and tries to update as soon as possible without blocking urgent updates. For example, if you have a search input and an expensive list, you can defer the list's value so typing stays responsive. The key difference from debouncing is that there's no fixed delay, it integrates with concurrent rendering, and React can interrupt the deferred update if new input comes in. I use useDeferredValue when I want to keep the UI responsive during expensive renders, while debouncing is better for reducing the frequency of operations like API calls.",
            "useDeferredValue defers updating part of the UI, keeping a stale value while urgent updates happen. Unlike debouncing with fixed delays, it's adaptive. React updates the deferred value when resources are available, and can interrupt if new urgent updates come in. For a search input with an expensive filtered list, defer the list's value so typing stays responsive. The list updates whenever React has time, not after a fixed delay. I use it for expensive renders that shouldn't block input. Debouncing is better for reducing API call frequency. useDeferredValue is about rendering priority, not reducing work."],
    },
    {
        text: "What is useTransition and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum["concurrent-mode"]],
        answers: ["useTransition is a React 18 hook that lets you mark certain state updates as non-urgent transitions, allowing React to keep the UI responsive by interrupting those updates if needed. It returns an isPending flag and a startTransition function. When you wrap state updates in startTransition, React knows it can interrupt them to handle more urgent updates like user input. This is perfect for situations where you're updating something expensive in response to user input, like filtering a large list or changing tabs. The isPending flag lets you show loading states during the transition. The main difference from useDeferredValue is that useTransition marks the state update itself as low priority, while useDeferredValue defers consuming a value. I use it whenever I have state updates that cause expensive renders and I want to keep the UI feeling snappy.",
            "useTransition marks state updates as non-urgent, allowing React to interrupt them for urgent updates. It returns isPending and startTransition. Wrap expensive state updates in startTransition to keep the UI responsive. React can pause the transition to handle user input, then resume. isPending lets you show loading indicators during transitions. Use it for tab changes, filtering large lists, or any expensive update triggered by user action. The difference from useDeferredValue: useTransition wraps the update itself, useDeferredValue defers consuming a value. I use useTransition when I control the state update and want it interruptible."],
    },
    // React Advanced
    {
        text: "What is React's reconciliation algorithm and how does it decide what to re-render?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.reconciliation, ValidTag.enum["virtual-dom"], ValidTag.enum.performance],
        answers: ["React's reconciliation is the diffing algorithm that compares the new virtual DOM tree with the previous one to determine what actually needs to update in the real DOM. React makes several assumptions to optimize this - elements of different types will produce different trees, and you can hint which children are stable using keys. When comparing trees, if the root elements are different types, React tears down the old tree and builds a new one. For same-type elements, React keeps the same DOM node and only updates changed attributes. Component updates trigger a re-render of that component and its children by default. Keys are crucial for lists because they help React identify which items changed, were added, or removed. The algorithm is O(n) instead of O(nÂ³) because of these heuristics. Understanding this helps you optimize by avoiding unnecessary type changes and using keys properly.",
            "Reconciliation is React's diffing algorithm comparing new and previous virtual DOM trees. It uses heuristics: different element types produce different trees, and keys identify stable children in lists. Same-type elements reuse DOM nodes with updated attributes. Different types trigger complete subtree rebuilds. Keys in lists help React identify additions, removals, and reorderings efficiently. Without keys, React re-renders entire lists on changes. The algorithm is O(n) thanks to these assumptions. Component updates re-render the component and children unless prevented by memo or shouldComponentUpdate. Understanding this helps optimize by using stable keys and avoiding conditional element type changes."],
    },
    {
        text: "What causes infinite re-render loops and how do you debug them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.debugging],
        answers: ["Infinite re-render loops typically happen when you update state during render, creating a cycle. Common causes include calling setState directly in the component body, or having a useEffect with missing dependencies that updates state and triggers itself again. Another common issue is creating new object or function references during render and using them as dependencies or props. To debug them, first check the error message - React often tells you 'too many re-renders' and points to the component. Add console.logs to see what's triggering renders. Check your useEffect dependencies - the eslint plugin for hooks is invaluable here. Look for setState calls outside of event handlers or effects. Use React DevTools profiler to see what's causing renders. The fix is usually adding proper dependencies, moving state updates to event handlers, or memoizing values with useMemo or useCallback.",
            "Infinite loops occur when state updates during render create cycles. Common causes: setState in component body, useEffect with missing dependencies that updates state triggering itself, or new object/function references in dependencies. Debug by checking React's error message, adding console.logs to trace renders, and reviewing useEffect dependencies. The hooks eslint plugin catches many issues. Fixes include proper dependency arrays, moving state updates to event handlers or effects, memoizing with useMemo/useCallback, and ensuring objects used as dependencies are stable. React DevTools profiler shows what triggers each render."],
    },
    {
        text: "What is StrictMode and why does it cause double-rendering?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["StrictMode is a development-only tool that helps find potential problems in your application by intentionally double-invoking certain functions like function components, useState initializers, and useEffect callbacks. This double-rendering only happens in development and helps catch side effects that aren't properly cleaned up or impure functions that might cause bugs. The idea is that if your component renders twice with the same props and produces different results, you probably have an impure component that's relying on external state incorrectly. It also highlights deprecated APIs and unsafe lifecycle methods. The double-rendering can be confusing at first, but it's actually helping you write more resilient code. Any side effects you see duplicated should be moved to useEffect or event handlers. In production builds, StrictMode does nothing, so there's no performance impact.",
            "StrictMode is development-only and double-invokes components, state initializers, and effects to catch impure code and missing cleanups. If your component behaves differently on second render, you have an impurity issue. It also warns about deprecated APIs and unsafe lifecycles. The double rendering helps verify effects clean up properly and components don't rely on external mutation. Side effects that appear duplicated should move to useEffect with proper cleanup. In production, StrictMode does nothing. I keep it on to catch subtle bugs early since code that works with double-rendering is more resilient."],
    },
    {
        text: "What are portals and when would you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.portals],
        answers: ["Portals let you render children into a DOM node that exists outside the hierarchy of the parent component. You create them with ReactDOM.createPortal, passing the JSX and the target DOM node. This is super useful for modals, tooltips, dropdowns, or popovers where you need the component to visually break out of the parent container, especially when dealing with overflow: hidden or z-index stacking contexts. Even though the component renders outside the parent DOM hierarchy, it still behaves like a normal React child - events bubble up through the React tree, not the DOM tree, and context still works. I use portals whenever I need something to appear above everything else or escape CSS constraints. The classic use case is a modal that needs to render at the document body level to avoid z-index issues, but still receive props and state from deep in the component tree.",
            "Portals render children into a DOM node outside the parent hierarchy using ReactDOM.createPortal(children, domNode). They're essential for modals, tooltips, and dropdowns that need to escape overflow: hidden or z-index stacking contexts. Despite rendering elsewhere in the DOM, portals behave like normal React children. Events bubble through the React tree, and context still works. I use portals for modals rendered at body level to avoid CSS conflicts while maintaining React data flow from deep in the tree. The component stays logically connected to its parent even though it renders elsewhere in the DOM."],
    },
    {
        text: "What is React.memo and when does it actually improve performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.performance, ValidTag.enum.memoization],
        answers: ["React.memo is a higher-order component that memoizes the component and only re-renders when props change. It does a shallow comparison of props by default, or you can provide a custom comparison function. However, it only improves performance in specific scenarios - when the component renders often with the same props, when the component is expensive to render, or when it's a pure component that doesn't depend on context or state. It won't help if props are new objects or functions every render, which is why you often need to combine it with useCallback and useMemo. I don't use React.memo by default everywhere - that actually adds overhead. I only add it after profiling and identifying components that re-render unnecessarily. It's most useful for components deep in the tree that receive stable props from higher up, or for items in large lists.",
            "React.memo memoizes components, skipping re-renders when props haven't changed. It uses shallow comparison by default or a custom comparator. It only helps when the component renders often with same props and is expensive enough to justify the comparison overhead. Won't help if props are new objects each render since they fail shallow comparison. Combine with useCallback for functions and useMemo for objects. I don't apply memo everywhere since it adds overhead. I profile first, then add it to components that re-render unnecessarily and are expensive. Most useful for list items and components deep in the tree with stable props."],
    },
    {
        text: "What are render props and when would you use them over hooks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Render props is a pattern where a component takes a function as a prop and calls it to determine what to render, allowing you to share code between components. Before hooks, this was a primary way to share stateful logic. The component handles the logic and state, then calls the render prop function with that data. While hooks have mostly replaced render props for sharing logic, render props are still useful in specific cases - when you need more control over rendering, when building library components that need maximum flexibility, or when the component needs to render multiple times with different data. For example, a virtualized list might use render props to let consumers control how each item renders. Hooks are generally simpler and more composable, but render props can be more explicit about data flow and give consumers fine-grained control over rendering.",
            "Render props pass a function as a prop that the component calls to determine what to render. The component manages logic and state, then passes that data to the render function. Before hooks, this was the primary way to share stateful logic. While hooks have largely replaced this pattern, render props still make sense when consumers need full control over rendering, for library components needing maximum flexibility, or when rendering multiple times with different data. Virtualized lists often use render props for item rendering control. Hooks are simpler for logic sharing, but render props offer explicit data flow and rendering control."],
    },
    {
        text: "What are compound components and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["design-patterns"]],
        answers: ["Compound components is a pattern where multiple components work together to form a complete UI, sharing implicit state through context. Think of HTML select and option elements - they work together as a unit. You implement this by creating a parent component that manages state and provides it via context, and child components that consume that context. For example, a Tabs component might have Tabs.List, Tabs.Tab, and Tabs.Panel subcomponents that all share selection state. The benefit is a flexible, expressive API - users can compose the components however they want while the internal state stays synchronized. Libraries like Radix UI and Headless UI use this pattern extensively. I implement compound components when building complex UI elements that need flexibility in structure but shared behavior. The key is using context to share state implicitly rather than requiring users to wire everything together with props.",
            "Compound components work together sharing implicit state via context, like HTML select and option. A parent manages state and provides context, children consume it. For Tabs: the parent tracks selection, Tab.List renders tabs, Tab.Panel renders content, all synced through context. Users compose freely while state stays synchronized. This creates expressive APIs without prop drilling. Libraries like Radix and Headless UI use this extensively. I implement compound components for complex widgets needing flexible composition but coordinated behavior. The key is implicit state sharing so consumers don't manually wire everything."],
    },
    {
        text: "What is the children prop and how do you manipulate it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["The children prop is a special prop in React that represents the content between component's opening and closing tags. It can be anything - a string, an element, an array of elements, or even a function. React provides React.Children utilities to work with children safely since children's structure can vary. React.Children.map lets you iterate and transform children, React.Children.count counts them, React.Children.only ensures there's exactly one child, and React.Children.toArray converts children to a flat array with keys. You can also clone children with React.cloneElement to add props. Common use cases include wrapping children with extra elements, filtering certain children, or injecting props into children. However, I try to avoid heavy children manipulation because it can make components harder to understand. When I need complex composition, I often use render props or compound components instead.",
            "The children prop represents content between component tags. It can be strings, elements, arrays, or functions. React.Children utilities handle varying structures safely: map to iterate and transform, count for length, only to assert single child, toArray for flat arrays with keys. React.cloneElement adds props to children. Common uses include wrapping children, filtering, or injecting props. However, heavy children manipulation makes components harder to follow. I prefer render props or compound components for complex composition since they're more explicit. Children manipulation works best for simple wrapping or layout components."],
    },
    {
        text: "What is React.lazy and how do you use it with Suspense?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.suspense, ValidTag.enum["lazy-loading"], ValidTag.enum.performance],
        answers: ["React.lazy lets you dynamically import components, enabling code splitting so you only load components when they're needed. You wrap a dynamic import in React.lazy, and it returns a component that can be rendered. You must wrap lazy components in a Suspense boundary that provides a fallback UI while the component loads. For example, React.lazy(() => import('./HeavyComponent')) will create a separate bundle for that component. Suspense shows the fallback until the component loads, then renders the actual component. This is great for reducing initial bundle size, especially for routes or features that aren't immediately needed. I use it for route-based code splitting and for heavy components like charts or editors that not all users will need. The key is finding the right granularity - too much splitting can actually hurt performance with too many network requests.",
            "React.lazy enables code splitting by dynamically importing components. Wrap a dynamic import: React.lazy(() => import('./Component')). This creates separate bundles loaded on demand. Lazy components require Suspense wrappers with fallback UI during loading. Great for reducing initial bundle size with route-based splitting or lazy-loading heavy components like charts. I split routes and features not immediately needed. Too fine-grained splitting creates too many requests, so find the right balance. Bundlers automatically create chunks for dynamic imports, making this straightforward to implement."],
    },
    {
        text: "What is forwardRef and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["forwardRef is a React API that lets function components accept a ref and forward it to a child element or component. Normally, function components can't receive refs because they're not instances. forwardRef solves this by wrapping your component and giving it a second parameter for the ref. This is essential when building reusable component libraries where consumers need direct access to DOM elements - like focusing an input, measuring an element, or integrating with third-party libraries. For example, a custom Input component would use forwardRef so parent components can call .focus() on it. You often use it with useImperativeHandle to control exactly what the ref exposes. I use forwardRef whenever I'm building a component that wraps a native element and consumers might need DOM access, or when building component libraries where ref forwarding is expected.",
            "forwardRef allows function components to receive and forward refs. Without it, refs can't pass through function components. Wrap your component with forwardRef to receive the ref as a second parameter, then attach it to a DOM element or use with useImperativeHandle. Essential for reusable components where consumers need DOM access, like focusing inputs, measuring elements, or third-party library integration. Custom Input components should forward refs so parents can focus them. I use forwardRef for any component wrapping native elements where ref access is expected."],
    },
    {
        text: "What are custom hooks and what makes a good custom hook?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum["custom-hooks"]],
        answers: ["Custom hooks are JavaScript functions that start with 'use' and can call other hooks, letting you extract and reuse stateful logic across components. A good custom hook has a single, clear purpose and returns exactly what consumers need - nothing more, nothing less. It should be composable, follow the rules of hooks, and have a clear API. Good examples include useLocalStorage for syncing state with localStorage, useDebounce for debouncing values, or useMediaQuery for responsive logic. A custom hook should abstract complexity while remaining flexible. I avoid hooks that are too specific to one component or that return too much - if consumers only use half of what it returns, it's probably doing too much. The best custom hooks feel obvious in hindsight and make the consuming code cleaner and more readable. They should handle their own cleanup and edge cases so consumers don't have to worry about implementation details.",
            "Custom hooks are functions starting with 'use' that can call other hooks, extracting reusable stateful logic. Good hooks have single clear purposes, return exactly what's needed, and handle cleanup internally. Examples: useLocalStorage for persisted state, useDebounce for debouncing, useMediaQuery for responsive logic. Avoid hooks too specific to one component or returning too much. The best hooks feel obvious and make consuming code cleaner. They should be composable and follow hook rules. I create custom hooks when the same logic appears in multiple components or when it would make a component cleaner by extracting complexity."],
    },
    {
        text: "What is the difference between optimistic and pessimistic UI updates?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Optimistic UI updates assume the operation will succeed and update the UI immediately before getting server confirmation, while pessimistic updates wait for the server response before updating. Optimistic updates make the app feel instant and responsive, but you need to handle rollback if the operation fails. Pessimistic updates are safer and simpler since you only update on success, but they feel slower to users. For example, with an optimistic like button, you'd increment the count immediately and show the filled heart, then rollback if the request fails. With pessimistic, you'd wait for the server, leaving the UI unchanged until success. I use optimistic updates for actions that rarely fail and have clear rollback states - like likes, simple form submissions, or reordering items. For critical operations like payments or deletes, I use pessimistic updates. Libraries like TanStack Query make optimistic updates easier with built-in rollback support.",
            "Optimistic updates change the UI immediately before server confirmation, assuming success. Pessimistic updates wait for the server. Optimistic feels instant but requires rollback handling on failure. Pessimistic is safer but feels slower. Use optimistic for actions that rarely fail with clear rollback states like likes, toggles, or list reordering. Use pessimistic for critical operations like payments or destructive actions. With optimistic, you update state immediately, then rollback on error. TanStack Query has built-in support with onMutate for caching previous state and onError for rollback."],
    },
    {
        text: "What is hydration and what causes hydration mismatches?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.ssr, ValidTag.enum.nextjs],
        answers: ["Hydration is when React attaches event listeners and makes the server-rendered HTML interactive on the client. React expects the client-rendered output to match the server-rendered HTML exactly. Hydration mismatches occur when they differ, causing React to warn or even re-render from scratch. Common causes include using browser-only APIs like window during render, different data between server and client, random values or timestamps, or content that depends on client state like localStorage. Third-party scripts that modify the DOM can also cause mismatches. To fix them, use useEffect for browser-only code, ensure data is consistent, use suppressHydrationWarning for intentionally different content like timestamps, or use two-pass rendering where you render a placeholder on the server and the real content after hydration. Hydration errors can hurt performance since React might have to throw away the server HTML and re-render everything.",
            "Hydration attaches event listeners to server-rendered HTML, making it interactive. React expects server and client output to match exactly. Mismatches occur from browser-only APIs like window accessed during render, different server/client data, random values, or client state like localStorage. Third-party DOM modifications also cause issues. Fix by using useEffect for browser-only code, ensuring consistent data, using suppressHydrationWarning for intentional differences like timestamps, or two-pass rendering with placeholder then real content. Mismatches hurt performance since React may discard server HTML entirely."],
    },
    {
        text: "What are concurrent features in React 18?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["concurrent-mode"]],
        answers: ["Concurrent features in React 18 allow React to work on multiple versions of the UI at the same time, pausing and resuming work as needed. The key features are useTransition for marking updates as non-urgent, useDeferredValue for deferring less important updates, Suspense for loading states, and automatic batching. Unlike the old synchronous rendering where React had to finish what it started, concurrent rendering can interrupt work to handle higher priority updates. This makes apps feel more responsive because urgent updates like typing or clicking aren't blocked by expensive renders. The mental model is that React can keep a low-priority render in memory while handling a high-priority update, then resume or discard the low-priority work. You opt into concurrent features by using createRoot instead of render and using the new hooks. It's backward compatible - existing code works fine, you just don't get the benefits until you adopt the new APIs.",
            "Concurrent features in React 18 let React work on multiple UI versions simultaneously, pausing and resuming as needed. Key features: useTransition for non-urgent updates, useDeferredValue for deferring updates, Suspense for loading states, and automatic batching everywhere. Unlike synchronous rendering, React can interrupt expensive work for urgent updates like user input. React keeps low-priority renders in memory while handling high-priority updates. Opt in with createRoot instead of render. It's backward compatible but you only get benefits using the new APIs. This makes apps feel snappier since expensive renders don't block urgent interactions."],
    },
    {
        text: "What is automatic batching in React 18?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.performance],
        answers: ["Automatic batching is a React 18 feature that groups multiple state updates into a single re-render for better performance. Before React 18, batching only happened in event handlers, but now it happens everywhere - in promises, setTimeout, native event handlers, and any other context. This means if you have multiple setState calls in an async function or callback, React will batch them automatically instead of re-rendering for each one. For example, if you fetch data and update multiple pieces of state, React batches those updates into one render. The performance benefit is significant since you avoid unnecessary renders. In rare cases where you need synchronous updates, you can use ReactDOM.flushSync to opt out of batching. I generally don't have to think about this - it just makes React faster by default. The only time you might notice is if you had code relying on the old synchronous behavior, which is rare.",
            "Automatic batching in React 18 groups multiple state updates into single re-renders everywhere, not just event handlers. Before 18, promises, setTimeout, and native events caused separate re-renders for each setState. Now all updates batch automatically. Fetching data and updating multiple state pieces triggers one render instead of several. Use ReactDOM.flushSync if you need synchronous updates, though that's rare. This is a free performance improvement since it just works. The only consideration is code that relied on old synchronous behavior, which is uncommon."],
    },

    // State Management
    {
        text: "When would you choose Redux vs React Context vs Zustand vs TanStack Query?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum.zustand, ValidTag.enum["context-api"], ValidTag.enum["tanstack-query"], ValidTag.enum.react],
        answers: ["Each has different use cases. TanStack Query is specifically for server state - data fetching, caching, and synchronization - so I reach for it first when dealing with API data. React Context is great for simple state that doesn't change often, like themes or auth, but causes re-renders of all consumers. Zustand is my go-to for complex client state - it's simpler than Redux with less boilerplate, good performance, and works outside React. Redux with Redux Toolkit is best for very large apps that need time-travel debugging, strict patterns, or have complex state interactions that benefit from the Redux DevTools. I typically use TanStack Query for all server data, Zustand for global client state like UI preferences or app-wide modals, Context for dependency injection or truly infrequent changes, and Redux only when specifically needed. Often I combine them - TanStack Query for server state and Zustand for everything else works really well.",
            "Each tool has distinct purposes. TanStack Query handles server state, so I use it for all API data with its caching and synchronization. Context works for infrequent changes like themes since all consumers re-render on changes. Zustand is my go-to for complex client state with simpler API than Redux and good performance. Redux with Toolkit suits large apps needing time-travel debugging or complex state interactions. I combine them: TanStack Query for server data, Zustand for global client state, Context for dependency injection. Redux only when its debugging tools or strict patterns are specifically needed."],
    },
    {
        text: "What are Redux Toolkit slices and how do they simplify Redux?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum["redux-toolkit"]],
        answers: ["Redux Toolkit slices are a way to define a piece of Redux state along with its reducers and actions in one place using createSlice. Instead of writing action types, action creators, and reducers separately, you define them together with reducer functions that can 'mutate' state directly - RTK uses Immer under the hood to make that safe. For example, you create a slice with a name, initial state, and reducers object, and it automatically generates action creators and action types. This dramatically reduces boilerplate and makes Redux code much more maintainable. Slices also encourage organizing your state by feature rather than by type of code. Redux Toolkit also includes createAsyncThunk for async logic and configureStore that sets up good defaults. I find Redux Toolkit makes Redux actually pleasant to use, whereas classic Redux had so much ceremony it was often not worth it.",
            "Slices combine reducer, actions, and action types in one place using createSlice. You define name, initial state, and reducers that can mutate state directly since Immer handles immutability. Actions are auto-generated from reducer names. This eliminates boilerplate dramatically. Organize by feature with each slice containing its domain logic. RTK also provides createAsyncThunk for async and configureStore with good defaults. Redux Toolkit makes Redux maintainable whereas classic Redux had excessive ceremony. I use slices to keep state logic organized and reducers readable."],
    },
    {
        text: "What is Redux middleware and how does Redux Thunk differ from Redux Saga?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum["redux-toolkit"]],
        answers: ["Redux middleware intercepts actions before they reach reducers, letting you add side effects, logging, async logic, or modify actions. Redux Thunk is the simplest middleware - it lets action creators return functions instead of actions, and those functions receive dispatch and getState. It's great for simple async logic like API calls. Redux Saga uses generator functions and an effects system to handle complex async flows. Sagas are more powerful for things like debouncing, retries, race conditions, or coordinating multiple actions, but they have a steeper learning curve. Thunk is imperative - you write normal async/await code. Saga is declarative - you yield effects that describe what should happen. I use Thunk for most cases since it's simpler and Redux Toolkit includes it by default. Saga makes sense for complex workflows like multi-step forms or when you need to cancel operations, but that's rare in my experience.",
            "Middleware intercepts actions before reducers for side effects, logging, or async logic. Thunk lets action creators return functions receiving dispatch and getState, great for simple async with normal async/await code. Saga uses generators with declarative effects for complex flows like debouncing, retries, or race conditions. Thunk is imperative and simpler, included by default in RTK. Saga has steeper learning curve but handles complex workflows better. I use Thunk for most cases. Saga only when I need its advanced patterns like cancellation or coordinating multiple actions, which is uncommon."],
    },
    {
        text: "What is the difference between normalized and denormalized state?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Normalized state stores entities by ID in a flat structure, avoiding duplication and making updates easier. Denormalized state nests data naturally, matching the API response structure. For example, normalized state might store users and posts separately in objects keyed by ID, while denormalized keeps posts nested within users. Normalization prevents inconsistencies when the same entity appears in multiple places - you update it once and it's updated everywhere. It also makes it easier to update, delete, or find specific entities. The tradeoff is that you need to do more work to select related data, often using selectors. Denormalized state is simpler to work with initially but can lead to bugs when you update one copy and forget others. I normalize state when entities are referenced in multiple places or when I need to update them frequently. For read-heavy, simple data structures, denormalized is fine. Redux Toolkit has createEntityAdapter to help with normalization.",
            "Normalized state stores entities by ID in flat structures, avoiding duplication. Denormalized nests data matching API structure. Normalization means updating once updates everywhere since there's no duplication. It simplifies updates, deletes, and lookups. The tradeoff is selectors to reassemble related data. Denormalized is simpler initially but leads to inconsistencies when updating one copy but not others. I normalize when entities appear in multiple places or need frequent updates. For simple read-heavy data, denormalized works fine. RTK's createEntityAdapter helps with normalized patterns."],
    },
    {
        text: "What are selectors and why are they important for performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum.performance],
        answers: ["Selectors are functions that extract specific pieces of derived state from the store. They encapsulate state shape knowledge, making it easier to refactor state structure. More importantly, memoized selectors with libraries like Reselect prevent unnecessary recalculations and re-renders. A memoized selector only recomputes when its inputs change, so if you have an expensive transformation like filtering or sorting a large array, it won't run on every render. Without memoization, you'd recreate that computed value every time, potentially causing performance issues. Selectors also let you keep your state minimal by deriving values rather than storing them. For example, instead of storing filtered and sorted data, you store the raw data and selection criteria, then derive the filtered list with a selector. I use Reselect or RTK's createSelector for any computed state that's used in components, especially when the computation is expensive or when the component renders frequently.",
            "Selectors extract derived state from the store, encapsulating state shape knowledge for easier refactoring. Memoized selectors with Reselect or RTK's createSelector only recompute when inputs change, preventing expensive recalculations on every render. Keep state minimal and derive values instead of storing them. Store raw data and filter criteria, then derive filtered lists with selectors. I use createSelector for any computed state used in components, especially expensive operations like filtering large arrays. Memoization is key since without it you'd recreate computed values on every render."],
    },
    {
        text: "What is state colocation and why does it matter?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["State colocation means keeping state as close as possible to where it's used, rather than lifting it to a global store or high up in the tree. This improves performance because fewer components re-render when that state changes, makes code easier to understand since the state is near its usage, and makes components more reusable since they don't depend on global state shape. The general principle is to start with local state and only lift it up when multiple components actually need it. Too often developers put everything in Redux or global state when most state could be local. For example, form state, UI state like modal open/closed, or local filters should usually be component state. I only lift state when it's genuinely shared or needs to persist across unmounts. Kent C. Dodds has a great article on this - the idea is that global state has real costs in terms of complexity and performance, so you should only pay that cost when necessary.",
            "State colocation keeps state as close as possible to its usage rather than lifting it globally. This improves performance since fewer components re-render, makes code clearer since state is near usage, and improves reusability since components don't depend on global state shape. Start with local state and only lift when multiple components genuinely need it. Form state, modal open/closed, and local filters should be component state. Too often everything ends up in Redux when it could be local. Global state has costs in complexity and performance, so only pay when necessary. I lift state only when it's truly shared."],
    },

    // TanStack Query Advanced
    {
        text: "What is optimistic updates and how do you implement them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Optimistic updates in TanStack Query let you update the UI immediately before the mutation completes, then rollback if it fails. You implement them using the onMutate callback to save a snapshot of current data and update the cache, onError to rollback using that snapshot, and onSettled to invalidate and refetch to ensure consistency. In onMutate, you cancel outgoing queries with queryClient.cancelQueries, save the previous value with getQueryData, and set the optimistic value with setQueryData. If the mutation fails, onError restores the previous value. This pattern makes the UI feel instant while maintaining data integrity. The key is properly handling the rollback case - users should clearly see if an action failed. I use optimistic updates for high-frequency actions like likes, votes, or toggling states where the success rate is high and the user expects immediate feedback.",
            "Optimistic updates in TanStack Query update the cache immediately before mutation completes. In onMutate: cancel outgoing queries, save previous value with getQueryData, set optimistic value with setQueryData. In onError: rollback using saved snapshot. In onSettled: invalidate to ensure consistency. This makes the UI feel instant while handling failures gracefully. The key is proper rollback handling so users clearly see failures. I use optimistic updates for likes, votes, and toggles where success rate is high and users expect immediate feedback."],
    },
    {
        text: "What is infinite queries and how do you implement pagination?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Infinite queries in TanStack Query handle paginated data where you load more as the user scrolls, like an infinite scroll feed. You use useInfiniteQuery instead of useQuery and provide a getNextPageParam function that determines the next page parameter from the last page of data. The hook returns pages as a flat array, plus fetchNextPage and hasNextPage helpers. Each page is cached independently, and TanStack Query manages loading more pages. For cursor-based pagination, getNextPageParam returns the next cursor from the response. For offset-based, it calculates the next offset. You can also implement getPreviousPageParam for bidirectional scrolling. I typically combine this with an intersection observer to trigger fetchNextPage when the user nears the bottom. The benefit over manual pagination is that TanStack Query handles caching, refetching, and state management automatically.",
            "useInfiniteQuery handles paginated data loaded incrementally. Provide getNextPageParam to determine the next page from the last response. It returns pages array, fetchNextPage function, and hasNextPage boolean. Pages are cached independently. For cursor pagination, return next cursor from response. For offset, calculate next offset. getPreviousPageParam enables bidirectional scrolling. I combine with intersection observer to trigger fetchNextPage near the bottom. TanStack Query handles caching, refetching, and state automatically. Much simpler than manual pagination with all the same benefits of query caching."],
    },
    {
        text: "What is prefetching and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.performance, ValidTag.enum.prefetching],
        answers: ["Prefetching in TanStack Query means loading data before it's needed so it's available instantly when requested. You use queryClient.prefetchQuery to load data without subscribing to it. Common use cases include prefetching on hover for things like tooltips or detail views, prefetching the next page of paginated data, or prefetching data for routes the user is likely to visit. The prefetched data goes into the cache with the same cache time rules, so it can become stale. Prefetching is different from initial data - prefetching makes a real request while initial data provides temporary data. I use prefetching for predictable user flows, like hovering over a product card to prefetch its details, or automatically prefetching the next page when displaying a list. The key is not to prefetch too aggressively, which wastes bandwidth and server resources.",
            "Prefetching loads data before it's needed using queryClient.prefetchQuery. Data goes into cache following normal stale/cache rules. Use for hover prefetch on links or cards, prefetching next pagination page, or likely routes. Unlike initialData which is temporary, prefetching makes real requests. I prefetch on hover for detail views or automatically prefetch the next page. The key is not being too aggressive since it wastes bandwidth. Prefetch predictable user flows where you're confident the data will be needed."],
    },
    {
        text: "What is the difference between refetch and invalidate?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Refetch and invalidate serve different purposes in TanStack Query. Refetch immediately re-runs queries and always makes a network request, regardless of whether data is stale. You call it with query.refetch() or queryClient.refetchQueries(). Invalidate marks queries as stale so they'll refetch on the next render or when components mount, but doesn't immediately fetch unless there are active observers. You use queryClient.invalidateQueries(). Invalidate is generally better because it's more efficient - it only refetches queries that are currently being used. Refetch is useful when you need to force an immediate update, like after a user action or in response to a websocket message. I use invalidateQueries after mutations to mark related data as stale, and refetch for explicit user-triggered refreshes like a pull-to-refresh gesture.",
            "Refetch immediately runs queries regardless of stale status. Invalidate marks as stale and only refetches if there are active observers. Use invalidateQueries after mutations since it only fetches actively used data. Use refetch for explicit user refreshes. Invalidate is smarter and more efficient. The key difference: refetch always makes a request, invalidate is conditional on active observers. I default to invalidate after mutations and reserve refetch for user-triggered actions."],
    },
    {
        text: "How does garbage collection work in TanStack Query?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.performance],
        answers: ["TanStack Query automatically cleans up unused query data through garbage collection. When a query has no active observers - meaning no components are using it - it becomes inactive. After the cacheTime duration (default 5 minutes), the query data is garbage collected and removed from memory. This prevents memory leaks in long-running applications. You can configure cacheTime per query - setting it to Infinity keeps data forever, while 0 removes it immediately when inactive. Stale time is separate from cache time - staleTime determines when to refetch, while cacheTime determines when to remove from memory. I usually keep the default cache time, but might increase it for data that's expensive to fetch and doesn't change often, or decrease it for sensitive data that shouldn't stay in memory.",
            "Garbage collection removes unused query data from memory. When a query has no observers, it becomes inactive. After cacheTime expires, 5 minutes by default, it's removed. This prevents memory leaks in long-running apps. cacheTime of Infinity keeps forever, 0 removes immediately. Separate from staleTime which controls refetching. I use default cache time usually, increase for expensive stable data, decrease for sensitive data that shouldn't persist."],
    },
    {
        text: "What is placeholder data vs initial data?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Placeholder data and initial data are both ways to provide data before a query loads, but they work differently. Initial data is treated as real data - it goes into the cache at the specified dataUpdatedAt timestamp and follows normal stale/cache rules. It won't refetch unless it's stale. Placeholder data is temporary fake data that's shown while the real query runs - it doesn't go into the cache and the query still runs in the background. Initial data is useful when you have real data from SSR, another query, or localStorage. Placeholder data is for showing skeleton structures or default values while loading. For example, you might use placeholder data to show empty arrays for lists while loading, or initial data to reuse data from a list view when showing a detail view. I use initialData when I have actual data to provide, and placeholderData when I just want to avoid loading states.",
            "InitialData is treated as real cached data with normal stale/cache rules. PlaceholderData is temporary while the query runs in background. InitialData goes into the cache, placeholder doesn't. Use initialData for SSR data, data from other queries, or localStorage. Use placeholderData for skeleton structures or default values during loading. I use initialData when I have actual data like from list to detail navigation, placeholderData when I just want to avoid loading spinners."],
    },
    {
        text: "How do you handle offline support?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["TanStack Query has built-in offline support through its caching and retry mechanisms. Queries automatically use cached data when offline, and mutations can be paused and resumed when connectivity returns. You configure this with networkMode option - 'online' only runs when online, 'always' runs regardless, and 'offlineFirst' tries the request but falls back to cache. For mutations, you can use mutation.persist() to store them in localStorage and retry when back online. The library also integrates with the browser's online/offline events to pause and resume operations. For a fully offline-first app, I typically combine TanStack Query with a service worker for offline caching, use optimistic updates for mutations, and configure appropriate cache times. The onlineManager lets you customize online detection logic. The key is deciding whether each operation should fail, queue, or use cached data when offline.",
            "TanStack Query supports offline through caching and retry. Queries use cache when offline. networkMode controls behavior: 'online' only runs online, 'always' runs regardless, 'offlineFirst' tries then falls back to cache. Mutations can pause and resume when connectivity returns. persist() stores them in localStorage. The library integrates with browser online/offline events. For offline-first apps, combine with service workers and optimistic updates. Decide per operation whether to fail, queue, or use cached data when offline."],
    },

    // Next.js Advanced
    {
        text: "What is the _document.js file and when would you customize it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"]],
        answers: ["The _document.js file in Next.js lets you customize the HTML document structure that wraps your application. It only runs on the server and allows you to modify the html and body tags, add meta tags that should be on every page, or inject third-party scripts. You'd customize it to add a lang attribute to the html tag, include fonts from external CDNs, add analytics scripts that need to be in the head, or integrate with CSS-in-JS libraries that need server-side setup. Unlike _app.js which runs on every page navigation, _document only runs during the initial server render. You extend the Document class and can customize the Html, Head, Main, and NextScript components. I typically only customize _document when I need to modify the base HTML structure or add global scripts - for most styling and layout, _app.js is the better choice.",
            "_document.js customizes the HTML document structure, only running on the server during initial render. Use it to add lang to html tag, include external fonts, add analytics scripts, or integrate CSS-in-JS server setup. Unlike _app.js which runs on navigation, _document only runs once. Extend Document and customize Html, Head, Main, NextScript components. I only modify _document for base HTML changes or global scripts. For styling and layout, _app.js is more appropriate."],
    },
    {
        text: "How do you handle redirects and rewrites?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs],
        answers: ["Next.js handles redirects and rewrites in next.config.js. Redirects send users to a different URL and change the URL in the browser, while rewrites proxy to a different URL but keep the original URL visible. Both support pattern matching with parameters and wildcards. Redirects are useful for moved content, forcing www or https, or redirecting old URLs to new ones. Rewrites are great for API proxying to avoid CORS, implementing i18n routing, or A/B testing. You can also do redirects in getServerSideProps or middleware for dynamic logic. Permanent redirects return 308, temporary ones return 307. I use redirects for SEO-friendly URL changes and rewrites when I want to hide the implementation details, like proxying API requests through the Next.js server to hide API keys or avoid CORS issues.",
            "Redirects send users to different URLs and change browser URL. Rewrites proxy to different URLs while keeping original URL visible. Configure in next.config.js with pattern matching. Redirects for moved content or forcing www/https. Rewrites for API proxying to avoid CORS or A/B testing. Dynamic redirects in getServerSideProps or middleware. Permanent redirects are 308, temporary 307. I use redirects for SEO-friendly URL changes, rewrites to hide implementation like API proxying."],
    },
    {
        text: "What is middleware in Next.js and what are valid use cases?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.middleware],
        answers: ["Next.js middleware runs before a request completes, letting you modify the response, redirect, rewrite, or set headers. It runs at the edge, before cached content or routes are matched, making it very fast. You create a middleware.js file and export a middleware function that receives a request and can return a response. Valid use cases include authentication checks, A/B testing by rewriting to different pages, internationalization routing, bot detection and blocking, adding security headers, logging and analytics, or redirecting based on geolocation. The key limitation is that middleware runs in a restricted runtime - you can't use Node.js APIs or large dependencies. I use middleware for authentication gates, setting security headers, and simple redirects. For complex logic, I'd use getServerSideProps or API routes instead since they have full Node.js access.",
            "Middleware runs before requests complete, at the edge for speed. Create middleware.js exporting a function that receives request and can return response. Use for auth checks, A/B testing, i18n routing, bot blocking, security headers, or geolocation redirects. Key limitation: restricted Edge runtime without full Node.js APIs. I use middleware for auth gates and security headers. For complex logic needing Node.js, use getServerSideProps or API routes."],
    },
    {
        text: "How do you debug large Next.js bundle sizes?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.debugging, ValidTag.enum.bundling],
        answers: ["To debug bundle sizes in Next.js, I start by running next build which shows bundle sizes for each page. For deeper analysis, I use @next/bundle-analyzer which generates an interactive treemap showing what's in each bundle. Common culprits are large libraries like moment.js or lodash being fully imported instead of tree-shaken, duplicate dependencies from different versions, or forgetting to dynamically import heavy components. I check for client-side imports of server-only code, ensure next/dynamic is used for heavy components, and verify tree-shaking is working by using named imports. The webpack Bundle Analyzer helps identify which specific packages are large. I also check if moment.js can be replaced with date-fns or day.js, if lodash can be replaced with lodash-es for better tree-shaking, and if any dependencies can be removed entirely. Setting the source map to 'hidden-source-map' in production also reduces bundle size.",
            "Start with next build to see page bundle sizes. Use @next/bundle-analyzer for interactive treemap visualization. Common issues: moment.js or lodash fully imported, duplicate dependencies, missing dynamic imports for heavy components. Check for client imports of server code. Use named imports for tree-shaking. Replace moment with date-fns or day.js, lodash with lodash-es. Use next/dynamic for heavy components. I verify tree-shaking works and remove unnecessary dependencies."],
    },
    {
        text: "What is the difference between fallback: false, fallback: true, and fallback: 'blocking'?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.ssg],
        answers: ["These options in getStaticPaths determine how Next.js handles paths not returned at build time. fallback: false returns 404 for any path not in the paths array - use this when you have all possible paths at build time. fallback: true immediately serves a fallback page while generating the real page in the background, then swaps it in - you need to handle the loading state with router.isFallback. fallback: 'blocking' waits to serve the page until it's fully generated, like getServerSideProps but only runs once then caches - no fallback UI needed. I use fallback: false for small, finite sets like a few marketing pages, fallback: true for large sets where I want instant navigation with a loading state, and fallback: 'blocking' when I can't show a fallback UI or when SEO is critical since the crawler gets the full page immediately.",
            "fallback: false returns 404 for paths not in build-time list. fallback: true serves fallback immediately while generating page, handle with router.isFallback. fallback: 'blocking' waits until page generates, like SSR but caches after. Use false for finite path sets. Use true for large sets with loading states. Use 'blocking' when you can't show fallback or need SEO since crawlers get full page. I choose based on path count and whether showing loading state is acceptable."],
    },
    {
        text: "How do you handle authentication in the Pages Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.auth],
        answers: ["Authentication in the Pages Router can happen at different layers. For client-side protection, I check auth state in useEffect and redirect if not authenticated - this is simple but shows a flash of protected content. For server-side, I use getServerSideProps to check cookies or tokens and redirect to login if missing - this prevents any flash but runs on every request. Middleware is great for protecting multiple routes at once by checking auth before the page loads. For API routes, I verify tokens in each route handler. NextAuth.js is the most popular solution - it handles sessions with JWTs or database sessions, supports many providers, and works well with the Pages Router. I typically combine approaches: middleware for route protection, NextAuth for session management, and getServerSideProps when I need to fetch user-specific data. The key is never trusting client-side auth checks alone - always verify on the server or in API routes.",
            "Auth happens at multiple layers. Client-side in useEffect is simple but shows flash of content. getServerSideProps checks cookies and redirects, no flash but runs per request. Middleware protects multiple routes at once. API routes verify tokens in handlers. NextAuth.js handles sessions with JWTs or database, supports many providers. I combine: middleware for route protection, NextAuth for session management, getServerSideProps for user-specific data. Never trust client-side auth alone, always verify server-side."],
    },
    {
        text: "How does the App Router differ from the Pages Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["pages-router"]],
        answers: ["The App Router is a fundamental redesign built on React Server Components. The key differences are file-based routing in an app directory with folders instead of files determining routes, Server Components by default instead of client components, built-in layouts and templates, streaming and Suspense support, and new data fetching with async components instead of getServerSideProps. The App Router has colocation - you can put components, tests, and styles in route folders. Loading and error states are file-based with loading.js and error.js. Metadata is handled with a new API instead of Head components. Caching is more aggressive and automatic. The mental model is different - components are server-rendered by default and you opt into client rendering with 'use client'. I use the App Router for new projects to get better performance through streaming and less JavaScript shipped to the client, but the Pages Router is still fully supported and fine for existing apps.",
            "App Router is built on Server Components with folders for routes. Key differences: Server Components by default, built-in layouts and templates, streaming and Suspense, async components for data fetching instead of getServerSideProps. Colocation puts components and tests in route folders. loading.js and error.js for states. New metadata API. More aggressive caching. Components are server by default, opt into client with 'use client'. I use App Router for new projects for better performance and less client JS."],
    },
    {
        text: "What are Server Components vs Client Components?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["server-components"], ValidTag.enum.rsc],
        answers: ["Server Components render on the server and send HTML to the client, while Client Components are the traditional React components that hydrate and run in the browser. Server Components can access databases directly, don't add to the client bundle, and can use server-only packages. They can't use hooks, browser APIs, or event handlers. Client Components can do all of those but add to the bundle size. The default in the App Router is Server Components - you add 'use client' to make something a Client Component. You can pass Server Components as children to Client Components, which is a powerful pattern. I use Server Components for layouts, static content, and data fetching, and Client Components only when I need interactivity, hooks, or browser APIs. The benefit is less JavaScript shipped to users and faster initial page loads since more work happens on the server.",
            "Server Components render on server, send HTML, don't add to bundle, can access databases directly. Can't use hooks, browser APIs, or events. Client Components hydrate and run in browser with full React features but add bundle size. Default is Server Components, add 'use client' for Client. Server Components can be children of Client Components. I use Server for layouts and data fetching, Client only for interactivity. Less client JS and faster initial loads."],
    },
    {
        text: "What is the use client directive and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum["server-components"]],
        answers: ["The 'use client' directive marks a boundary between Server and Client Components. You put it at the top of a file to indicate that component and everything it imports should be Client Components. You need it when using React hooks like useState or useEffect, when handling browser events like onClick, when using browser-only APIs like localStorage or window, when using libraries that depend on browser APIs, or when using React features like Context or createContext. You don't need it for components that just render props and children. The key insight is to push 'use client' as far down the tree as possible - make leaf interactive components Client Components while keeping layouts and wrappers as Server Components. This minimizes the client bundle. I typically create small Client Components for interactive pieces like buttons or forms, and keep everything else as Server Components by default.",
            "'use client' marks components as Client Components. Need it for hooks, events, browser APIs, or libraries needing browser. Don't need for components just rendering props and children. Push 'use client' as far down as possible to minimize bundle. Make leaf interactive components client, keep layouts server. I create small Client Components for buttons and forms, everything else stays Server by default."],
    },
    {
        text: "What are loading.js and error.js files?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"]],
        answers: ["loading.js and error.js are special files in the App Router that handle loading and error states automatically. loading.js wraps the page in a Suspense boundary and shows while the page is loading - it enables instant loading states and streaming. error.js wraps the page in an Error Boundary and catches errors during rendering, making error handling declarative. Both are scoped to their route segment and nested routes inherit them. This means you can have different loading and error UIs for different parts of your app without manually wrapping everything in Suspense or Error Boundaries. loading.js receives no props, while error.js receives the error and a reset function. I use loading.js to show skeletons while data loads, and error.js to show user-friendly error messages with retry buttons. This pattern makes error handling and loading states much more manageable than manually adding them everywhere.",
            "loading.js wraps pages in Suspense showing during load. error.js wraps in Error Boundary catching render errors. Both scoped to route segment, nested routes inherit them. Different loading and error UIs per section without manual wrapping. loading.js has no props, error.js receives error and reset function. I use loading.js for skeletons, error.js for friendly error messages with retry. Much cleaner than manual Suspense and Error Boundary everywhere."],
    },
    {
        text: "How does the App Router handle layouts and nested layouts?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"]],
        answers: ["Layouts in the App Router are components that wrap pages and persist across navigation. Each route segment can have a layout.js file that wraps that segment and all nested segments. Layouts are nested automatically - the root layout wraps the entire app, then each nested layout wraps its children. This is different from the Pages Router where you had to manually compose layouts. Layouts preserve state across navigation and don't re-render, making them perfect for navigation, sidebars, or shared UI. The root layout must include html and body tags and is required. You can also use template.js which is like layout but creates a new instance on navigation. I use layouts for persistent navigation and sidebars, and templates when I need components to remount on navigation, like for animations or resetting scroll position.",
            "Layouts wrap pages and persist across navigation, each segment can have layout.js. Nested automatically with root layout wrapping everything. Unlike Pages Router, no manual composition. Layouts preserve state, don't re-render, perfect for navigation and sidebars. Root layout must have html and body tags. template.js is like layout but remounts on navigation. I use layouts for persistent UI, templates when components need to remount like for animations."],
    },
    {
        text: "What is the metadata API in the App Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum.seo],
        answers: ["The metadata API in the App Router provides a declarative way to set meta tags for SEO and social sharing. You export a metadata object or generateMetadata function from pages or layouts. The metadata object can include title, description, openGraph, twitter, robots, and more. Metadata merges down the tree, so you can set defaults in the root layout and override in specific pages. generateMetadata is async, letting you fetch data for dynamic metadata like blog post titles. There's also a metadataBase for resolving relative URLs, and special files like opengraph-image.js for generating images. This is much better than the Pages Router's Head component because metadata is type-safe, can be async, and Next.js handles deduplication automatically. I set global defaults in the root layout and use generateMetadata in dynamic pages to create proper meta tags from data.",
            "Export metadata object or async generateMetadata function. Includes title, description, openGraph, twitter, robots. Merges down the tree, set defaults in root layout, override in pages. generateMetadata can fetch data for dynamic titles. metadataBase for relative URLs, opengraph-image.js for image generation. Type-safe, async capable, automatic deduplication. Better than Pages Router Head. I set defaults in root layout, generateMetadata for dynamic pages."],
    },
    {
        text: "How does caching work in the App Router?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["app-router"], ValidTag.enum.caching, ValidTag.enum.performance],
        answers: ["The App Router has multiple caching layers that are more aggressive than the Pages Router. Fetch requests are cached by default indefinitely - you opt out with {cache: 'no-store'} or set revalidation with {next: {revalidate: 60}}. React has a Request Memoization layer that deduplicates fetch calls during a single render. There's a Full Route Cache for static routes at build time, and a Router Cache on the client that persists across navigations. This multi-layer caching is powerful but can be confusing - data might be cached when you don't expect it. To opt out, use dynamic functions like cookies() or headers(), set dynamic = 'force-dynamic', or use no-store on fetches. For ISR-like behavior, use revalidatePath or revalidateTag in Server Actions. I find the caching great for performance but you need to understand it to avoid stale data issues. The key is being explicit about your caching strategy.",
            "Multiple caching layers more aggressive than Pages Router. Fetches cached indefinitely by default, opt out with {cache: 'no-store'} or {next: {revalidate: 60}}. Request Memoization deduplicates fetches during render. Full Route Cache for static routes. Router Cache persists client-side. Can be confusing since data caches unexpectedly. Opt out with cookies(), headers(), dynamic = 'force-dynamic', or no-store. Use revalidatePath or revalidateTag for ISR-like behavior. Be explicit about caching strategy."],
    },

    // React Native Advanced
    {
        text: "What are native modules and when would you create one?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["Native modules are bridges between JavaScript and native platform code, letting you access platform-specific APIs that React Native doesn't expose. You create one when you need functionality not available in React Native's core - like accessing device sensors, integrating with native SDKs, optimizing performance-critical code in native languages, or accessing platform-specific features. You write native code in Java/Kotlin for Android and Objective-C/Swift for iOS, then expose methods to JavaScript. The new architecture uses TurboModules which are faster and type-safe. Creating a native module requires platform-specific knowledge and complicates builds, so I only do it when necessary. Common use cases include payment SDKs, biometric authentication, or complex native UI components. For most apps, community packages cover common needs, but custom native modules are essential when you need something unique or performance-critical.",
            "Native modules bridge JavaScript and native code for platform APIs not exposed by React Native. Write in Java/Kotlin for Android, Objective-C/Swift for iOS, expose to JS. New architecture uses TurboModules, faster and type-safe. Create them for device sensors, native SDKs, performance-critical code, or platform features. Requires platform knowledge and complicates builds. Community packages cover most needs. I create native modules only when necessary for unique functionality or performance."],
    },
    {
        text: "What is the new architecture in React Native (Fabric, TurboModules, JSI)?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["The new React Native architecture consists of three main parts: JSI (JavaScript Interface) which replaces the asynchronous bridge with synchronous native calls, Fabric which is the new rendering system allowing better interoperability between JavaScript and native UI, and TurboModules which lazy-load native modules and provide type safety. The old architecture used a bridge for all JS-to-native communication, creating serialization overhead and making synchronous calls impossible. The new architecture enables synchronous method calls, better startup time through lazy loading, improved type safety with codegen, and true concurrent rendering support. Fabric specifically allows for more sophisticated UI interactions and better performance since it can prioritize high-priority updates. The migration requires updating native modules and careful testing, but the performance improvements are significant. I'd adopt it in new projects now that it's stable, but migration for existing apps requires planning.",
            "Three main parts: JSI replaces async bridge with synchronous native calls. Fabric is new rendering with better JS-native interop and priority handling. TurboModules lazy-load with type safety via codegen. Old bridge had serialization overhead, no sync calls. New architecture enables sync calls, faster startup, concurrent rendering. Migration requires updating native modules. Adopt for new projects, plan carefully for existing apps. Performance improvements are significant."],
    },
    {
        text: "What are performance considerations specific to React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.performance],
        answers: ["React Native has unique performance considerations because of the JavaScript-to-native bridge. Key issues include bridge congestion from too many messages, slow list rendering without proper virtualization, dropped frames from running heavy computations on the UI thread, and large bundle sizes affecting startup time. I optimize by using FlatList or SectionList with proper keyExtractor and item separators, avoiding inline functions and object creation in render, using the useNativeDriver option for animations to run them on the native thread, and keeping images optimized and properly sized. The new Hermes JavaScript engine improves startup time. For navigation, React Navigation performs better than alternatives. I profile with the built-in performance monitor, Flipper, or React DevTools profiler. Memory leaks from event listeners or subscriptions are also common - always clean up in useEffect. The key is understanding what crosses the bridge and minimizing those operations.",
            "Key issues: bridge congestion from too many messages, slow lists without virtualization, dropped frames from heavy UI thread work, large bundles hurting startup. Optimize with FlatList using proper keyExtractor, avoid inline functions in render, use useNativeDriver for animations, optimize images. Hermes engine improves startup. Profile with performance monitor, Flipper, or DevTools. Clean up listeners in useEffect to prevent leaks. Minimize bridge crossings."],
    },
    {
        text: "How do you handle deep linking in React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.routing],
        answers: ["Deep linking in React Native involves configuring URL schemes for your app and handling incoming URLs. You set up URL schemes in native config files - Info.plist for iOS and AndroidManifest.xml for Android. For universal links (iOS) and App Links (Android), you also need to configure associated domains and host an apple-app-site-association or assetlinks.json file. React Navigation has built-in deep linking support through the linking config, which maps URL paths to screens. You handle URLs with the Linking API to detect incoming links and navigate accordingly. The tricky part is handling cold starts versus warm starts - the app might not be running when the link opens. I configure the navigation linking prop with prefixes and screen mappings, handle authentication flows before navigating, and test thoroughly with both custom schemes and universal links. Deep links are essential for features like email verification, password resets, or marketing campaigns.",
            "Configure URL schemes in Info.plist for iOS, AndroidManifest.xml for Android. Universal links and App Links need associated domains and hosted verification files. React Navigation has linking config mapping URLs to screens. Linking API handles incoming URLs. Tricky part: cold starts vs warm starts since app might not be running. Configure navigation linking with prefixes and screen mappings. Handle auth before navigating. Test custom schemes and universal links. Essential for email verification, password resets, marketing."],
    },
    {
        text: "What is CodePush and how does over-the-air updates work?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["CodePush is a Microsoft service that enables over-the-air updates for React Native apps, letting you push JavaScript and asset changes without going through app store review. It works by uploading a new bundle to CodePush, then the app checks for updates on launch or resume and downloads the new bundle in the background. Updates can be mandatory or optional, and you control when they're applied - immediately, on next resume, or on next restart. This is incredibly useful for bug fixes, content updates, or A/B testing. However, you can only update JavaScript and assets - native code changes still require an app store release. I use CodePush for rapid iteration and hot fixes, with staged rollouts to catch issues early. The key is proper versioning and targeting - make sure updates match the native app version. It's also important to have rollback capabilities in case an update causes issues.",
            "CodePush enables OTA updates without app store review. Upload new bundle, app checks on launch or resume, downloads in background. Updates can be mandatory or optional, applied immediately, on resume, or restart. Great for bug fixes, content updates, A/B testing. Only updates JS and assets, native code needs store release. Use staged rollouts to catch issues. Proper versioning ensures updates match native version. Have rollback capabilities ready."],
    },
    {
        text: "How do you handle gestures and animations in React Native?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.animations],
        answers: ["For animations, I use the Animated API with useNativeDriver: true to run animations on the native thread for 60fps performance. For complex gestures, React Native Gesture Handler provides native gesture recognizers that feel more responsive than the built-in PanResponder. React Native Reanimated is the most powerful option, combining gestures and animations with a worklet-based API that runs entirely on the UI thread. For simple animations like fading or sliding, Animated is sufficient. For interactive gestures like swipe-to-delete or draggable cards, I use Gesture Handler with Reanimated. The key difference from web is that you want animations running on the native thread to avoid jank - the JavaScript thread can be busy but animations stay smooth. I avoid animating properties that don't support native driver, like layout properties. For layout animations, LayoutAnimation provides simple spring-based animations. The ecosystem around Reanimated 2+ is really mature now with great developer experience.",
            "Animated API with useNativeDriver runs on native thread for 60fps. Gesture Handler provides native gesture recognizers better than PanResponder. Reanimated is most powerful with worklet-based API on UI thread. Animated for simple fades and slides. Gesture Handler with Reanimated for interactive gestures like swipe-to-delete. Animations must run on native thread to avoid jank. LayoutAnimation for simple layout transitions. Reanimated 2+ has mature ecosystem with great DX."],
    },

    // Accessibility Advanced
    {
        text: "What is the accessibility tree?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.a11y],
        answers: ["The accessibility tree is a simplified version of the DOM tree that assistive technologies like screen readers use to navigate and understand a page. Browsers create it from the DOM by including only semantically meaningful elements and their relationships, roles, states, and properties. Elements like divs without semantic meaning are excluded, while buttons, headings, and links are included with their accessible names and roles. Proper HTML semantics automatically create a good accessibility tree - using button instead of div with onClick, nav for navigation, and proper heading hierarchy. ARIA attributes let you enhance or override the default tree when semantic HTML isn't enough. I check the accessibility tree using browser DevTools to ensure interactive elements are properly exposed and have correct roles and names. Understanding the accessibility tree helps debug why a screen reader isn't working as expected - often it's because elements aren't in the tree or have the wrong information.",
            "The accessibility tree is a simplified DOM that assistive tech uses. Browsers create it including semantic elements with roles, states, and names. Divs without meaning are excluded, buttons and headings included. Semantic HTML creates good accessibility trees automatically. ARIA enhances or overrides when needed. Check it in DevTools to verify elements are exposed correctly. Understanding it helps debug screen reader issues since elements might not be in the tree or have wrong info."],
    },
    {
        text: "How do you make a custom dropdown accessible?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.aria],
        answers: ["Making a custom dropdown accessible requires proper ARIA roles, keyboard navigation, and focus management. The button should have aria-expanded to indicate state, aria-haspopup=\"listbox\", and aria-controls pointing to the dropdown ID. The dropdown itself should have role=\"listbox\" and each option role=\"option\" with aria-selected state. Implement keyboard navigation - Space/Enter to open, Escape to close, Arrow keys to move between options, and Home/End for first/last. Focus management is crucial - focus should move into the dropdown when opened and return to the trigger when closed. The selected option should be indicated visually and with aria-selected. I also add aria-label or aria-labelledby for screen reader users. Honestly, building fully accessible custom dropdowns is complex, so I often use libraries like Radix UI or Headless UI that handle this correctly. If building from scratch, I follow the ARIA Authoring Practices Guide patterns exactly.",
            "Button needs aria-expanded, aria-haspopup=\"listbox\", aria-controls. Dropdown has role=\"listbox\", options have role=\"option\" with aria-selected. Keyboard: Space/Enter to open, Escape to close, Arrow keys for navigation, Home/End for first/last. Focus moves into dropdown on open, returns to trigger on close. aria-label for screen readers. It's complex, so I use Radix or Headless UI. If building from scratch, follow ARIA Authoring Practices exactly."],
    },
    {
        text: "What are skip links?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility],
        answers: ["Skip links are hidden links at the very top of a page that let keyboard users jump directly to main content, bypassing repetitive navigation. They're typically invisible until focused, then appear at the top of the page. This is essential for keyboard and screen reader users who otherwise have to tab through dozens of navigation links on every page. You implement them with an anchor link that points to the main content's ID, styled to be visually hidden until focused. When clicked, focus should move to the target element, which might need tabindex=\"-1\" if it's not naturally focusable. I make sure skip links are the first focusable element on the page and are visible when focused. Common skip links include 'Skip to main content', 'Skip to navigation', and 'Skip to footer'. This is a simple addition that dramatically improves keyboard navigation experience.",
            "Skip links let keyboard users bypass repetitive navigation. Hidden until focused, then visible at page top. Essential since keyboard users otherwise tab through many nav links per page. Implement with anchor links to content IDs, visually hidden until focused. Target elements might need tabindex=\"-1\". First focusable element on page. Common ones: 'Skip to main content', 'Skip to navigation', 'Skip to footer'. Simple addition with big impact."],
    },
    {
        text: "How do you test for accessibility?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.testing],
        answers: ["I use a combination of automated and manual testing. Automated tools like axe DevTools, Lighthouse, or eslint-plugin-jsx-a11y catch obvious issues like missing alt text or color contrast problems, but they only catch about 30-40% of issues. Manual testing is essential - I navigate the entire site with only keyboard, use a screen reader like NVDA or VoiceOver to verify the experience, and test with browser zoom at 200%. I check that all interactive elements are reachable and operable with keyboard, focus indicators are visible, form errors are announced, and dynamic content updates are communicated. For automated testing in CI, I use jest-axe or pa11y. I also test with real users who use assistive tech when possible. The WCAG guidelines are my reference - I aim for AA compliance minimum. Color contrast, keyboard navigation, and screen reader announcements are the areas where I find the most issues.",
            "Combine automated and manual testing. axe DevTools, Lighthouse, eslint-plugin-jsx-a11y catch obvious issues but only 30-40%. Manual is essential: keyboard-only navigation, screen reader with NVDA or VoiceOver, 200% zoom. Check all elements reachable by keyboard, visible focus indicators, form errors announced. jest-axe or pa11y in CI. Test with real assistive tech users when possible. Target WCAG AA minimum. Most issues in contrast, keyboard nav, and announcements."],
    },
    {
        text: "What is keyboard navigation and how do you implement it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum["keyboard-navigation"]],
        answers: ["Keyboard navigation means users can operate your entire site using only the keyboard, without a mouse. The basics are Tab to move forward, Shift+Tab to move backward, Enter/Space to activate, and Arrow keys for component navigation. Implementation starts with using semantic HTML - buttons, links, and form elements are naturally keyboard accessible. For custom interactive elements, add tabindex=\"0\" to make them focusable and handle keyboard events. Focus indicators must be visible - never remove outline without providing an alternative. Focus order should follow visual order and make logical sense. For complex widgets like tabs or menus, implement roving tabindex so only one item is in the tab order and arrow keys move between items. Trapped focus in modals is important - focus should stay within the modal until it closes. I test by unplugging my mouse and trying to complete all tasks. The Web AIM keyboard testing guide is a great resource.",
            "Tab forward, Shift+Tab backward, Enter/Space activate, Arrow keys for widget navigation. Semantic HTML is naturally accessible. Custom elements need tabindex=\"0\" and keyboard event handlers. Never remove focus outline without alternative. Focus order follows visual order. Complex widgets use roving tabindex with Arrow key navigation. Modal focus should be trapped. Test by completing all tasks with keyboard only. Follow Web AIM keyboard guide."],
    },
    {
        text: "What are ARIA live regions?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.aria],
        answers: ["ARIA live regions announce dynamic content changes to screen reader users. You mark an element with aria-live=\"polite\" or aria-live=\"assertive\" to indicate updates should be announced. Polite waits for the user to finish what they're doing, while assertive interrupts immediately - use assertive sparingly for critical alerts. There's also aria-live=\"off\" which is the default. Related attributes include aria-atomic to announce the entire region versus just changes, and aria-relevant to specify what types of changes to announce. Common use cases are form validation errors, loading states, notification toasts, or updating counts. The live region must exist in the DOM before content changes for announcements to work - you can't add both the region and content simultaneously. I typically have a live region in my layout that I update with messages as needed. Role=\"status\" and role=\"alert\" are shortcuts for common live region patterns.",
            "aria-live regions announce dynamic content to screen readers. 'polite' waits for user activity to finish, 'assertive' interrupts immediately for critical alerts. aria-atomic announces entire region vs just changes. aria-relevant specifies what changes to announce. Use for validation errors, loading states, toasts, updating counts. Live region must exist before content changes. role=\"status\" and role=\"alert\" are shortcuts. I keep a persistent live region and update it as needed."],
    },
    {
        text: "What is color contrast and how do you check it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.wcag],
        answers: ["Color contrast is the difference in luminance between text and background, ensuring text is readable for people with visual impairments. WCAG AA requires a ratio of at least 4.5:1 for normal text and 3:1 for large text. AAA level requires 7:1 and 4.5:1 respectively. This also applies to interactive elements and graphics. I check contrast using browser DevTools which show the ratio when inspecting elements, or dedicated tools like the WebAIM contrast checker or Colour Contrast Analyser. Design systems should define color palettes with accessible combinations. Common mistakes are gray text on white backgrounds, light text on colorful backgrounds, or relying only on color to convey information. I make sure links are distinguishable from body text without relying solely on color - underlines or other visual indicators help. Automated tools flag contrast issues, but I also visually review to ensure readability in real-world conditions.",
            "Color contrast is luminance difference between text and background. WCAG AA needs 4.5:1 for normal text, 3:1 for large text. AAA needs 7:1 and 4.5:1. Check in DevTools when inspecting, or WebAIM contrast checker. Design systems should define accessible color combinations. Common mistakes: gray on white, light on colorful, relying only on color. Links need more than just color to distinguish. Automated tools flag issues but also review visually."],
    },

    // Testing Advanced
    {
        text: "How do you test components that fetch data?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["I mock the data fetching mechanism and test the component's behavior with different data states. For components using fetch directly, I mock global fetch with jest.spyOn or a library like MSW (Mock Service Worker) which intercepts network requests. For components using TanStack Query, I wrap them in a QueryClientProvider with a test QueryClient. The test pattern is: render the component, wait for loading state to resolve with waitFor or findBy queries, assert the correct data is displayed, and test error states by mocking failed requests. MSW is my preferred approach because it mocks at the network level, making tests more realistic and not coupled to implementation details. I also test loading states, error states, and empty states. The key is making tests resilient to timing issues with proper async utilities like waitFor, and not testing implementation details like whether useState was called - focus on what the user sees.",
            "Mock the data fetching mechanism. For fetch, use jest.spyOn or MSW which intercepts at network level. For TanStack Query, wrap in QueryClientProvider with test client. Render, wait with waitFor or findBy, assert data displays correctly. Test loading, error, and empty states. MSW is preferred since it's network-level and not coupled to implementation. Use async utilities for timing resilience. Focus on what user sees, not implementation details."],
    },
    {
        text: "What is act() and when do you need it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["act() is a test utility that ensures all updates related to state changes, effects, and rendering are processed before making assertions. React Testing Library wraps most operations in act() automatically - things like render, fireEvent, and userEvent - so you rarely need it directly. You might need act() when using asynchronous utilities outside of RTL's helpers, or when manually triggering state updates in tests. If you see act() warnings, it usually means there are state updates happening after your test completes - often from useEffect or timers. The fix is typically to wait for those updates with waitFor or to clean up side effects properly. Modern RTL with async utilities like findBy queries handles most act() requirements automatically. I avoid wrapping things in act() unless I'm getting warnings, and when I am, I first try to fix the underlying issue rather than silencing the warning.",
            "act() ensures state changes and effects complete before assertions. RTL wraps render, fireEvent, userEvent automatically so rarely needed directly. act() warnings mean updates happen after test completes, usually from effects or timers. Fix by waiting with waitFor or cleaning up effects properly. Modern RTL with findBy handles most cases automatically. I only use act() when getting warnings, and first try fixing underlying issues rather than silencing."],
    },
    {
        text: "How do you test custom hooks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["I use @testing-library/react-hooks or the newer renderHook utility from React Testing Library. renderHook wraps the hook in a test component and provides a result object to access the hook's return value. You can test the initial state, call returned functions to test state updates, and test effects by waiting for changes. For hooks with dependencies like context or providers, you pass a wrapper option. The pattern is: call renderHook, access result.current for the hook's value, call rerender to test prop changes, and use waitFor for async updates. For example, testing a useToggle hook: render it, assert initial state is false, call toggle function from result.current, assert state is now true. For hooks using TanStack Query or other providers, wrap in the necessary providers. I test the hook's API and behavior, not implementation details - if the hook returns the right values and functions work correctly, implementation can change.",
            "Use renderHook from React Testing Library. It wraps hooks in a test component, returning result object for the hook's value. Access result.current for state, call returned functions, use rerender for prop changes, waitFor for async. Pass wrapper option for hooks needing providers. Test the hook's API and behavior, not implementation. For useToggle: render, assert initial false, call toggle, assert true. For hooks with TanStack Query, wrap in QueryClientProvider. Focus on what the hook returns and whether functions work, not internal details."],
    },
    {
        text: "How do you test error boundaries?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["Testing error boundaries requires making a component throw an error and asserting the error UI renders. I create a component that throws when a prop is true, wrap it in the error boundary, and render with that prop. Console errors during testing are noisy, so I mock console.error to suppress them. The pattern is: spy on console.error to suppress output, render the error boundary wrapping a component that will throw, assert the fallback UI is displayed, and restore console.error after the test. You might need to use act() or waitFor if the error boundary updates asynchronously. React Testing Library has a suppressConsoleWarnings option. I also test that the error boundary doesn't show for components that don't throw, and test the reset functionality if the error boundary provides one. Error boundaries are a good case for integration testing since they test React's error handling mechanism.",
            "Create a component that throws conditionally, wrap in the error boundary, trigger the throw. Mock console.error to suppress noise since React logs errors. Assert fallback UI renders. Use act() or waitFor for async boundaries. Also test normal rendering when nothing throws and reset functionality if available. The pattern: spy on console.error, render with throwing component, assert fallback shows, restore console.error. Error boundaries need integration-style tests since they test React's error handling mechanism."],
    },
    {
        text: "What is screen and why should you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl],
        answers: ["screen is an object from React Testing Library that provides query methods scoped to the entire document body. Instead of destructuring queries from render, you use screen.getByRole, screen.findByText, etc. The benefit is you don't need to keep track of the render result, tests are more concise, and it encourages querying from the user's perspective of the entire page. It also makes tests more maintainable since you're not constantly destructuring and managing query functions. The pattern is: render the component, then use screen.getByRole('button') instead of const { getByRole } = render() and getByRole('button'). This is the recommended approach in modern RTL. I use screen for all queries unless I specifically need container or other values from render. It makes tests cleaner and aligns with the library's philosophy of testing from the user's perspective of the whole document.",
            "screen provides queries scoped to document body. Instead of destructuring from render, use screen.getByRole, screen.findByText. Cleaner since no managing render result. Tests are concise and encourage user-perspective queries. Pattern: render component, then screen.getByRole('button'). Recommended modern RTL approach. I use screen for all queries unless I need container from render. Aligns with testing from user perspective."],
    },
    {
        text: "How do you debug failing tests?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.debugging],
        answers: ["I start with screen.debug() to see what's actually rendered - this often reveals the component isn't in the state I expected. For specific elements, screen.logTestingPlaygroundURL() gives an interactive playground to find the right queries. I check if I'm using the right query type - getBy throws immediately, queryBy returns null, findBy is async. If timing issues, I add waitFor or switch to findBy queries. I run the single failing test with .only to isolate it, and use --watch mode to iterate quickly. For async issues, I increase the timeout or add better waiting. Common issues are: querying for text that's not actually rendered, not waiting for async operations, querying too specifically and being brittle, or having multiple elements matching the query. The RTL error messages are usually helpful - they show what was found and suggest alternatives. I also check test coverage to ensure I'm actually testing the paths I think I am.",
            "Start with screen.debug() to see rendered output. screen.logTestingPlaygroundURL() gives interactive playground for queries. Check query type: getBy throws, queryBy returns null, findBy is async. Add waitFor or findBy for timing issues. Use .only to isolate and --watch for quick iteration. Common issues: querying for unrendered text, missing async waits, brittle queries, multiple matches. RTL errors show what was found and suggest alternatives."],
    },

    // Git Advanced
    {
        text: "What is interactive rebase?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.rebase],
        answers: ["Interactive rebase lets you rewrite commit history by editing, reordering, squashing, or dropping commits. You run git rebase -i HEAD~n to rebase the last n commits, which opens an editor with commands for each commit. Commands include pick (keep), reword (change message), edit (amend commit), squash (combine with previous), fixup (squash but discard message), and drop (remove). This is incredibly useful for cleaning up commits before merging - you can combine WIP commits, fix typos in messages, or reorder commits logically. I use it to maintain a clean, meaningful commit history on feature branches. The golden rule is never rebase commits that have been pushed to shared branches - only rebase local commits or commits on your personal feature branch. After rebasing, you'll need to force push, which is why it's dangerous on shared branches.",
            "git rebase -i HEAD~n opens editor with commit commands: pick keeps, reword changes message, edit amends, squash combines, fixup squashes without message, drop removes. Clean up commits before merging by combining WIP commits, fixing message typos, reordering logically. Never rebase pushed commits on shared branches since it requires force push. Only rebase local or personal feature branch commits."],
    },
    {
        text: "What is git bisect?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.debugging],
        answers: ["git bisect is a binary search tool for finding which commit introduced a bug. You start with git bisect start, mark the current bad commit with git bisect bad, and mark a known good commit with git bisect good <commit>. Git then checks out commits in the middle of that range, you test each one and mark it good or bad, and Git narrows down until it finds the first bad commit. You can automate this with git bisect run <script> that returns 0 for good, 1 for bad. This is incredibly powerful when you know something broke but don't know when or why. It's much faster than manually checking commits. I've used it to find performance regressions or broken tests when the commit history is long. Once bisect finds the culprit commit, you can examine what changed and why it broke.",
            "Bisect performs binary search through commit history to find bugs. Run git bisect start, then git bisect bad to mark current as broken, git bisect good <hash> for a known working commit. Git checks out middle commits and you mark each good or bad until it isolates the problem commit. The power is logarithmic efficiency - 1000 commits takes only about 10 tests. Automate with git bisect run <test-script> where exit code 0 means good, non-zero means bad. When done, git bisect reset returns to original branch. I use this for regressions when someone reports 'this used to work' - I find a commit I know worked, run bisect, and it pinpoints exactly where things broke."],
    },
    {
        text: "What are Git submodules?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git submodules let you include one Git repository inside another as a subdirectory, keeping them as separate repos with independent histories. You add one with git submodule add <url>, which creates a .gitmodules file and checks out the submodule at a specific commit. The parent repo stores a pointer to the submodule's commit, not the submodule's content. When cloning a repo with submodules, you need git clone --recursive or git submodule update --init. Submodules are tricky because they're pinned to specific commits - updating requires explicitly pulling in the submodule and committing the new pointer. They're useful for shared libraries or dependencies you want version-controlled separately. However, they're complex and I avoid them when possible - modern package managers and monorepo tools like pnpm workspaces are usually better solutions. If I do use them, I document the workflow clearly.",
            "Submodules embed one repo inside another at a fixed commit. The parent tracks a commit pointer, not the actual files. Add with git submodule add <url> <path>, clone with --recursive flag, update with git submodule update --remote. The complexity comes from version management - submodules don't auto-update, you must explicitly update and commit the new pointer. Common issues: forgetting --recursive when cloning, submodule stuck on wrong commit, or team members not updating after pulls. Honestly, I rarely use them now. pnpm workspaces or npm workspaces handle monorepo needs better. If I must use submodules for legacy reasons, I add git hooks or CI checks to ensure everyone's submodules stay in sync."],
    },
    {
        text: "What is force push and when is it safe to use?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Force push overwrites the remote branch with your local branch, discarding any commits that exist remotely but not locally. It's needed after rewriting history with rebase, amend, or reset. The danger is that it can delete other people's work if they've pushed to the same branch. It's safe when: you're the only one working on the branch, you've coordinated with your team, or you're fixing your own feature branch. Use --force-with-lease instead of --force - it only succeeds if the remote hasn't been updated since you last fetched, preventing you from overwriting someone else's work. Never force push to main, master, or shared branches unless there's an emergency and you've coordinated with the team. I use it regularly on feature branches after cleaning up commits with interactive rebase, but I'm very careful and always use --force-with-lease.",
            "git push --force replaces the remote branch entirely with your local version, which is dangerous because it can erase others' commits. It's necessary after rebase, amend, or reset since these rewrite history. The safer alternative is --force-with-lease which fails if someone else pushed since your last fetch. Safe scenarios: personal feature branches, after coordinating with team, or when you're certain no one else pushed. Never force push protected branches like main. I alias push --force-with-lease to make it my default force push. Before force pushing, I double-check I'm on the right branch and that it's not shared. Some teams disable force push entirely on shared branches through GitHub branch protection rules."],
    },
    {
        text: "What is squashing commits and when would you do it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Squashing combines multiple commits into a single commit, cleaning up history. You can squash with interactive rebase (using squash or fixup commands) or when merging with git merge --squash. Squashing is useful for combining WIP commits, cleanup commits, or multiple commits that represent a single logical change. It makes the main branch history cleaner and easier to understand. However, it loses the granular history, so you lose the ability to understand the detailed progression of changes or to cherry-pick specific parts. I squash commits on feature branches before merging to main to keep one commit per feature or fix. Some teams prefer preserving all commits for full history. The right approach depends on your team's workflow - some want atomic commits in main, others want full history. GitHub and GitLab both support squash merging, which makes this easy.",
            "Squashing merges multiple commits into one, creating cleaner history. Use git rebase -i and mark commits as 'squash' or 'fixup' to combine them, or use GitHub's squash merge button. I squash when my branch has messy WIP commits like 'fix typo', 'oops', 'actually fix it' - nobody needs to see that in main. The tradeoff is losing granular history for that feature. Some prefer preserving all commits for detailed archaeology. My approach: squash feature branches to single well-described commits when merging to main, but keep atomic commits within the branch during development for easier rollback. GitHub's squash merge is convenient but I sometimes prefer local squash for more control over the final message."],
    },

    // Performance Advanced
    {
        text: "What is the Performance panel and how do you identify bottlenecks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["The Performance panel in Chrome DevTools records runtime performance by capturing all activity during a session. You start a recording, interact with the page, stop recording, and analyze the timeline. The flame graph shows which functions took the longest - wider bars mean longer execution. Look for long tasks (over 50ms) that block the main thread, excessive layout recalculations from forced reflows, or long paint times. The Summary tab shows time breakdown by activity type - scripting, rendering, painting. I look for unnecessary re-renders in React, expensive operations in loops, or layout thrashing from reading and writing layout properties. The Bottom-Up and Call Tree tabs help identify which functions are most expensive. Common bottlenecks include large DOM operations, unoptimized images, inefficient selectors, or heavy JavaScript execution. The key is recording realistic user interactions, not just page load.",
            "Start a Performance recording, interact with your app, stop, and analyze the flame chart. Width equals time - wide blocks are slow. Red triangles flag long tasks blocking the main thread. Yellow is scripting, purple is layout/rendering, green is painting. I focus on finding forced synchronous layouts where JavaScript reads layout values then writes, causing repeated recalculations. The Call Tree shows where time went hierarchically, Bottom-Up shows what functions took most time regardless of call stack. For React apps, I look for component renders taking too long. The key insight: if the main thread is blocked for over 50ms, the UI feels laggy. Break up long tasks with setTimeout or requestIdleCallback."],
    },
    {
        text: "What is the Memory panel and how do you detect memory leaks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["The Memory panel takes heap snapshots showing what's using memory in your app. To detect leaks, I take a snapshot, perform actions that should clean up (like navigating away), force garbage collection, take another snapshot, and compare them. If memory keeps growing, there's likely a leak. The Comparison view shows what increased between snapshots. Common culprits are event listeners not removed, timers not cleared, DOM references kept after elements removed, or closures holding onto large objects. The Retainers section shows why an object is still in memory. I also use the Allocation Timeline to see memory allocation over time - steady growth indicates leaks. In React, leaks often come from useEffect without cleanup, subscriptions not unsubscribed, or listeners not removed. The DevTools highlight detached DOM nodes which are leaked elements. Fix leaks by ensuring cleanup in useEffect, clearing timers, and removing event listeners.",
            "Take heap snapshots before and after suspect actions, force garbage collection between, and compare. Leaks show up as growing memory that should have been freed. The Comparison view highlights what increased. Look for detached DOM nodes - elements removed from page but still referenced. Retainers chain shows why objects can't be garbage collected. Common React leaks: useEffect missing cleanup function for subscriptions or timers, storing references to unmounted component state, event listeners on window not removed. Allocation Timeline visualizes allocations over time - steady growth means leak. Fix by returning cleanup functions from useEffect, using AbortController for fetch requests, and ensuring all subscriptions are cleaned up on unmount."],
    },
    {
        text: "What are React DevTools and how do you use them?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.debugging],
        answers: ["React DevTools is a browser extension with Components and Profiler tabs. The Components tab shows the React component tree, lets you inspect props and state, edit values in real-time, and see which component rendered which DOM elements. You can also view hooks and their values. The Profiler tab records rendering performance - which components rendered, why they rendered, and how long they took. I use it to identify unnecessary re-renders by looking at why each component rendered (props changed, parent rendered, etc.). The flame graph shows expensive renders, and the ranked chart shows which components took longest. You can highlight renders in the browser to visually see what's re-rendering. I also use the 'rendered by' feature to trace where components come from. For debugging, I inspect component props and state to verify they match expectations. The ability to edit state live is incredibly useful for testing different scenarios without changing code.",
            "React DevTools has two main tabs: Components for inspecting the tree and Profiler for render performance. In Components, I navigate the hierarchy, check props and state values, and even edit them live to test scenarios. Hooks are shown with their current values. The Profiler records renders - I start recording, interact with the app, stop, and see which components rendered and why. 'Why did this render?' shows if props changed, parent rendered, or hooks changed. The ranked view sorts by render time to spot expensive components. I enable 'Highlight updates' to visually see what's re-rendering during interaction. This immediately reveals wasted renders. Combined with React.memo and proper dependency arrays, I use DevTools findings to eliminate unnecessary renders."],
    },
    {
        text: "What are some debugging tools you use?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging],
        answers: ["I use a combination of tools depending on the issue. Chrome DevTools is essential - Console for logging, Sources for breakpoints and stepping through code, Network for API issues, Performance for runtime bottlenecks, and Application for storage and service workers. React DevTools for component inspection and performance profiling. For network issues, I use the Network panel or tools like Postman for API testing. For build issues, I check webpack stats or use bundle analyzers. For state management, Redux DevTools or Zustand DevTools. VS Code debugger for stepping through Next.js server-side code. console.log is still useful but I try to use debugger statements and breakpoints instead. For production issues, error tracking with Sentry or LogRocket that includes session replay. The key is using the right tool for the problem - network issues need Network tab, rendering issues need Performance panel, logic issues need the debugger.",
            "Chrome DevTools is my primary tool - Console for errors and logging, Sources for breakpoints instead of console.log everywhere, Network for API debugging, Performance for runtime issues, Memory for leaks. React DevTools for component tree and render profiling. Redux/Zustand DevTools for state inspection. VS Code's debugger for server-side Node and Next.js code. For production, Sentry captures errors with stack traces and LogRocket provides session replay to see exactly what users did. The debugger statement in code opens DevTools automatically. I also use the Network tab's throttling to test slow connections. Match the tool to the problem: console for quick checks, debugger for logic issues, Network for API problems, Performance for slowness, Memory for leaks."],
    },
    {
        text: "What is the difference between LCP, FID/INP, and CLS?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum["web-vitals"], ValidTag.enum.lcp, ValidTag.enum.fid, ValidTag.enum.cls],
        answers: ["These are Core Web Vitals that measure user experience. LCP (Largest Contentful Paint) measures loading performance - when the largest content element becomes visible. Good LCP is under 2.5 seconds. Improve it by optimizing images, server response times, and render-blocking resources. FID (First Input Delay) measures interactivity - the delay between user interaction and browser response. It's being replaced by INP (Interaction to Next Paint) which measures overall responsiveness. Good FID is under 100ms, INP under 200ms. Improve by reducing JavaScript execution time and breaking up long tasks. CLS (Cumulative Layout Shift) measures visual stability - unexpected layout shifts. Good CLS is under 0.1. Improve by setting dimensions on images and embeds, avoiding inserting content above existing content, and using transform animations instead of layout properties. I monitor these with Lighthouse, Chrome UX Report, or real user monitoring. They directly impact SEO and user experience.",
            "Core Web Vitals are Google's key UX metrics that affect SEO. LCP tracks when main content appears - optimize images, preload critical resources, fast server response. FID/INP measure responsiveness to user input - reduce JavaScript execution, break up long tasks, avoid heavy handlers. CLS measures layout stability - set explicit dimensions on images and embeds, don't inject content above existing content, use transform for animations instead of position. Good scores: LCP under 2.5s, INP under 200ms, CLS under 0.1. Measure with Lighthouse, PageSpeed Insights, or web-vitals npm package for real user monitoring. These metrics directly correlate to user satisfaction and Google ranks them, so they're worth optimizing."],
    },
    {
        text: "What are source maps and how do they work?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging],
        answers: ["Source maps are files that map minified/compiled code back to the original source code, making debugging production code possible. When your code is bundled and minified, error stack traces point to meaningless minified code. Source maps let the browser show you the original file, line, and column numbers. They work through a special comment in the compiled code pointing to the .map file, which contains a mapping between transformed and original code. In development, you want inline source maps for fast debugging. In production, I use hidden source maps that aren't deployed publicly but can be uploaded to error tracking services like Sentry. This keeps source code private while still enabling debugging. Different source map types have tradeoffs between build speed and quality - 'eval-source-map' is fast but large, 'source-map' is slower but production-ready. Most tools like webpack handle source map generation automatically.",
            "Source maps connect minified production code back to original source. Without them, stack traces show cryptic one-letter variables at line 1. The .map file contains the mapping and browsers use it to show original files in DevTools. The bundled file has a comment like //# sourceMappingURL=app.js.map pointing to it. For development, inline source maps are fast. For production, I generate them but don't deploy publicly - instead upload to Sentry so error reports show real code. 'hidden-source-map' generates maps without the URL comment, keeping them private. Different webpack devtool options trade build speed for accuracy: eval is fast but low quality, source-map is slow but precise. TypeScript and Babel also generate source maps that can chain together."],
    },
    {
        text: "How do you profile JavaScript performance?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["I use the Performance panel in DevTools to record and analyze JavaScript execution. Start recording, perform the action to profile, stop recording, and examine the flame chart. Look for long tasks over 50ms, excessive function calls, or expensive operations. The Bottom-Up view shows which functions consumed the most time. For detailed function-level profiling, console.time/timeEnd or performance.mark/measure work well. React Profiler specifically shows component render times. I look for unnecessary calculations, inefficient algorithms, or operations in tight loops. Common issues include excessive object creation, unoptimized loops, synchronous operations blocking the thread, or missing memoization. The key is testing with realistic data sizes - performance problems often only appear at scale. For production monitoring, tools like Sentry track long tasks and slow renders. The goal is keeping the main thread free so the UI stays responsive - break up long tasks, use web workers for heavy computation, or defer non-critical work.",
            "Record in DevTools Performance panel during the slow action, then examine the flame chart for wide blocks indicating long execution. console.time('label') and console.timeEnd('label') give quick measurements. performance.mark() and performance.measure() provide precise timing for custom metrics. For React specifically, the Profiler component or DevTools Profiler shows render times. I look for O(nÂ²) operations, excessive DOM manipulation, synchronous file operations, or functions called too frequently. Test with production-like data sizes since small data hides problems. Web Workers offload heavy computation off the main thread. requestIdleCallback schedules non-critical work during idle time. The goal is keeping the main thread responsive - no task should block for more than 50ms."],
    },
    {
        text: "What is the difference between client-side and server-side profiling?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["Client-side profiling measures performance in the user's browser - render times, JavaScript execution, network requests. Tools are browser DevTools, Lighthouse, or Web Vitals. You see real user device constraints and network conditions. Server-side profiling measures backend performance - database queries, API response times, server CPU/memory. Tools are APMs like New Relic, Datadog, or built-in Node profilers. With SSR frameworks like Next.js, you need both - server-side for getServerSideProps execution and database queries, client-side for hydration and interactivity. The challenges are different - client-side is affected by device capabilities and network, server-side by database performance and concurrent requests. I use client-side profiling to optimize bundle size, rendering, and user interactions. Server-side to optimize API response times and database queries. Real User Monitoring combines both to see the full picture of how fast the app feels to actual users.",
            "Client-side measures what happens in the browser - JavaScript execution, rendering, network requests. Use Chrome DevTools Performance and Memory panels, Lighthouse, or real user monitoring libraries. Server-side measures backend work - database queries, API processing, memory usage. Use APMs like Datadog, New Relic, or Node's built-in profiler. With Next.js you need both: server profiling for data fetching in Server Components or API routes, client profiling for hydration and interactivity. They have different bottlenecks - client depends on device capability and network, server depends on database performance and concurrent load. Real User Monitoring bridges them by capturing end-to-end timing from actual users' perspectives."],
    },
    {
        text: "How do you identify and fix memory leaks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["I use the Memory panel to take heap snapshots before and after actions. The workflow is: take snapshot, perform action that might leak (like opening/closing a modal repeatedly), force garbage collection, take another snapshot, compare. Growing memory between snapshots indicates a leak. The Comparison view shows what increased. Common React leaks are useEffect without cleanup returning functions, event listeners added but not removed, intervals not cleared, or subscriptions not unsubscribed. Detached DOM nodes are elements removed from the DOM but still referenced in memory. The Retainers view shows why an object can't be garbage collected. To fix: ensure useEffect cleanup functions remove listeners and clear timers, unsubscribe from observables, remove event listeners with the same function reference used to add them. WeakMap can help for caching without preventing garbage collection. I also watch for growing arrays or objects that should be bounded. The key is reproducing the leak in a controlled way so you can snapshot before and after.",
            "Memory leaks happen when objects can't be garbage collected. Symptoms: app gets slower over time, memory usage grows. To debug: take heap snapshot, perform action, force GC, take another snapshot, compare. Look for objects that should be gone but aren't. Check for detached DOM nodes - elements removed from DOM but still referenced. Common culprits: useEffect without cleanup function, event listeners on window never removed, setInterval not cleared, closures capturing large objects. The Retainers chain shows what's keeping an object alive. Fix by always returning cleanup from useEffect, using AbortController for fetch, clearing intervals with clearInterval. WeakMap and WeakSet hold references that allow GC. Bounded caches prevent unbounded growth."],
    },
    {
        text: "What causes render blocking and how do you avoid it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance],
        answers: ["Render blocking happens when resources prevent the browser from displaying content. CSS is render-blocking by default because the browser needs styles before painting. JavaScript blocks HTML parsing unless it's async or defer. Large synchronous scripts in the head are the worst offenders. To avoid: load critical CSS inline or as high priority, defer non-critical CSS with media queries or rel=\"preload\", use async or defer on scripts, split code to only load what's needed, move scripts to the end of body. For CSS, extract critical above-the-fold styles and inline them, lazy-load the rest. Next.js and modern frameworks handle a lot of this automatically with code splitting and optimized loading. Font loading can also block - use font-display: swap to show fallback fonts immediately. The goal is getting content visible as fast as possible, even if not fully styled or interactive. Lighthouse flags render-blocking resources and suggests optimizations.",
            "CSS blocks rendering because the browser won't paint until CSSOM is built. JavaScript blocks HTML parsing unless marked async or defer. To fix: inline critical CSS in head for immediate styling, load non-critical CSS asynchronously. Use defer for scripts that need DOM ready, async for independent scripts. Move large scripts to body end or use dynamic import(). Font-display: swap shows fallback fonts while custom fonts load. Preload critical resources with link rel='preload'. Code splitting ensures only needed code loads initially. Modern frameworks handle much of this - Next.js automatically optimizes script and style loading. Check Lighthouse for 'Eliminate render-blocking resources' opportunities. The goal: show content fast, even if enhancement loads later."],
    },
    // Bundle Sizing
    {
        text: "How would you debug large bundle sizes?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["I start with a bundle analyzer to visualize what's taking up space. webpack-bundle-analyzer or Next.js's @next/bundle-analyzer show a treemap of your bundle. Common issues include: importing entire libraries instead of specific functions (lodash instead of lodash/debounce), multiple versions of the same package, large dependencies that could be replaced, source maps or development code in production builds, or duplicate code across chunks. I check the import statements - using named imports from barrel files can prevent tree shaking. I look for moment.js which is huge and can be replaced with date-fns or day.js. I verify production mode is enabled and minification is working. I also check for accidentally imported dev dependencies. The bundle analyzer quickly shows the biggest contributors. Once identified, solutions include using dynamic imports for large features, replacing heavy libraries, fixing duplicate dependencies in package.json, or splitting vendor code into separate chunks.",
            "Run webpack-bundle-analyzer to see a treemap of what's in the bundle. The biggest rectangles are your targets. Check for: whole lodash instead of specific imports, moment.js that could be day.js, duplicate versions of packages, accidentally bundled dev dependencies, or barrel file imports defeating tree-shaking. npm ls can reveal duplicate package versions. Check if you're accidentally bundling source maps in production. Dynamic imports help - lazy load routes and heavy features. Replace large libraries with lighter alternatives. Import specifically from package paths not barrel files. For Next.js, check both client and server bundles separately since issues may be isolated to one."],
    },
    {
        text: "What tools do you use to analyze bundle size?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance, ValidTag.enum["bundle-analysis"]],
        answers: ["webpack-bundle-analyzer is my go-to for visualizing bundle composition as an interactive treemap. For Next.js, @next/bundle-analyzer integrates seamlessly. bundlephobia.com lets me check the size of packages before installing them. Source-map-explorer analyzes the final bundle using source maps. For ongoing monitoring, I use bundlesize or size-limit in CI to fail builds that exceed thresholds. VS Code extensions like Import Cost show package sizes inline. In the browser, the Coverage tab in DevTools shows unused code in each file. Lighthouse reports bundle size issues. For build output, most bundlers show gzipped sizes which is what actually matters over the network. I also check the Network tab to see actual transferred sizes. The combination of visualization tools for investigation and automated checks in CI for prevention works well. The key is making bundle size visible and tracked, not just a one-time check.",
            "webpack-bundle-analyzer gives interactive treemaps - install, run analyze script, inspect visually. @next/bundle-analyzer wraps it for Next.js. bundlephobia.com checks package sizes before installation. source-map-explorer uses source maps to show what's in the compiled output. For CI, size-limit or bundlesize fail builds exceeding thresholds. Import Cost VS Code extension shows sizes inline as you code. DevTools Coverage tab reveals unused JavaScript. Lighthouse scores include bundle size considerations. I track gzipped size since that's what's transferred. Network tab shows real transfer sizes. Set up automated monitoring so regressions are caught at PR time, not after deployment."],
    },
    {
        text: "What is webpack-bundle-analyzer?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.webpack, ValidTag.enum.bundling, ValidTag.enum["bundle-analysis"]],
        answers: ["webpack-bundle-analyzer is a plugin that generates an interactive treemap visualization of your webpack bundle contents. Each rectangle represents a module, sized by its actual size in the bundle. You can see what's taking up space, drill down into dependencies, and identify optimization opportunities. It shows both stat size (original), parsed size (output), and gzipped size (what's actually transferred). I add it to webpack config or use it as a one-off with the CLI. The visualization makes it immediately obvious if you're accidentally including a massive library or have duplicate code. You can hover over modules to see their full path and size. It's invaluable for optimization - I've found things like entire icon libraries included when only a few icons were needed, or dev dependencies accidentally bundled. For Next.js there's @next/bundle-analyzer which wraps this. I run it periodically, especially after adding new dependencies.",
            "It's a webpack plugin generating an interactive visualization of your bundle. Rectangles represent modules, sized proportionally. Hover to see full paths and sizes. Three size types: stat (original), parsed (output), gzipped (network transfer). Install it, add to webpack config with analyzerMode: 'static' for HTML output. The visual immediately reveals problems - giant node_modules, duplicate packages, or unexpected inclusions. I discovered we were bundling all of lodash when we only needed debounce. For Next.js, use @next/bundle-analyzer wrapper. Run after major dependency changes or when bundle feels bloated. Compare before and after to verify optimizations worked."],
    },
    {
        text: "What is the difference between named exports and default exports for tree shaking?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tree-shaking"], ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["Named exports generally tree-shake better than default exports because bundlers can statically analyze exactly what's imported. With named exports, if you import { specific } from 'library', the bundler knows you only need 'specific' and can remove everything else. With default exports, especially when re-exporting an object with many properties, it's harder for bundlers to determine what's actually used. However, the real issue is barrel files - index files that re-export everything. Even with named imports, if the barrel file imports everything first, you might get the whole library. The best practice for tree-shakeable libraries is named exports with separate entry points, not barrel files. For example, lodash-es tree shakes better than lodash because it uses ES modules. I use named exports for library code and avoid deep barrel files. The sideEffects field in package.json also helps bundlers know it's safe to tree shake.",
            "Named exports enable better tree-shaking because bundlers can trace exactly which exports are used. import { debounce } from 'lodash-es' only bundles debounce. Default exports, especially objects with many properties, are harder to analyze statically. The real culprit is often barrel files - index.ts that re-exports everything. When the barrel imports all modules, tree-shaking can't help even with named imports. Solution: import from specific paths like 'lodash-es/debounce' instead of barrel files. Libraries should set 'sideEffects: false' in package.json to signal they're safe to tree-shake. Use ES modules (lodash-es not lodash) since CommonJS doesn't tree-shake well. Check if your imports actually tree-shake with bundle analyzer."],
    },
    {
        text: "How do you optimize third-party library imports?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance],
        answers: ["I start by importing only what I need using specific paths rather than barrel files. For lodash, import debounce from 'lodash/debounce' instead of import { debounce } from 'lodash'. For icon libraries, use individual icon imports or optimized packages. I check if there's a lighter alternative - replace moment.js with date-fns or day.js, use preact instead of react for smaller bundles if possible. For UI libraries like Material-UI, configure babel plugins for automatic tree shaking. I avoid libraries that don't support tree shaking or are unnecessarily large. Before adding dependencies, I check bundlephobia for their impact. For already included libraries, I look for CDN options or dynamic imports to defer loading. I also check for duplicate dependencies with npm dedupe or pnpm. Using the browser's native APIs instead of libraries is ideal - intersection observer instead of scroll libraries, native date formatting instead of moment.",
            "Import from specific paths: import debounce from 'lodash/debounce' not from 'lodash'. For icons, import individually not the whole library. Replace heavy libraries: moment.js with day.js, lodash with lodash-es or native methods. Check bundlephobia before adding dependencies. Configure babel plugins for MUI or similar libraries that need special tree-shaking setup. Dynamic import heavy features so they're not in initial bundle. Use native browser APIs when possible - IntersectionObserver, Intl.DateTimeFormat, native fetch. Run npm dedupe to eliminate duplicate package versions. For already large bundles, identify the biggest offenders with bundle analyzer and either replace them or lazy load them."],
    },
    {
        text: "What is dynamic imports and how do they help bundle size?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum["code-splitting"], ValidTag.enum.performance],
        answers: ["Dynamic imports use import() syntax to load modules asynchronously at runtime instead of bundling them in the main bundle. They return a promise that resolves to the module. This creates separate chunks that are only loaded when needed, reducing the initial bundle size. For example, import('./HeavyComponent').then(mod => ...) loads HeavyComponent only when that code path executes. In React, React.lazy wraps dynamic imports for component code splitting. I use dynamic imports for routes - each route becomes a separate chunk loaded on navigation. Also for features not immediately needed like modals, charts, or admin sections. The tradeoff is a network request when the code is needed, so there's a slight delay. But the improved initial load time usually outweighs this. Next.js handles this automatically with dynamic imports. The key is identifying code that isn't needed on initial render and splitting it into separate chunks.",
            "Dynamic import() returns a promise for the module, loaded on demand rather than upfront. The bundler creates separate chunks for dynamically imported code. This reduces initial load - users only download what they immediately need. React.lazy(() => import('./Component')) wraps this for components with Suspense for loading states. I use it for routes (each page loads separately), modals, rich text editors, charts - anything not needed immediately. Next.js next/dynamic adds SSR support and loading components. The tradeoff is a network request when first needed. Prefetching with link rel='prefetch' or Next.js Link can load chunks during idle time. Identify good candidates by finding large components or libraries used only in specific flows."],
    },
    {
        text: "How do you measure and track bundle size over time?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.bundling, ValidTag.enum.performance, ValidTag.enum["bundle-analysis"]],
        answers: ["I use automated tools in CI to prevent regressions. bundlesize or size-limit run on every PR and fail if bundles exceed defined thresholds. They can comment on PRs with size changes. bundlewatch tracks size over time and integrates with GitHub status checks. For more detailed tracking, I export webpack stats JSON and use tools like bundle-stats or relative-ci to compare builds. These show historical trends and highlight what changed. In the build output itself, most bundlers show compressed sizes - I track these numbers in release notes. For production monitoring, tools like SpeedCurve or Calibre track real bundle sizes from actual users. I set alerts for significant increases. The workflow is: define size budgets based on performance goals, enforce them in CI, review changes in PRs, and monitor production. The key is making bundle size a team concern, not just something one person checks occasionally. Automation prevents accidental bloat.",
            "Set up size-limit or bundlesize in CI to fail PRs that exceed thresholds. They comment on PRs showing exactly what changed and by how much. Define budgets based on performance goals - maybe 200KB for main bundle. bundlewatch provides GitHub status checks and historical tracking. relative-ci compares bundles between commits showing what grew. Track gzipped sizes since that's what users download. For production monitoring, Real User Monitoring captures actual bundle load times. Set alerts for significant size increases. The key is automation - manual checks get skipped, but CI gates are reliable. Review size changes in PR reviews like you review code. Make bundle size everyone's responsibility, not just an occasional audit."],
    },
];

