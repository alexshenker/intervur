import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const juniorAdvanced: QuestionForCategoryAndLevel<
    typeof Category.enum.frontend,
    typeof Level.enum["junior-advanced"]
>[] = [
    // JavaScript
    {
        text: "What are closures and how would you use them in practice?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.closures, ValidTag.enum.scope],
        answers: ["A closure is when a function remembers and can access variables from its outer scope, even after the outer function has finished executing. This happens because JavaScript functions form closures around the data they need. In practice, I use closures all the time for things like data privacy and creating function factories. For example, if I need to create multiple event handlers that each need to remember a specific value, closures make that really clean. They're also super useful for callbacks and handling asynchronous operations where you need to preserve state. The key thing to remember is that closures keep references to variables, not copies, so if those variables change, the closure sees the updated values.",
            "Closures let inner functions access outer function variables even after the outer function returns. Every function in JavaScript creates a closure over its lexical scope. Practically, I use them for encapsulating private state, like in module patterns or factory functions. A common example is creating click handlers in a loop where each handler needs to remember its specific index. Without understanding closures, that code behaves unexpectedly. They're fundamental to JavaScript and appear everywhere once you start looking."],
    },
    {
        text: "What is the this keyword and how does its binding work in different contexts?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum["this-keyword"]],
        answers: ["The 'this' keyword refers to the context in which a function is executed, and it can be tricky because it changes depending on how the function is called. In a regular function, 'this' refers to the object that called the function. In a method, it's the object the method belongs to. With arrow functions, 'this' is lexically bound, meaning it uses 'this' from the enclosing scope, which is why arrow functions are great for callbacks. You can also explicitly set 'this' using call, apply, or bind. One common gotcha is when you pass a method as a callback - it loses its original 'this' binding, so you either need to use bind or an arrow function to preserve the context. In strict mode, 'this' is undefined in regular functions if they're not called as methods.",
            "In JavaScript, 'this' is determined by how a function is called, not where it's defined - unless you use arrow functions, which inherit 'this' from their definition context. Methods get 'this' as the object they're called on. Standalone functions get the global object or undefined in strict mode. You can force 'this' using bind, call, or apply. The most common bug I see is losing 'this' in callbacks, which is why I often use arrow functions or bind when passing methods around. Understanding 'this' is crucial for working with classes and event handlers."],
    },
    {
        text: "What is destructuring and how do you use it with nested objects and arrays?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.destructuring],
        answers: ["Destructuring is a clean way to extract values from objects and arrays into variables. For basic cases, you can pull properties directly like const { name, age } = user. With nested objects, you can chain the destructuring, like const { address: { city } } = user to get the city from a nested address object. For arrays, you use square brackets and the order matters - const [first, second] = array. You can also combine them, skip elements with commas, use default values, and rename variables during destructuring. It's really handy for function parameters too, especially in React where you often destructure props. One thing to watch out for is trying to destructure undefined or null - that'll throw an error, so you might need default values or optional chaining.",
            "Destructuring lets you unpack values from arrays or properties from objects into distinct variables in one line. For objects, use curly braces with property names. For arrays, use brackets and position matters. You can go as deep as needed - nested objects just require nested destructuring syntax. I use it constantly in React to destructure props and in function parameters to make APIs cleaner. You can provide defaults for missing values and rename variables on the fly. It's essential syntax for modern JavaScript."],
    },
    {
        text: "What is the spread operator and how does it differ from rest parameters?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum["spread-operator"]],
        answers: ["Both use the three dots syntax, but they do opposite things. The spread operator expands an array or object into individual elements, like when you want to copy an array or merge objects. For example, const newArray = [...oldArray] creates a shallow copy. Rest parameters, on the other hand, collect multiple arguments into an array - you use them in function parameters like function sum(...numbers) to handle any number of arguments. The key difference is spread spreads things out, while rest collects things together. I use spread constantly for creating copies without mutating the original, passing array elements as function arguments, or merging objects. Rest is great when you don't know how many arguments you'll get or when you want to handle the first few parameters separately and collect the rest.",
            "The three dots serve two purposes depending on context. Spread expands iterables - I use it to clone arrays, merge objects, or pass array elements as individual function arguments. Rest collects multiple values into a single array - perfect for functions with variable arguments or extracting 'the remaining' items. In function parameters, rest must come last. A quick mental model: spread unpacks, rest packs. Both are essential for immutable updates, which is why you see them everywhere in React state management."],
    },
    {
        text: "What are higher-order functions and how do map, filter, and reduce work?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript],
        answers: ["Higher-order functions are functions that either take other functions as arguments or return functions. Map, filter, and reduce are the most common ones for working with arrays. Map transforms each element and returns a new array of the same length - like converting an array of numbers to their doubles. Filter creates a new array with only elements that pass a test - like getting all users over 18. Reduce is the most powerful but takes some getting used to - it accumulates values into a single result, like summing numbers or grouping objects. The key thing is they don't mutate the original array, which is great for avoiding bugs. I use map when I need to transform data, filter when I need to narrow it down, and reduce when I need to calculate something or reshape data into a different structure.",
            "Higher-order functions take functions as arguments or return them. The array methods map, filter, and reduce are classics. Map returns a new array with each element transformed. Filter returns a new array with only elements passing your test. Reduce accumulates everything into a single value - numbers into a sum, objects into a lookup table, whatever you need. They don't mutate the original, which is perfect for React's immutability requirements. I chain these constantly for data transformations."],
    },
    {
        text: "What is the difference between shallow copy and deep copy?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript],
        answers: ["A shallow copy duplicates the top level of an object or array, but nested objects or arrays are still referenced, not copied. So if you have an object with nested objects and you make a shallow copy, changes to those nested objects affect both the original and the copy. The spread operator and Object.assign create shallow copies. A deep copy, on the other hand, recursively copies everything, including all nested structures, so the copy is completely independent. For simple cases, you might use JSON.parse and JSON.stringify for a deep copy, but that has limitations - it doesn't handle functions, dates, or circular references well. For production code, I'd usually reach for a library like lodash's cloneDeep or use structuredClone if the environment supports it. Understanding this distinction is crucial for avoiding subtle bugs with state management.",
            "Shallow copy duplicates the first level only - nested references point to the same memory as the original. Deep copy recursively clones everything, creating fully independent data. Spread and Object.assign are shallow. For deep copying, structuredClone is now built into browsers, or use a library like lodash. This matters immensely in React where you need immutable updates - accidentally mutating nested state causes bugs where components don't re-render. I always verify my copying approach matches my nesting depth."],
    },
    {
        text: "What is the difference between call, apply, and bind?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum["this-keyword"]],
        answers: ["All three methods let you explicitly set the 'this' context for a function, but they work differently. Call invokes the function immediately and you pass arguments individually - func.call(context, arg1, arg2). Apply is almost identical but takes arguments as an array - func.apply(context, [arg1, arg2]), which is handy when you already have arguments in array form. Bind is different because it doesn't call the function immediately - instead, it returns a new function with 'this' permanently bound to the context you specify. I use bind most often for event handlers or callbacks where I need to preserve 'this', and call or apply when I need to borrow a method from another object or when working with array-like objects. The mnemonic I remember is that apply takes an array, and bind returns a bound function.",
            "These are ways to control 'this' in function calls. Call executes immediately with arguments listed out. Apply executes immediately with arguments as an array. Bind returns a new function with 'this' locked in. I use bind when setting up event handlers where I need 'this' preserved for later. Call is handy for borrowing methods - like using array methods on array-like objects. With modern JavaScript, arrow functions and spread syntax have reduced how often I need these, but they're still essential for certain patterns."],
    },
    {
        text: "What is the difference between Object.freeze() and Object.seal()?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript],
        answers: ["Both methods prevent adding or removing properties from an object, but they differ in what you can do with existing properties. Object.seal() prevents adding new properties and deleting existing ones, but you can still modify the values of existing properties. Object.freeze() is stricter - it does everything seal does, plus it makes all existing properties read-only, so you can't change their values either. Neither method works recursively though, so nested objects can still be modified unless you freeze or seal them separately. I use freeze when I want to create truly immutable objects, like for constants or configuration objects that should never change. Seal is useful when you want a fixed structure but still need to update values. In practice, I don't use these super often because modern patterns like const and immutability libraries handle most use cases.",
            "Seal locks the object structure - no adding or removing properties - but values can still change. Freeze goes further and makes everything read-only. Neither is deep, so nested objects remain mutable unless you manually freeze them too. In practice, I use freeze for configuration objects or constants that shouldn't change. TypeScript's readonly is often more practical since it catches issues at compile time. But Object.freeze provides runtime protection, which matters when consuming untrusted data."],
    },

    // TypeScript
    {
        text: "What is TypeScript and what problems does it solve?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript],
        answers: ["TypeScript is a superset of JavaScript that adds static type checking. It compiles down to regular JavaScript, so it runs anywhere JavaScript runs. The main problem it solves is catching type-related errors at development time instead of runtime - like trying to call a method that doesn't exist or passing the wrong type of argument to a function. It also provides excellent IDE support with autocomplete and refactoring tools. I find it really valuable for larger codebases because the type system acts like documentation that's always up to date, and it makes refactoring so much safer. You can gradually adopt it too, starting with basic types and adding more sophisticated types as you go. The tradeoff is some extra setup and a learning curve, but for most projects, especially team projects, the benefits far outweigh the costs.",
            "TypeScript adds static types to JavaScript, catching errors before runtime. It compiles to plain JavaScript, so it works everywhere JS does. The value is tremendous for larger projects - types serve as living documentation, refactoring becomes safe, and autocomplete dramatically improves. It catches entire categories of bugs that would otherwise only appear in production. I consider it essential for any serious project. The learning curve pays off quickly, especially with modern tooling that handles most configuration automatically."],
    },
    {
        text: "What is the difference between type and interface?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.interfaces, ValidTag.enum.types],
        answers: ["Both can define object shapes, but they have some differences. Interfaces can be extended and merged - if you declare the same interface twice, TypeScript automatically merges them, which is useful for extending third-party types. Types are more flexible - you can use them for unions, intersections, primitives, and more complex type operations. For object shapes, they're mostly interchangeable, but interfaces tend to give slightly better error messages. In practice, I use interfaces for defining object structures and public APIs because they can be extended, and types for everything else like unions, mapped types, or when I need more advanced type features. Some teams have style guides that prefer one over the other, but honestly, either works fine for most cases as long as you're consistent.",
            "Interfaces are primarily for object shapes and can be extended or merged. Types are more versatile - unions, intersections, mapped types, and primitives all require type. For objects, they're largely interchangeable in modern TypeScript. My rule: use interface for objects that might be extended, type for everything else. Interfaces also produce cleaner error messages. Some codebases standardize on one or the other for consistency, which is fine. The capability differences matter less than they used to."],
    },
    {
        text: "What is the unknown type and how does it differ from any?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Unknown is a type-safe version of any. With any, you can do anything with the value - TypeScript basically turns off type checking for it, which defeats the purpose of using TypeScript. Unknown is safer because you can assign any value to it, but you can't do anything with it until you narrow the type or perform type checking. For example, if you have a value of type unknown, you need to check if it's a string before calling string methods on it. I use unknown when I'm working with values from external sources like API responses or user input where I genuinely don't know the type ahead of time. It forces you to handle the uncertainty explicitly, which catches bugs. Any should really only be used as a last resort or when gradually migrating JavaScript to TypeScript.",
            "Any disables type checking entirely - you can do anything with it, which defeats TypeScript's purpose. Unknown is the safe alternative - you can assign anything to it, but you must narrow the type before using it. This forces explicit handling of uncertain types. I use unknown for API responses, parsed JSON, or any external data. It makes you prove the type is what you expect before using it. Any is essentially an escape hatch that should be avoided in production code."],
    },
    {
        text: "What is type assertion and when should you avoid it?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Type assertion is when you tell TypeScript to treat a value as a specific type, using either 'as Type' or the angle bracket syntax. It's basically you overriding TypeScript's inference and saying 'trust me, I know better.' The problem is, you might be wrong, and TypeScript won't catch it at compile time, leading to runtime errors. You should avoid it when there's a better way to properly type your code - like using type guards, proper typing at the source, or narrowing types through control flow. I use type assertions sparingly, mainly when working with DOM elements where I know more about the actual type than TypeScript can infer, or when dealing with third-party libraries that have incomplete type definitions. But every time I use one, I double-check if there's a safer alternative.",
            "Type assertion overrides TypeScript's inference, telling the compiler you know the type better than it does. This bypasses type safety - if you're wrong, you get runtime errors. I avoid it when possible, preferring type guards or narrowing through conditionals. The valid use cases are when you genuinely know more than TypeScript, like with DOM queries where you know the element type. Each assertion is a potential bug, so I treat them as code smells worth investigating for safer alternatives."],
    },
    {
        text: "What is the difference between readonly and const?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript],
        answers: ["Const is a JavaScript keyword for variable declarations that prevents reassignment of the variable itself. Readonly is a TypeScript modifier for object properties that prevents those properties from being changed after initialization. So const prevents you from doing 'x = something else', while readonly prevents you from doing 'obj.property = something else'. A const object can still have its properties modified unless those properties are marked as readonly. Also, const is enforced at runtime in JavaScript, while readonly is only a compile-time TypeScript check that disappears when the code is compiled. I use const for all variables that shouldn't be reassigned, and readonly for object properties and array types when I want to ensure immutability. They complement each other - a const reference to an object with readonly properties gives you full immutability.",
            "Const prevents reassigning a variable - it's runtime JavaScript. Readonly prevents modifying object properties - it's compile-time TypeScript only, disappearing after compilation. A const object can still have its properties mutated unless those properties are readonly. For true immutability, you need both: const for the reference, readonly for the properties. I use const by default for all declarations, and readonly in interfaces and types to communicate immutability intent and catch accidental mutations during development."],
    },
    {
        text: "What is strict mode in TypeScript and what does it enable?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript],
        answers: ["Strict mode is a compiler option that enables a bunch of strict type-checking options all at once. It includes things like noImplicitAny, which requires you to explicitly type things instead of falling back to any, strictNullChecks which treats null and undefined as distinct types that you have to handle explicitly, and strictFunctionTypes for better checking of function parameter types. There are several other flags too. Enabling strict mode makes TypeScript much more rigorous about catching potential errors, but it also means more work upfront to satisfy the type checker. I always enable it for new projects because it catches so many potential bugs. For existing JavaScript projects being migrated to TypeScript, you might start without it and gradually enable strict checks as you improve the type coverage. It's definitely more typing work, but the safety is worth it.",
            "Strict mode enables several stricter compiler checks at once: no implicit any, strict null checks, stricter function types, and more. It makes TypeScript much safer by forcing you to handle edge cases explicitly. I enable it on every new project - it catches bugs that would otherwise surface in production. For migrations from JavaScript, you might enable strict checks incrementally. The extra upfront work of satisfying the type checker pays off in fewer runtime bugs and more maintainable code."],
    },
    {
        text: "What are enums and what are the differences between numeric, string, and const enums?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.enums],
        answers: ["Enums let you define a set of named constants, which is great for representing a fixed set of values like directions or status codes. Numeric enums auto-increment starting from 0, so if you define Up, Down, Left, Right, they get values 0, 1, 2, 3. String enums require explicit string values for each member, which makes them more self-documenting and better for debugging since you see the actual string value. Const enums are completely erased during compilation and get inlined wherever they're used, making them more efficient but you can't iterate over them or reference them dynamically. I prefer string enums for most cases because they're clearer in logs and debugging. Numeric enums are fine when you don't care about the actual values. Const enums are an optimization I only use when bundle size really matters.",
            "Enums define named constant sets. Numeric enums auto-increment from zero, which can cause surprises if you serialize them - the number 2 is less meaningful than a string. String enums are self-documenting in logs and debuggers, which is why I prefer them. Const enums inline values at compile time for smaller bundles but can't be iterated or referenced dynamically. Some teams prefer union types over enums for simpler code. I use enums when I need a closed set of related constants with good IDE support."],
    },

    // HTML
    {
        text: "What are data attributes and how do you use them?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.html],
        answers: ["Data attributes are custom attributes you can add to HTML elements using the data- prefix, like data-user-id or data-status. They're a standard way to store extra information on elements without using non-standard attributes or putting data in classes. In JavaScript, you can access them through the dataset property, which converts the kebab-case names to camelCase - so data-user-id becomes dataset.userId. They're perfect for storing metadata that your JavaScript needs, like IDs for API calls, configuration options, or state information. I use them all the time for things like tracking analytics events, storing identifiers for dynamic content, or passing data to JavaScript components. They're also useful for CSS selectors when you want to style elements based on their state. Just remember they're publicly visible in the HTML, so don't put sensitive data in them.",
            "Data attributes store custom metadata on HTML elements using the data- prefix. Access them via element.dataset in JavaScript, where kebab-case converts to camelCase. They're perfect for configuration, IDs, or state that JavaScript needs but doesn't belong in classes or non-standard attributes. I use them for analytics tracking, storing entity IDs, and configuration for JavaScript widgets. CSS can also target them with attribute selectors for state-based styling. Keep in mind they're visible in the DOM - never store sensitive data."],
    },
    {
        text: "What is the difference between <script>, <script async>, and <script defer>?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.html, ValidTag.enum.performance],
        answers: ["These control how and when scripts load and execute. A regular script tag blocks HTML parsing while the script downloads and executes, which can slow down page load. Async downloads the script in parallel with HTML parsing and executes it as soon as it's ready, which can interrupt parsing. Defer also downloads in parallel but waits to execute until HTML parsing is complete, and multiple deferred scripts execute in order. For performance, I use defer for scripts that need the DOM or depend on other scripts, because it doesn't block parsing and maintains execution order. Async is good for independent scripts like analytics that don't need the DOM or other scripts. Regular script tags are mostly just for inline scripts or when you specifically need blocking behavior, which is rare.",
            "Regular script blocks parsing - the browser stops and waits. Async downloads in parallel and executes immediately when ready, potentially interrupting parsing. Defer downloads in parallel but executes after parsing, preserving order for multiple deferred scripts. I use defer for most scripts since they typically need the DOM and other scripts. Async is ideal for isolated scripts like analytics or ads that don't depend on anything. This is a simple performance win for any project."],
    },
    {
        text: "What are meta tags and which ones are important for SEO?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.html, ValidTag.enum.seo],
        answers: ["Meta tags provide metadata about your HTML document and go in the head section. For SEO, the most important ones are the title tag (technically not a meta tag, but crucial), meta description which shows up in search results, and the viewport meta tag for mobile responsiveness. You also want Open Graph tags for social media sharing - og:title, og:description, og:image - and similar Twitter card tags. The charset meta tag is essential for proper character encoding. The robots meta tag can control indexing and following links. Canonical tags help prevent duplicate content issues. I make sure every page has a unique, descriptive title and meta description, proper Open Graph tags, and viewport settings. Keywords meta tag used to matter but search engines mostly ignore it now. The key is making these tags descriptive and relevant to the actual content.",
            "Essential meta tags for SEO include: title and description for search results, viewport for mobile responsiveness, canonical to prevent duplicate content issues, and Open Graph tags for social sharing. The charset meta ensures proper encoding. Robots meta controls indexing. Each page should have unique, descriptive title and description that accurately represent the content. The old keywords meta is obsolete. Modern frameworks like Next.js have APIs for managing metadata, making it easier to set these per page."],
    },
    {
        text: "What are web components and shadow DOM?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.html, ValidTag.enum.javascript],
        answers: ["Web components are a set of web platform APIs that let you create reusable custom elements with encapsulated functionality. They consist of three main technologies: custom elements for defining new HTML tags, shadow DOM for encapsulation, and HTML templates for defining reusable markup. Shadow DOM is the encapsulation piece - it creates a separate DOM tree attached to an element that's isolated from the main document. Styles and scripts in the shadow DOM don't leak out, and global styles don't leak in, which solves a lot of CSS collision problems. It's what gives web components their true encapsulation. I think web components are really powerful for creating design system components that need to work across different frameworks, though for most projects, I'd still reach for React or Vue since their ecosystems are more mature.",
            "Web components let you create custom HTML elements using native browser APIs. The shadow DOM provides style and markup encapsulation - CSS doesn't leak in or out. Custom elements define new tags. Templates provide reusable markup. The main value is framework-agnostic components that work anywhere. Design systems use them to share components across React, Vue, and vanilla JS projects. For single-framework apps, React or Vue components are simpler. Web components shine when you need true cross-framework interoperability."],
    },
    {
        text: "What are forms and how do you handle form validation?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.html, ValidTag.enum.forms, ValidTag.enum.validation],
        answers: ["Forms are HTML structures for collecting user input, using the form element with various input types like text, email, checkbox, etc. For validation, there are two approaches: HTML5 built-in validation using attributes like required, pattern, min, max, which is easy and works without JavaScript, and custom JavaScript validation for more complex rules. HTML5 validation is great for basic checks and provides good user experience with browser-native error messages. For anything more sophisticated, I use JavaScript to validate on both input and submit events, showing custom error messages. The key is to always validate on the server too, since client-side validation can be bypassed. In React, I often use libraries like react-hook-form or Formik which handle validation, state management, and error display. Good form validation should be immediate enough to help users but not annoying, and error messages should be clear about how to fix the problem.",
            "Forms collect user input through various input types, selects, and textareas. Validation happens on two levels: HTML5 attributes for basic validation, JavaScript for complex rules. Built-in HTML validation gives you required fields, pattern matching, and type validation for free. For sophisticated validation, libraries like react-hook-form or Zod handle the heavy lifting. Always validate server-side too - client validation is for UX, server validation is for security. Good forms provide immediate feedback without being intrusive."],
    },
    // CSS
    {
        text: "What is specificity and how is it calculated?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css],
        answers: ["Specificity determines which CSS rule applies when multiple rules target the same element. It's calculated based on the selector types: inline styles are most specific, then IDs, then classes/attributes/pseudo-classes, then elements/pseudo-elements. You can think of it as a point system - IDs are worth 100 points, classes are 10, elements are 1. So a selector like '#header .nav a' has a specificity of 111. When specificities are equal, the last rule wins. The !important flag overrides everything, but it's a bad practice that makes debugging harder. I try to keep specificity low by using classes instead of IDs for styling and avoiding overly nested selectors. If you're fighting specificity with !important or super specific selectors, it's usually a sign your CSS architecture needs refactoring.",
            "Specificity is a scoring system that determines which CSS rules win when multiple rules target the same property. Inline styles beat IDs, IDs beat classes, classes beat elements. Calculate it as three numbers: ID count, class count, element count. Higher numbers in earlier positions win. Keep specificity low by using classes primarily and avoiding deep nesting. !important overrides everything but creates maintenance nightmares. When I see !important in code, I look for the underlying specificity problem instead of adding more."],
    },
    {
        text: "What is the difference between Flexbox and Grid and when would you use each?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.flexbox, ValidTag.enum.grid],
        answers: ["Flexbox is one-dimensional, designed for laying out items in a row or column, while Grid is two-dimensional, designed for both rows and columns simultaneously. Flexbox is great for components and smaller-scale layouts - navigation bars, button groups, centering items, distributing space between items. Grid excels at page-level layouts and complex two-dimensional designs - magazine-style layouts, dashboards, card grids where you want precise control over both dimensions. You can use them together too - Grid for the overall page structure and Flexbox for the components within grid cells. I reach for Flexbox when I'm arranging items along a single axis and want them to be flexible and responsive. I use Grid when I need to align content in both dimensions or when I want to create a complex layout without nested containers.",
            "Flexbox handles one dimension - either row or column. Grid handles two dimensions simultaneously. I use Flexbox for component-level layout: navigation, button groups, centering, equal spacing. I use Grid for page-level layout and anything needing precise row and column control. They work great together - Grid for the page skeleton, Flexbox for components inside grid areas. The modern approach is defaulting to Flexbox and reaching for Grid when you genuinely need two-dimensional control."],
    },
    {
        text: "What are pseudo-classes and pseudo-elements?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css],
        answers: ["Pseudo-classes select elements based on their state or position, using a single colon like :hover, :focus, :first-child, :nth-child. They target existing elements in specific conditions. Pseudo-elements create virtual elements that don't exist in the HTML, using double colons like ::before, ::after, ::first-letter, ::first-line. They let you style specific parts of an element or insert generated content. For example, ::before can add decorative content before an element. I use pseudo-classes constantly for interactive states like hover effects and form validation styling. Pseudo-elements are great for decorative elements like icons, quotes, or clearfix hacks without cluttering the HTML. A simple way to remember: pseudo-classes select what exists, pseudo-elements create what doesn't. The double colon syntax for pseudo-elements helps distinguish them, though single colons still work for backward compatibility.",
            "Pseudo-classes (:hover, :focus, :nth-child) select elements based on state or position - they filter what already exists. Pseudo-elements (::before, ::after) create virtual elements for styling purposes without adding HTML markup. I use pseudo-classes for interaction states and structural selection. I use pseudo-elements for decorative additions, custom bullets, overlays, or the classic clearfix. Double colons for pseudo-elements distinguish them from pseudo-classes, though browsers still accept single colons for legacy pseudo-elements."],
    },
    {
        text: "What is z-index and how does stacking context work?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css],
        answers: ["Z-index controls the stacking order of positioned elements - higher values appear in front. But it only works on positioned elements, meaning position must be something other than static. The tricky part is stacking contexts - certain properties create a new stacking context, like positioning with z-index, opacity less than 1, transforms, and several others. Within a stacking context, z-index values only compete with siblings in that same context. So if a parent has z-index 1 and a child has z-index 9999, the child still can't appear above an element outside that context with z-index 2. This trips people up all the time. I've learned to check if a new stacking context was unintentionally created when z-index isn't working as expected. For managing z-index at scale, I use CSS variables or constants to maintain a clear hierarchy rather than arbitrary numbers.",
            "Z-index sets stacking order for positioned elements - higher values go on top. The catch is stacking contexts: certain CSS properties create isolated contexts where z-index only competes within that context, not globally. Opacity, transforms, and position with z-index all create new contexts. When z-index doesn't work as expected, an unintended stacking context is usually the culprit. I define a z-index scale in variables for consistency and audit for accidental context creation when debugging stacking issues."],
    },
    {
        text: "What are media queries and how do you implement responsive design?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum["media-queries"], ValidTag.enum["responsive-design"]],
        answers: ["Media queries let you apply CSS conditionally based on device characteristics, primarily screen width. They're the foundation of responsive design. You use @media with conditions like min-width or max-width to change styles at different breakpoints. For responsive design, I start with a mobile-first approach, writing base styles for mobile and using min-width media queries to progressively enhance for larger screens. Common breakpoints are around 640px for tablets and 1024px for desktops, but I choose based on where the design actually breaks rather than specific devices. Beyond width, you can query for things like orientation, hover capability, and color scheme preferences for dark mode. The key to good responsive design is fluid layouts using percentages or viewport units, flexible images with max-width 100%, and thoughtful breakpoints that serve the content, not just popular device sizes.",
            "Media queries apply CSS based on viewport characteristics like width, orientation, or color-scheme preference. I use mobile-first with min-width breakpoints, writing base mobile styles and progressively enhancing for larger screens. Breakpoints should follow content needs, not specific device sizes. Beyond basic responsive layout, modern queries can detect hover capability, prefers-reduced-motion for accessibility, and prefers-color-scheme for dark mode. Container queries are the newer evolution, letting components respond to their container's size rather than the viewport."],
    },
    {
        text: "What are CSS transitions and animations?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.animations, ValidTag.enum.transitions],
        answers: ["Transitions and animations both create motion in CSS, but they work differently. Transitions animate from one state to another when triggered by a change, like hovering or toggling a class. You specify which properties to transition, the duration, and timing function. They're perfect for simple state changes like button hovers or showing and hiding elements. Animations use keyframes to define multiple states and can run automatically, loop, and have more complex timing. You define the animation with @keyframes and apply it with the animation property. I use transitions for most interactive UI elements because they're simpler and tied to user actions. Animations are better for loading spinners, attention-grabbing effects, or anything that needs to run independently of state changes. For performance, I stick to animating transform and opacity when possible, since those are GPU-accelerated and won't cause layout recalculations.",
            "Transitions smoothly animate between two states on property changes - great for hover effects, toggles, and state changes. Animations use @keyframes to define multiple states and can run automatically, loop, or play in reverse. I use transitions for interactive feedback and animations for continuous motion like loaders. For performance, stick to transform and opacity since they're GPU-accelerated. Animating layout properties like width or top causes expensive reflows. Also consider prefers-reduced-motion for accessibility."],
    },

    // CSS Modules and Tailwind
    {
        text: "What are CSS Modules and how do they prevent naming collisions?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum["css-modules"]],
        answers: ["CSS Modules are CSS files where class names are locally scoped by default. When you import a CSS Module, the build tool automatically generates unique class names by hashing them, so .button might become .button_a3x2k in the final CSS. This prevents naming collisions across different components because each component's styles are isolated. You write normal CSS but import it like a JavaScript module and reference classes through that object, like styles.button. It solves the global namespace problem in CSS without completely changing how you write styles. I really like CSS Modules for component-based architectures because you can use simple, semantic class names without worrying about conflicts, and the scoping makes it clear which styles apply to which components. They work great with React and other component frameworks, and you still get the benefits of regular CSS like preprocessors and better IDE support compared to CSS-in-JS.",
            "CSS Modules scope class names to the file by hashing them during build, preventing naming collisions without changing how you write CSS. Import the module and reference classes as properties: styles.button. You get isolated styles per component while keeping standard CSS syntax and tooling. It's simpler than CSS-in-JS with similar benefits. The explicit import also makes it clear which styles affect which components. I use CSS Modules when I want scoped styles without the runtime overhead of styled-components."],
    },
    {
        text: "What is Tailwind CSS and what problems does it solve?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum.css],
        answers: ["Tailwind is a utility-first CSS framework that provides low-level utility classes for building designs directly in your HTML. Instead of writing custom CSS, you compose styles using classes like flex, pt-4, text-center. It solves several problems: you don't have to context-switch between HTML and CSS files, you avoid naming fatigue from inventing class names, and it enforces consistency through a design system. The utility classes are constrained to a design token system, so spacing, colors, and sizes stay consistent. It also has great tree-shaking, so unused styles are removed from production builds. The downside is HTML can get verbose with all the classes. I find Tailwind really speeds up development once you learn the class names, and it's particularly good for teams because it creates a shared vocabulary for styling. The configuration file lets you customize the design system to match your brand.",
            "Tailwind provides utility classes for styling directly in markup, eliminating the need for custom CSS files and class naming. The utilities follow a consistent design system for spacing, colors, and typography. Production builds purge unused classes for small bundles. Development is fast once you learn the naming conventions, and the VS Code extension provides excellent autocomplete. The trade-off is verbose class lists, but component abstraction mitigates that. It works especially well with component frameworks where styles stay colocated with markup."],
    },
    {
        text: "How do you handle responsive design in Tailwind?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind, ValidTag.enum["responsive-design"]],
        answers: ["Tailwind uses a mobile-first approach with responsive prefixes. You apply base styles for mobile, then add prefixed utilities for larger screens like md:flex or lg:grid. The default breakpoints are sm at 640px, md at 768px, lg at 1024px, xl at 1280px, and 2xl at 1536px. So if you write 'text-sm md:text-base lg:text-lg', the text starts small on mobile and gets larger on medium and large screens. This is really intuitive once you get used to it - you can see all the responsive behavior right in the class names. You can also customize breakpoints in the config file if the defaults don't match your design. I like how explicit it is compared to writing media queries - you can immediately see what happens at different screen sizes. The mobile-first approach also encourages you to think about the mobile experience first, which usually leads to better responsive designs.",
            "Tailwind handles responsive design with breakpoint prefixes: sm:, md:, lg:, xl:, 2xl:. Write mobile styles first, then override with prefixed classes for larger screens. 'hidden md:block' means hidden on mobile, visible from medium screens up. All responsive behavior is visible right in the class list, making it clear what happens at each breakpoint. Customize breakpoints in the config if needed. This mobile-first approach with explicit breakpoint classes is more readable than hunting through CSS files for media queries."],
    },
    {
        text: "What is the Tailwind configuration file and how do you customize it?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind],
        answers: ["The tailwind.config.js file is where you customize your Tailwind setup. You can extend or override the default theme with your brand colors, custom spacing values, fonts, breakpoints, and more. The 'extend' key adds to the defaults, while directly modifying theme properties replaces them. You also configure the content paths here - telling Tailwind which files to scan for class names so it can purge unused styles. I typically extend the theme with project-specific colors and maybe custom spacing or font sizes while keeping most defaults. You can also add plugins, configure dark mode, and set up custom variants. It's really powerful for creating a consistent design system. For example, adding your brand colors means you can use them like text-brand-primary or bg-brand-secondary throughout your app. The key is finding the right balance between customization and keeping the helpful defaults.",
            "The config file customizes your Tailwind design system. Use 'extend' to add custom values while preserving defaults, or replace theme sections entirely. Critical settings include content paths for class purging, custom colors for branding, and plugins for additional utilities. You can add brand colors so 'bg-brand-primary' works everywhere, custom spacing scales, and font configurations. Dark mode settings and custom variants also go here. I keep customizations minimal initially and add as needed to maintain consistency."],
    },

    // React
    {
        text: "What are the rules of hooks?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["There are two main rules: first, only call hooks at the top level - never inside loops, conditions, or nested functions. Second, only call hooks from React function components or custom hooks, not regular JavaScript functions. These rules exist because React relies on the order of hook calls to maintain state correctly between renders. If you conditionally call a hook, the order might change, breaking React's internal state tracking. The eslint plugin for hooks helps catch violations automatically. In practice, this means you can't do something like 'if (condition) useState()', but you can use the hook unconditionally and use conditions with the value. For conditional logic, you move the condition inside the hook or effect. Once you understand why these rules exist, they become second nature, and the linter catches mistakes anyway.",
            "Two rules govern hooks: call them at the top level only, never in conditions or loops; call them only from React functions or custom hooks. React tracks hooks by call order, so conditional calls break state tracking. Don't conditionally call hooks - instead, call the hook unconditionally and handle conditions in your logic. The eslint-plugin-react-hooks catches violations automatically and is essential for any React project. Understanding why these rules exist - React's reliance on stable hook ordering - makes them easy to follow."],
    },
    {
        text: "What is the component lifecycle and how do hooks map to lifecycle methods?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.lifecycle, ValidTag.enum.hooks],
        answers: ["In class components, the lifecycle has mounting (componentDidMount), updating (componentDidUpdate), and unmounting (componentWillUnmount) phases. With hooks, useEffect handles all of these. A useEffect with an empty dependency array runs once after mount, like componentDidMount. With dependencies, it runs on mount and whenever those dependencies change, like componentDidUpdate. The cleanup function returned from useEffect runs on unmount, like componentWillUnmount. However, it's better to think in terms of synchronization rather than lifecycle - useEffect keeps something in sync with props or state. If you need to run an effect only once, use the empty array. If it depends on certain values, include them in the dependency array. The ESLint plugin helps ensure you list all dependencies. This mental model shift from lifecycle methods to synchronization effects was tricky at first, but it's more flexible and composable.",
            "Class components have distinct lifecycle phases: mount, update, unmount. Hooks unify these through useEffect. Empty dependency array means run on mount only, populated array means run when dependencies change, cleanup function handles unmount. The better mental model isn't lifecycle mapping but synchronization - useEffect keeps external systems in sync with React state. This approach is more composable than class lifecycles since you can have multiple effects for different concerns rather than cramming everything into componentDidMount."],
    },
    {
        text: "What is prop drilling and why is it a problem?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Prop drilling is when you pass props through multiple layers of components just to get them to a deeply nested component that actually needs them. The intermediate components don't use these props, they just pass them along, which clutters their interfaces and makes refactoring harder. It's a problem because it creates tight coupling between components and makes the codebase harder to maintain. If you need to change the prop structure, you have to update all the intermediate components too. There are several solutions: Context API lets you skip the intermediate components entirely, composition with the children prop can restructure components to avoid drilling, and state management libraries like Redux or Zustand provide global state. For small amounts of drilling, it's fine to just pass props, but for deeply nested or widely used data like theme or user info, I reach for Context or a state management solution.",
            "Prop drilling passes data through intermediate components that don't use it, just to reach a nested component that does. It's problematic because it couples components tightly and makes refactoring tedious - every intermediate component must be updated. Solutions include React Context for skipping intermediate layers, component composition using children to restructure the tree, or state management libraries. Some drilling is fine, but widely-used data like user info or theme should use Context. I evaluate depth and usage frequency when deciding."],
    },
    {
        text: "What are keys in React and why are they important?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["Keys help React identify which items in a list have changed, been added, or removed. They should be stable, unique identifiers - usually IDs from your data. React uses keys to optimize rendering by reusing existing DOM elements instead of destroying and recreating them. Without proper keys, React falls back to using the array index, which can cause bugs when the list order changes - you might see incorrect data displayed, lost input state, or poor performance. The key should uniquely identify the item itself, not its position. So database IDs are perfect, but array indices are problematic if items can be reordered, added, or removed from anywhere but the end. I always use stable IDs when available. If you absolutely don't have them, generating them with a library like uuid during data creation works, but never generate keys during render because they'll be different each time.",
            "Keys uniquely identify list items so React can efficiently track changes, additions, and removals. Use stable identifiers like database IDs, not array indices which shift with list mutations. Bad keys cause subtle bugs: input focus lost, animation glitches, stale state. React falls back to indices without explicit keys, which works only for static lists. Generate IDs at data creation time if your data lacks them, never during render. This is a common interview topic because the bugs are hard to diagnose."],
    },
    {
        text: "What is the difference between controlled and uncontrolled components?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.forms],
        answers: ["Controlled components have their form data managed by React state - the value comes from state and updates through event handlers like onChange. This gives you full control and makes it easy to implement validation, formatting, or conditional logic. Uncontrolled components manage their own state internally in the DOM, and you access their values using refs when you need them, like on form submission. Controlled components are more React-ish and give you more power, but they can be verbose for simple forms. Uncontrolled components are closer to traditional HTML forms and can be simpler for basic cases. I default to controlled components because they integrate better with React's unidirectional data flow and make form state predictable. Uncontrolled can be useful for file inputs (which can't be controlled) or when integrating with non-React code. Most form libraries like react-hook-form optimize controlled components to avoid performance issues.",
            "Controlled inputs have their value driven by React state with onChange handlers updating that state. Uncontrolled inputs manage their own DOM state, accessed via refs when needed. Controlled gives you real-time access for validation and formatting. Uncontrolled is simpler for basic forms and required for file inputs. I prefer controlled for the predictability and integration with React's data flow. Form libraries like react-hook-form bridge both approaches, providing controlled-like APIs with uncontrolled performance."],
    },

    // Git
    {
        text: "What is the difference between merge and rebase?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.merge, ValidTag.enum.rebase],
        answers: ["Merge combines two branches by creating a new merge commit that has both branches as parents, preserving the complete history and branch structure. Rebase takes your commits and replays them on top of another branch, creating new commits with the same changes but different commit hashes, resulting in a linear history. Merge keeps the full context of when and how branches diverged and converged. Rebase rewrites history to make it look like your work was done on top of the latest code. The key thing is never rebase commits that you've pushed to a shared branch, because it changes commit hashes and will cause problems for others. I use merge for integrating feature branches into main because it preserves the feature branch history. I might use rebase on my local feature branch to clean up messy commits before pushing, or to update my branch with the latest main before creating a pull request.",
            "Merge preserves history by creating a merge commit with two parents. Rebase rewrites history by replaying your commits on top of another branch, creating new commit hashes and a linear history. Never rebase commits already pushed to shared branches - it rewrites history others depend on. I merge for integrating completed work and rebase locally for cleaning up before sharing. Teams vary on preference, but understanding both lets you adapt to any workflow."],
    },
    {
        text: "When would you use merge vs rebase?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.merge, ValidTag.enum.rebase],
        answers: ["I use merge when integrating completed work into main branches or when working on shared branches, because it preserves history and doesn't cause issues for collaborators. Merge is also good when the branch history itself is meaningful and you want to preserve the context of how the feature developed. I use rebase for cleaning up my local work before sharing it - like squashing 'fix typo' commits or reorganizing commits logically. It's also useful for updating a feature branch with the latest main before opening a pull request, giving a cleaner history. The golden rule is never rebase anything that's been pushed to a shared branch. Some teams have a rebase-heavy workflow where feature branches are always rebased and only the final merge is a true merge. Other teams prefer merge-only for simplicity. I think the hybrid approach works well: rebase locally for cleanup, merge for integration.",
            "Merge for shared or completed work, rebase for local cleanup. I merge feature branches into main to preserve the feature's development history. I rebase locally to tidy commits before pushing - squashing WIP commits or updating with the latest main for a clean PR. The cardinal rule: never rebase shared history. Different teams have preferences, but understanding both lets me adapt. When updating a feature branch, some prefer rebase for cleaner history, others merge main in to avoid rewriting."],
    },
    {
        text: "What is git stash and how do you use it?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git stash temporarily saves your uncommitted changes and reverts your working directory to a clean state, letting you switch contexts without committing half-done work. You use 'git stash' to save changes, 'git stash pop' to reapply them, and 'git stash list' to see all stashes. It's super useful when you need to quickly switch branches to fix a bug or review something, but you're in the middle of work that isn't ready to commit. By default it only stashes tracked files, but you can use 'git stash -u' to include untracked files. You can also create multiple stashes and apply specific ones by their index. I use stash all the time when I get interrupted or need to change contexts quickly. Just remember that stashes are local to your repository - they don't get pushed or shared, so they're only for temporary personal storage.",
            "Stash saves uncommitted work temporarily so you can switch contexts with a clean working directory. 'git stash' saves, 'git stash pop' restores and removes, 'git stash apply' restores but keeps the stash. Add -u to include untracked files. I use it constantly when interrupted mid-work or needing to quickly check another branch. You can maintain multiple stashes and reference them by index. They're local only - not pushed to remote. Name stashes with 'git stash save \"message\"' to remember what they contain."],
    },
    {
        text: "What are Git branches and branching strategies?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.branching],
        answers: ["Branches are lightweight pointers to commits that let you work on different features or fixes in isolation. They're fundamental to Git workflow because they enable parallel development without affecting the main codebase. Common branching strategies include Git Flow, which uses main, develop, feature, release, and hotfix branches for structured releases, and trunk-based development where everyone works off main with short-lived feature branches. GitHub Flow is simpler - just main and feature branches with pull requests for integration. I've found that smaller teams do well with GitHub Flow because it's straightforward and encourages continuous deployment. Larger teams might need Git Flow's structure for managing releases. The key is choosing a strategy that matches your deployment frequency and team size. Whatever strategy you use, keep feature branches short-lived and merge frequently to avoid painful merge conflicts.",
            "Branches isolate parallel development without affecting the main codebase. Strategies vary by team needs. GitHub Flow is simple: main plus short-lived feature branches with PRs. Git Flow adds develop, release, and hotfix branches for structured releases. Trunk-based development keeps everyone on main with tiny feature branches. The right choice depends on release cadence and team size. Whatever strategy, keep branches short-lived and merge frequently to minimize conflict pain."],
    },
    {
        text: "What is a pull request and what makes a good one?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum["pull-requests"]],
        answers: ["A pull request is a way to propose changes to a codebase and request that someone review and merge them. It's not actually a Git feature but a GitHub/GitLab/Bitbucket feature that facilitates code review and collaboration. A good PR is focused on a single feature or fix, has a clear title and description explaining what changed and why, includes tests if applicable, and is small enough to review in a reasonable time. The description should give context - what problem are you solving, any tradeoffs or decisions made, and how to test it. I try to keep PRs under 400 lines of changes when possible, because smaller PRs get reviewed faster and more thoroughly. Screenshots or videos help for UI changes. Self-reviewing your own PR before requesting reviews catches a lot of issues. Good PRs also respond to feedback graciously and keep the conversation focused on the code, not personal.",
            "Pull requests propose changes for review before merging. Good PRs are focused on one thing, small enough to review thoroughly, and include clear descriptions of what and why. Describe the problem solved, any tradeoffs, and how to test. Include screenshots for UI changes. Self-review before requesting others. Smaller PRs get faster, better reviews - I aim for under 400 lines. Respond to feedback constructively and keep discussions about the code. PRs are also documentation for future developers understanding why changes were made."],
    },
    {
        text: "What is the difference between git fetch and git pull?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git fetch downloads changes from the remote repository but doesn't integrate them into your current branch - it just updates your local copy of remote branches. Git pull does fetch plus merge, automatically merging the remote changes into your current branch. Fetch is safer because it lets you see what changed before integrating. I can fetch, look at the changes with git log or git diff, and then decide whether to merge or rebase. Pull is convenient when you trust the remote changes and want them immediately. However, pull can sometimes create unexpected merge commits if you have local changes. I generally prefer fetch for more control, especially on shared branches where I want to review incoming changes first. Some people configure pull to rebase instead of merge with git config pull.rebase true, which keeps a cleaner history. Understanding the difference helps you be more intentional about integrating remote changes.",
            "Fetch downloads remote changes without integrating them - it updates your local view of remote branches. Pull does fetch plus merge in one step. Fetch is safer for reviewing changes before integrating; pull is convenient when you trust updates. I often fetch first, review with git log origin/main, then decide to merge or rebase. Pull can create unexpected merge commits with local changes. Configure 'git config pull.rebase true' if you prefer rebasing on pull for cleaner history."],
    },
    {
        text: "How do you write good commit messages?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Good commit messages have a concise subject line (under 50 characters) that completes the sentence 'This commit will...', followed by a blank line and then a more detailed explanation if needed. The subject should be imperative mood - 'Add feature' not 'Added feature'. The body should explain what and why, not how - the code shows how. I write messages thinking about future developers (including myself) who need to understand why a change was made. Conventional Commits is a popular format that prefixes commits with types like feat:, fix:, docs:, which helps with automated changelog generation. Some teams require referencing ticket numbers. The key is being clear and descriptive without being verbose. Bad messages like 'fix bug' or 'updates' are useless in git log. Good messages like 'Fix memory leak in image upload by clearing event listeners' tell a story and make debugging and code archaeology much easier.",
            "Good commits have a short imperative subject under 50 characters, optionally followed by a blank line and detailed body. Write 'Add feature' not 'Added feature.' The body explains why, not how - code shows how. Think about future developers needing to understand the change. Conventional Commits prefixes like 'feat:' or 'fix:' enable automated changelogs. 'Fix bug' is useless; 'Fix race condition in checkout by debouncing submit' is searchable and informative. Good messages make git log and bisect actually useful."],
    },
];
