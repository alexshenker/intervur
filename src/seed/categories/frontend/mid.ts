import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.frontend,
    typeof Level.enum.mid
>[] = [
    // JavaScript
    {
        text: "What is the event loop and how does it handle asynchronous operations?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum["event-loop"], ValidTag.enum["async-await"]],
        answers: ["The event loop is JavaScript's mechanism for handling asynchronous operations in a single-threaded environment. It continuously monitors the call stack and task queues. When the call stack is empty, it takes the first task from the queue and pushes it onto the stack for execution. Async operations like setTimeout or API calls are handled by browser APIs or Node.js APIs outside the main thread. When they complete, their callbacks are placed in either the microtask queue for Promises or the macrotask queue for things like setTimeout. Microtasks always run before macrotasks, which is why Promise callbacks execute before setTimeout callbacks even if they're scheduled at the same time.",
            "The event loop enables JavaScript's non-blocking behavior despite being single-threaded. It runs synchronous code first, then processes microtasks like Promise callbacks, then macrotasks like setTimeout. Browser APIs handle async operations off the main thread, queuing callbacks when complete. Understanding this explains why a Promise.resolve callback runs before a setTimeout with 0ms delay. When debugging async issues, visualizing the call stack and queues helps predict execution order."],
    },
    {
        text: "What are Promises and how do they differ from async/await?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.promises, ValidTag.enum["async-await"]],
        answers: ["Promises are objects representing the eventual completion or failure of an asynchronous operation. They have three states: pending, fulfilled, or rejected, and you chain operations using .then() and .catch(). Async/await is syntactic sugar built on top of Promises that makes asynchronous code look more synchronous and easier to read. The key difference is readability and error handling - with async/await you can use try/catch blocks instead of chaining .catch(), and the code flows more naturally. However, they're fundamentally the same under the hood. One thing to note is that async/await makes sequential operations easier to write, but you need to be careful not to serialize operations that could run in parallel - for that, you'd still use Promise.all().",
            "Promises handle async operations with pending, fulfilled, and rejected states, chaining via then and catch. Async/await is syntactic sugar that makes Promise code read like synchronous code. I prefer async/await for readability - try/catch is cleaner than chaining. The gotcha is accidentally serializing independent operations. 'await a(); await b()' runs sequentially; use 'await Promise.all([a(), b()])' for parallel execution. Under the hood, async functions return Promises, so they're fully interoperable."],
    },
    {
        text: "What is event delegation and why is it useful?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript],
        answers: ["Event delegation is a technique where you attach a single event listener to a parent element instead of multiple listeners to individual child elements. It works because of event bubbling - when an event occurs on a child element, it bubbles up through its ancestors. You can check event.target to see which specific element triggered the event. This is really useful for performance when you have many similar elements like list items or table rows, since you only need one listener instead of hundreds. It also handles dynamically added elements automatically without needing to attach new listeners. I use this pattern frequently when building things like todo lists or data tables where items are added and removed dynamically.",
            "Event delegation leverages bubbling by placing one listener on a parent instead of many on children. Check event.target to identify which child triggered it. Benefits are performance with many elements and automatic handling of dynamically added items. Without delegation, adding list items requires attaching listeners to each one. With it, new items automatically work through the parent listener. I use it for lists, tables, or any repeated interactive elements. React handles this automatically with its synthetic event system."],
    },
    {
        text: "What are generators and iterators?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.generators, ValidTag.enum.iterators],
        answers: ["Iterators are objects that implement the iterator protocol by having a next() method that returns an object with 'value' and 'done' properties. They let you define custom iteration behavior for objects. Generators are special functions defined with function* that can pause and resume execution using the yield keyword. When you call a generator function, it returns a generator object which is also an iterator. Generators make it much easier to create iterators without manually implementing the iterator protocol. They're useful for lazy evaluation, like generating infinite sequences, or for managing complex async flows. Redux-Saga, for example, uses generators heavily for side effect management. Though honestly, in modern applications, you see async/await more often than generators for async operations.",
            "Iterators define how to traverse a data structure via the next() method returning {value, done}. Generators simplify creating iterators with function* and yield syntax. They pause execution at each yield, resuming on next call. Use cases include lazy evaluation of infinite sequences, streaming data processing, or complex async flows like Redux-Saga. While generators can handle async patterns, async/await is now preferred for most async work. Understanding them helps when encountering Saga-style code or implementing custom iterables."],
    },
    {
        text: "What are Web Workers and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.performance],
        answers: ["Web Workers are JavaScript scripts that run in background threads separate from the main browser thread. They let you perform heavy computations without blocking the UI. You communicate with workers through message passing using postMessage and onmessage. I'd use them for CPU-intensive tasks like image processing, large data parsing, complex calculations, or cryptographic operations. The main limitation is that workers don't have access to the DOM - they run in a completely separate context. A common use case would be processing a large CSV file or performing real-time data analysis without freezing the interface. However, there's overhead in setting them up and transferring data, so for quick operations they might not be worth it.",
            "Web Workers run JavaScript in background threads without blocking the UI. Communicate via postMessage - data is copied, not shared, though Transferable objects can be moved for efficiency. Use them for CPU-heavy tasks: image manipulation, large data parsing, complex calculations. The limitation is no DOM access. There's setup overhead, so only worthwhile for significant computation. I've used them for processing large spreadsheets and real-time audio analysis. Service Workers are a special type for offline caching and network interception."],
    },
    {
        text: "What is prototype inheritance and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.prototypes],
        answers: ["JavaScript uses prototype-based inheritance where objects can inherit properties and methods from other objects. Every object has an internal [[Prototype]] property that points to another object. When you try to access a property, JavaScript first looks on the object itself, and if it's not found, it walks up the prototype chain until it finds the property or reaches null. Functions have a prototype property that becomes the [[Prototype]] of instances created with the new keyword. While we now have class syntax in ES6, it's just syntactic sugar over this prototype system. Understanding prototypes is important for debugging and understanding how inheritance actually works under the hood, even though modern code tends to use classes for clarity.",
            "JavaScript inheritance is prototype-based: objects inherit from other objects via the prototype chain. Property lookup walks up this chain until found or reaching null. ES6 classes are syntactic sugar over prototypes. Understanding this is essential for debugging and performance. Modifying built-in prototypes is possible but dangerous - it can break libraries. Object.create() creates objects with specific prototypes. This knowledge matters when extending built-in types or understanding library internals."],
    },
    {
        text: "What is the module pattern and how do ES modules differ from CommonJS?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.modules],
        answers: ["The module pattern is a way to create private scope and encapsulation, historically using IIFEs to create closures. ES modules are the modern standard using import/export syntax, while CommonJS uses require() and module.exports, mainly in Node.js. The key differences are that ES modules are statically analyzed, meaning imports are resolved at compile time, which enables tree shaking and better tooling. They're also asynchronous by nature. CommonJS loads modules synchronously and dynamically at runtime, so you can conditionally require modules. ES modules have strict mode by default and have their own scope, while CommonJS modules are wrapped in a function. In modern development, I use ES modules everywhere, though you'll still see CommonJS in older Node.js projects or configuration files.",
            "ES modules use import/export with static analysis at compile time, enabling tree shaking. CommonJS uses require/module.exports with dynamic runtime loading, allowing conditional requires. ES modules are async and have strict mode by default; CommonJS is sync. The module pattern predates both, using IIFEs for encapsulation. Modern projects use ES modules. You'll encounter CommonJS in Node.js config files and older codebases. Node.js now supports ES modules natively, though some tooling still requires CommonJS."],
    },
    {
        text: "What is optional chaining and nullish coalescing?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript],
        answers: ["Optional chaining, using the ?. operator, lets you safely access nested properties without having to check each level for null or undefined. If any part of the chain is nullish, it short-circuits and returns undefined instead of throwing an error. Nullish coalescing, using the ?? operator, provides a default value only when the left side is null or undefined, unlike the OR operator which triggers on any falsy value. These are game-changers for cleaner code. Instead of writing 'user && user.address && user.address.street', you can write 'user?.address?.street'. And instead of 'value || defaultValue' which fails for 0 or empty string, you use 'value ?? defaultValue'. I use these constantly when working with API responses or optional props.",
            "Optional chaining (?.) safely accesses nested properties, returning undefined if any part is nullish instead of throwing. Nullish coalescing (??) provides defaults only for null/undefined, unlike || which triggers on any falsy value including 0 and empty string. These operators dramatically clean up defensive code. Combined: 'user?.settings?.theme ?? 'light'' gets nested theme or falls back safely. I use these constantly with API data where properties might be missing. They're essential modern JavaScript."],
    },
    {
        text: "What is debouncing vs throttling and when would you use each?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.javascript, ValidTag.enum.performance],
        answers: ["Debouncing delays executing a function until after a certain amount of time has passed since it was last called. It's perfect for search inputs - you wait until the user stops typing before making an API call. Throttling ensures a function is only called at most once in a specified time period, regardless of how many times it's triggered. This is great for scroll or resize events where you want regular updates but not on every single pixel change. Think of debouncing as 'wait until they're done' and throttling as 'do this at regular intervals'. I use debounce for autocomplete searches with maybe a 300ms delay, and throttle for infinite scroll detection or window resize handlers. Both are essential for performance optimization.",
            "Debounce waits until activity stops - great for search inputs where you want to wait for the user to finish typing. Throttle limits execution to at most once per interval - ideal for scroll or resize where you want regular updates without overwhelming frequency. Memory aid: debounce is 'wait for pause', throttle is 'limit rate'. Libraries like lodash provide both. I typically debounce search with 300ms delay and throttle scroll handlers to 100ms. Both prevent unnecessary work and API calls."],
    },

    // TypeScript
    {
        text: "What are generics in TypeScript and how do you use them?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.generics],
        answers: ["Generics allow you to write reusable code that works with multiple types while maintaining type safety. Instead of using 'any', you use a type parameter like <T> that acts as a placeholder for the actual type. A classic example is an array - Array<string> or Array<number>. When I write a function like 'function identity<T>(arg: T): T', TypeScript infers the type based on what you pass in. This is incredibly useful for utility functions, API response types, or component props. For instance, you might have a generic API fetch function that returns different data types, or a React component that accepts different item types. Generics let you be flexible without losing the benefits of type checking.",
            "Generics create reusable, type-safe code that works with multiple types. Instead of 'any', use type parameters like <T> that TypeScript infers from usage. Array<T>, Promise<T>, and Map<K, V> are built-in examples. I use generics for API wrappers returning different response types, utility functions like debounce that preserve input/output types, and component props accepting various item types. Constraints like <T extends BaseType> limit what types can be used while maintaining flexibility."],
    },
    {
        text: "What are union types and discriminated unions?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Union types let you specify that a value can be one of several types using the pipe operator, like 'string | number'. This is useful when a function can accept multiple types or when working with different possible states. Discriminated unions, also called tagged unions, are a pattern where you have a common property (usually called 'type' or 'kind') that distinguishes between the different union members. TypeScript can then narrow the type based on that discriminant property. For example, with a Result type that's either Success or Error, each with a 'type' property, TypeScript knows if you check 'result.type === success', then result must be the Success type. I use discriminated unions all the time for state machines, API responses, or Redux actions.",
            "Union types represent 'one of several types' using the pipe operator: string | number. Discriminated unions add a common discriminant property like 'type' that TypeScript uses to narrow which variant you have. Check 'if (result.type === \"success\")' and TypeScript knows result is the Success variant with its specific properties. This pattern is essential for modeling states: loading, success, or error with different payloads. Redux actions, API responses, and state machines all benefit from discriminated unions."],
    },
    {
        text: "What is type narrowing and how does TypeScript infer types?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["type-guards"], ValidTag.enum.types],
        answers: ["Type narrowing is when TypeScript refines a broader type to a more specific type based on your code's logic. This happens with control flow analysis - if you check 'typeof x === string', TypeScript knows x is a string in that block. It also works with truthiness checks, instanceof, and custom type guards. Type inference is TypeScript's ability to automatically determine types without explicit annotations. It looks at the value you assign, the return statements in functions, and how variables are used. For example, if you write 'const x = 5', TypeScript infers x is a number. The combination is powerful - you write minimal type annotations and TypeScript figures out the rest, while still catching type errors. I try to rely on inference when possible for cleaner code.",
            "Type narrowing refines broad types to specific ones through control flow: typeof checks, instanceof, truthiness, discriminated unions, and custom type guards. After 'if (typeof x === \"string\")', TypeScript knows x is string in that block. Inference automatically determines types from values and usage without explicit annotations. Together they minimize boilerplate while maintaining safety. I annotate function parameters and return types but let inference handle local variables. Custom type guards use 'is' syntax to teach TypeScript new narrowing checks."],
    },
    {
        text: "What are utility types and when would you use Partial, Pick, Omit, Record, Required, or Readonly?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum["utility-types"]],
        answers: ["Utility types are built-in TypeScript helpers that transform existing types. Partial makes all properties optional - great for update functions where you might only change some fields. Pick extracts specific properties from a type, useful when you only need a subset. Omit is the opposite, excluding certain properties. Record creates an object type with specific keys and value types - I use this for lookup maps. Required makes all properties required, opposite of Partial. Readonly makes properties immutable. In practice, I use Partial constantly for update operations, Pick when building form types from larger entities, and Omit to exclude things like passwords from user types. Record is perfect for creating typed dictionaries. These save you from manually redefining types and keep them in sync.",
            "Utility types transform existing types. Partial<User> makes all User properties optional - perfect for update functions. Pick<User, 'id' | 'name'> extracts specific properties. Omit<User, 'password'> excludes properties. Record<string, User> creates typed dictionaries. Required reverses Partial; Readonly prevents mutation. I use these constantly: Partial for partial updates, Pick for form subsets, Omit to hide sensitive fields in responses. They derive from source types, so changes propagate automatically."],
    },
    {
        text: "What is the never type and when is it used?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["The never type represents values that never occur. It's used for functions that never return, like ones that always throw errors or have infinite loops. It's also the type of variables in situations that should be impossible. A really useful pattern is exhaustive checking in switch statements - if you handle all cases of a union type, the default case will have type never. This way, if you add a new case to the union later and forget to handle it, TypeScript will error because you're trying to assign a real type to never. I use this pattern regularly with discriminated unions to make sure I've covered all possible states. It's a powerful tool for catching logic errors at compile time.",
            "Never represents values that can't exist: functions that always throw, infinite loops, or impossible branches. Its killer use is exhaustive checking in switches. After handling all union cases, the default has type never. If you add a new case and forget to handle it, assigning to never errors. This catches bugs at compile time when enums or unions expand. I use this pattern with discriminated unions to ensure all states are handled. It's TypeScript helping enforce complete logic coverage."],
    },
    {
        text: "What are declaration files and when do you need them?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript],
        answers: ["Declaration files with the .d.ts extension provide type information for JavaScript code that doesn't have built-in types. They describe the shape of existing JavaScript modules without containing implementation. You need them when using JavaScript libraries in TypeScript projects. Many popular libraries ship with their own declaration files or have them available through DefinitelyTyped (@types packages). If you're working with a JavaScript library that doesn't have types, you might write your own declaration file. You also generate them when publishing a TypeScript library so consumers get type information. I've had to write custom declarations for internal JavaScript utilities or when using libraries without type definitions. They're basically type contracts without the actual code.",
            "Declaration files (.d.ts) provide type information for JavaScript without implementation. Install @types/libraryname for community-maintained types from DefinitelyTyped. Many libraries now ship their own types. For untyped libraries, write custom declarations or use 'declare module' for quick fixes. When publishing TypeScript libraries, generate declarations so consumers get types. The 'declaration' compiler option auto-generates them. I've written custom declarations for internal JS utilities. They're the bridge letting TypeScript understand JavaScript."],
    },
    {
        text: "What are index signatures and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.typescript, ValidTag.enum.types],
        answers: ["Index signatures let you define types for objects where you don't know the property names ahead of time, but you know the shape of the values. The syntax looks like '[key: string]: number' which means any string key will have a number value. This is useful for dictionaries, maps, or dynamic data structures. For example, if you're building a lookup table where user IDs map to user objects, you'd use an index signature. However, they're less type-safe than known properties since any string key is valid. When possible, I prefer using Record utility type or Map, but index signatures are essential when working with truly dynamic data. Just be aware that once you add an index signature, TypeScript assumes any property access is valid.",
            "Index signatures type objects with dynamic keys: { [key: string]: User } means any string key maps to a User. Useful for dictionaries and lookup tables. The trade-off is less safety - TypeScript allows any string access. I prefer Record<string, User> for the same effect with cleaner syntax. For runtime key checking, Map is often better since it has explicit get/set. Use index signatures when object keys are truly dynamic, like user-provided data or API responses with variable structure."],
    },

    // CSS
    {
        text: "What is the critical rendering path and how does CSS block rendering?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.performance],
        answers: ["The critical rendering path is the sequence of steps browsers take to convert HTML, CSS, and JavaScript into pixels on screen. It involves building the DOM from HTML, the CSSOM from CSS, combining them into a render tree, calculating layout, and finally painting. CSS is render-blocking because the browser won't render the page until the CSSOM is complete - it needs to know all the styles before it can paint. This prevents the flash of unstyled content but can delay first paint. To optimize, you want to minimize CSS file size, inline critical CSS for above-the-fold content, and defer non-critical CSS. I usually ensure critical styles are loaded first and lazy load the rest, or use media queries to load print styles asynchronously.",
            "The critical rendering path transforms HTML, CSS, and JS into pixels: DOM construction, CSSOM construction, render tree, layout, paint, and composite. CSS blocks rendering because browsers need complete styles before painting. To optimize: inline critical above-the-fold CSS, defer non-critical CSS with media queries or JavaScript, minimize CSS size. JavaScript also blocks unless async or defer is used. Understanding this path explains why certain resources delay first paint and guides optimization strategies."],
    },
    {
        text: "What causes layout thrashing and how do you avoid it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum.performance],
        answers: ["Layout thrashing, or forced synchronous layout, happens when you repeatedly read layout properties and then write to the DOM in a loop. Each write invalidates the layout, and each read forces the browser to recalculate it immediately, creating a performance bottleneck. For example, reading offsetHeight and then changing styles in a loop causes this. To avoid it, you batch your reads and writes separately - read all your layout properties first, then make all your DOM changes. Libraries like FastDOM help manage this. Another approach is using requestAnimationFrame to schedule DOM updates. I also try to use CSS transforms instead of properties that trigger layout, and avoid layout properties in loops altogether when possible. It's one of those easy mistakes to make that can really hurt performance.",
            "Layout thrashing occurs when reading and writing layout properties alternately in loops. Each write invalidates layout, each read forces recalculation - doing this repeatedly kills performance. Example: reading offsetHeight then setting height in a loop. Fix by batching reads before writes. FastDOM library helps manage this. Also use requestAnimationFrame to batch updates. Prefer transforms over layout-triggering properties like width or top. The Performance panel in DevTools shows forced reflows - look for purple layout events."],
    },
    {
        text: "How do you compose styles in CSS Modules?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.css, ValidTag.enum["css-modules"]],
        answers: ["CSS Modules provide composition through the 'composes' keyword, which lets you inherit styles from other classes. You can compose from classes in the same file or import from other modules. For example, 'composes: button from ./shared.module.css' would pull in the button styles. This creates multiple class names on the element but keeps your styles modular and reusable. The benefit over @extend in Sass is that it's more explicit and doesn't combine selectors. In practice, I create base component styles and compose them for variants, like a base button style that gets composed into primary, secondary, danger buttons. It keeps the CSS DRY while maintaining the scoping benefits of CSS Modules. You can also combine it with traditional class application in your components.",
            "CSS Modules use the 'composes' keyword to inherit from other classes. Write '.primary { composes: button; }' to include button styles. You can compose from external files too: 'composes: base from ./shared.module.css'. This generates multiple class names in the output, maintaining modularity. I use it for variants - a base button composed into primary, secondary, danger variations. Unlike Sass @extend, composition doesn't merge selectors, keeping output predictable. Combined with scoped class names, it's a clean pattern for reusable component styles."],
    },
    {
        text: "How do you create custom utilities and components in Tailwind?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind],
        answers: ["In Tailwind, you extend the theme in tailwind.config.js to add custom values for existing utilities, like new colors or spacing. For completely new utilities, you use the addUtilities function in a plugin. For components - repeated patterns of utilities - you can use the addComponents function, though Tailwind generally discourages this in favor of composition. The preferred approach is actually creating React components or template partials that encapsulate the utility classes. For example, instead of a Button component in CSS, you'd make a Button React component with the Tailwind classes. When I do need custom utilities, like a specific text shadow pattern, I add them through plugins. The key is extending the existing system rather than fighting against it.",
            "Custom Tailwind work happens in tailwind.config.js. Extend the theme for new values in existing utilities - custom colors, spacing, fonts. For new utility classes, use addUtilities in a plugin. Tailwind prefers component abstraction at the template level - a React Button component with Tailwind classes, not CSS. Use addComponents sparingly for truly reusable CSS patterns. Plugins can add variants, utilities, or base styles. The philosophy is extending the design system rather than escaping it."],
    },
    {
        text: "What is the @apply directive and when should you avoid it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.tailwind],
        answers: ["The @apply directive lets you extract Tailwind utility classes into custom CSS classes. You write something like '.btn { @apply px-4 py-2 bg-blue-500; }' to create reusable class names. While this seems convenient, Tailwind's docs actually recommend avoiding it in most cases. The problem is you lose the benefits of utility-first CSS - you're back to naming things and maintaining CSS files. It also increases bundle size since you're duplicating utilities. The better approach is component extraction at the template level with React components or partials. I only use @apply for very specific cases, like third-party component styling where I can't modify the markup, or when I absolutely need a semantic class name for a design system. For everything else, composition with components is cleaner.",
            "@apply extracts utilities into custom classes: '.btn { @apply px-4 py-2; }'. Tailwind discourages overuse because it negates utility-first benefits - you're back to naming classes and maintaining CSS. It also duplicates utilities, increasing bundle size. Prefer React components encapsulating Tailwind classes. Use @apply only when you can't control markup, like styling third-party components, or for design system token classes. Component-level abstraction is the intended pattern."],
    },
    // React Core
    {
        text: "What is the React Virtual DOM and why is it important?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["virtual-dom"]],
        answers: ["The Virtual DOM is React's lightweight JavaScript representation of the actual DOM. When state changes, React creates a new Virtual DOM tree, compares it with the previous one through a process called reconciliation, calculates the minimal set of changes needed, and then updates only those parts of the real DOM. This is important because direct DOM manipulation is expensive - reading and writing to it triggers reflows and repaints. By batching changes and minimizing DOM updates, React makes UI updates much more efficient. The diffing algorithm is smart enough to identify what changed and update just those nodes. While the Virtual DOM isn't always faster than direct DOM manipulation for simple cases, it provides a programming model where you can write code as if you're re-rendering everything, while React optimizes the actual updates.",
            "The Virtual DOM is an in-memory JavaScript representation of the real DOM. React diffs new and previous Virtual DOM trees to calculate minimal changes, then patches the real DOM efficiently. This lets you write declarative code as if re-rendering everything while React optimizes actual updates. The reconciliation algorithm uses heuristics like element type comparison and keys for list efficiency. It's not always faster than direct DOM for simple cases, but it provides a maintainable programming model that scales well."],
    },
    {
        text: "What are synthetic events in React?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["synthetic-events"]],
        answers: ["Synthetic events are React's cross-browser wrapper around native browser events. They provide the same interface as native events but work consistently across all browsers, smoothing out inconsistencies. React implements its own event system for performance - it uses event delegation at the root level rather than attaching listeners to each element. One important thing to know is that synthetic events are pooled and reused for performance, so they're nullified after the event handler runs. If you need to access event properties asynchronously, you need to call event.persist() or store the values you need in variables. In practice, this mostly just works transparently, but it's important to understand when debugging event-related issues or doing something async with event data.",
            "Synthetic events wrap native browser events for cross-browser consistency. React uses event delegation at the root instead of individual listeners per element. Historically, events were pooled and reused, requiring event.persist() for async access - though React 17+ removed pooling. Access event.nativeEvent for the underlying browser event when needed. The synthetic system normalizes differences between browsers. Understanding this helps when debugging event issues or integrating with non-React code that expects native events."],
    },
    {
        text: "How do you handle forms in React and what are the tradeoffs of different approaches?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.forms],
        answers: ["There are three main approaches. Controlled components store form data in React state, with value and onChange props, giving you full control but requiring more boilerplate. Uncontrolled components use refs to access DOM values directly, which is simpler but less React-idiomatic. The third approach is using form libraries like React Hook Form or Formik. React Hook Form uses uncontrolled components under the hood with refs, minimizing re-renders and boosting performance, while Formik uses controlled components. For simple forms, controlled components are fine. For complex forms with validation, I reach for React Hook Form - it handles validation, error states, and submission with minimal re-renders. The tradeoff is learning another API, but it's worth it for forms with more than a few fields.",
            "Form approaches: controlled uses React state for values with onChange handlers - full control but more re-renders. Uncontrolled uses refs to DOM elements - simpler but less integrated. Libraries like React Hook Form provide the best of both: uncontrolled performance with controlled-like DX, plus built-in validation. Formik uses controlled components with more abstraction. I choose React Hook Form for complex forms - it handles validation, errors, and submission efficiently. Simple forms work fine with basic controlled inputs."],
    },

    // React Hooks
    {
        text: "What is useState and how does state batching work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useState],
        answers: ["useState is React's hook for adding state to functional components. You call it with an initial value and it returns the current state and a setter function. State batching is React's optimization where multiple setState calls in the same event handler are batched together into a single re-render. This used to only work in React event handlers, but with React 18's automatic batching, it works everywhere - timeouts, promises, and native event handlers. If you need the latest state for the next update, you use the functional form of setState that receives the previous state. It's important to understand that setState is asynchronous and batched, so you can't read the new value immediately after setting it. This batching is crucial for performance since re-rendering is expensive.",
            "useState returns state and a setter. State updates are async and batched - multiple setStates in one handler trigger only one re-render. React 18 extended batching to promises, timeouts, and native events. Use functional updates 'setState(prev => prev + 1)' when the new value depends on previous state. The initial value can be a function for expensive computations run only on first render. Understanding batching prevents bugs where you expect immediate state reads after setting."],
    },
    {
        text: "What is useEffect and what are its cleanup patterns?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useEffect],
        answers: ["useEffect lets you perform side effects in functional components - data fetching, subscriptions, manual DOM manipulation. It runs after render by default. The cleanup pattern is returning a function from useEffect that React calls before the next effect runs and when the component unmounts. This is essential for preventing memory leaks. Common cleanup scenarios include canceling API requests, clearing timers, unsubscribing from events, or closing WebSocket connections. For example, if you add an event listener, you return a function that removes it. The dependency array controls when effects run - empty array means only on mount, no array means every render, and with dependencies means when those change. The cleanup is your way of undoing whatever the effect did.",
            "useEffect handles side effects after render. Return a cleanup function to prevent leaks - clear timers, remove listeners, abort requests. Dependencies control execution: empty array for mount-only, dependencies for reactive execution. Common mistake: missing dependencies cause stale closures. Include all values the effect uses or you'll get bugs. For data fetching, consider React Query instead since it handles cleanup, caching, and race conditions that raw useEffect makes tricky."],
    },
    {
        text: "What is the difference between useEffect and useLayoutEffect?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useEffect],
        answers: ["The key difference is timing. useEffect runs asynchronously after the browser has painted, so it doesn't block visual updates. useLayoutEffect runs synchronously after DOM mutations but before the browser paints, making it blocking. Use useLayoutEffect when you need to read layout from the DOM and synchronously re-render, or when you need to make DOM mutations that the user shouldn't see. A classic case is measuring an element's size and positioning another element based on it - with useEffect you might see a flicker, but useLayoutEffect prevents that. However, because it's synchronous, it can hurt performance if you do heavy computation. In 99% of cases, useEffect is the right choice. I only reach for useLayoutEffect when I have visual bugs from async timing.",
            "useEffect runs after paint - non-blocking. useLayoutEffect runs synchronously before paint - blocking. Use useLayoutEffect to prevent visual flicker when measuring DOM and immediately applying styles. Classic example: measuring element dimensions for positioning. useEffect would show then shift; useLayoutEffect measures and positions before any paint. Since it blocks, keep work minimal. Default to useEffect; only reach for useLayoutEffect when you see flicker bugs that async timing causes."],
    },
    {
        text: "What is useRef and what are its use cases beyond DOM references?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useRef],
        answers: ["useRef creates a mutable object that persists for the component's lifetime and doesn't trigger re-renders when changed. The most common use is accessing DOM elements directly, but it's also incredibly useful for storing any mutable value that you don't want to trigger re-renders - like timer IDs, previous values, or instance variables. I use it to store interval or timeout IDs so I can clear them later, to keep track of whether a component is mounted to avoid state updates after unmount, or to store the previous value of a prop or state for comparison. Unlike state, updating ref.current doesn't cause a re-render, which is perfect for values you need to track but don't need to display. It's like having instance variables in a functional component.",
            "useRef persists a mutable value across renders without causing re-renders when changed. Beyond DOM access, use it for timer IDs, previous values, mounted state tracking, or any instance variable. Unlike state, updating ref.current is immediate and doesn't trigger renders. Pattern for previous value: useEffect to update ref after render. Pattern for mounted check: set ref true on mount, false on cleanup. Think of refs as escape hatches from React's declarative model when you need imperative control."],
    },
    {
        text: "What is useCallback and when should you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useCallback, ValidTag.enum.performance],
        answers: ["useCallback memoizes a function so it maintains the same reference between renders unless its dependencies change. This is primarily useful when passing callbacks to optimized child components that rely on reference equality to prevent re-renders, like components wrapped in React.memo. Without useCallback, you'd create a new function on every render, causing memoized children to re-render unnecessarily. It's also useful when the function is a dependency of useEffect or other hooks, to prevent infinite loops. However, I don't use it everywhere - there's overhead to memoization itself. I only use it when I'm actually seeing performance issues or when I know I'm passing the function to a memoized component. Premature optimization with useCallback can make code harder to read without real benefit.",
            "useCallback returns a memoized function with stable reference unless dependencies change. Main uses: passing callbacks to React.memo children and stabilizing effect dependencies. Without it, inline functions are new each render, breaking memoization. But don't use everywhere - memoization has overhead. Profile first. I use it when passing to memoized components or when a function is in a dependency array causing issues. Premature useCallback adds complexity without benefit."],
    },
    {
        text: "What is useMemo and when does it actually help performance?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useMemo, ValidTag.enum.performance],
        answers: ["useMemo memoizes a computed value, recalculating it only when its dependencies change. It's useful for expensive calculations that don't need to run on every render. However, it only helps when the computation is actually expensive - like filtering or sorting large arrays, complex mathematical operations, or deep object transformations. For simple operations, the memoization overhead might cost more than just recalculating. I also use it to maintain referential equality for objects or arrays passed to child components, similar to useCallback for functions. A good rule of thumb is to measure first - use React DevTools Profiler to identify actual performance issues before memoizing. Memoization isn't free and makes code more complex, so I only use it when I can measure the benefit.",
            "useMemo caches a computed value until dependencies change. Use for expensive calculations - large array sorts, complex transformations. Also maintains referential equality for objects/arrays passed to memoized children. Don't overuse - simple computations don't need it; memoization has overhead. Profile with React DevTools before adding. I use useMemo when I see actual performance issues or when passing computed objects to memo'd components. The goal is fewer re-renders and faster renders, measure to confirm."],
    },
    {
        text: "What is the difference between useCallback and useMemo?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useCallback, ValidTag.enum.useMemo],
        answers: ["The core difference is what they memoize. useCallback memoizes a function itself, returning the same function reference between renders. useMemo memoizes the result of a function call, returning the same computed value. Technically, useCallback is just syntactic sugar - 'useCallback(fn, deps)' is equivalent to 'useMemo(() => fn, deps)'. Use useCallback when you need to pass a stable function reference to child components or dependency arrays. Use useMemo when you have an expensive computation and want to cache its result. In practice, useCallback is for preventing function identity changes, while useMemo is for avoiding expensive recalculations. Both serve performance optimization but in different ways - one caches the function, the other caches the value.",
            "useCallback caches a function; useMemo caches a computed value. Technically, useCallback(fn, deps) equals useMemo(() => fn, deps). Use useCallback for stable function references passed to children or in dependency arrays. Use useMemo for expensive computations you want to cache. Both prevent unnecessary work, but target different things: function identity vs computation result. Choose based on whether you're passing a function or a value."],
    },
    {
        text: "What is useContext and what are its performance implications?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useContext, ValidTag.enum.performance],
        answers: ["useContext lets you consume values from React Context without prop drilling. You pass it a context object created with createContext, and it returns the current context value from the nearest Provider above it in the tree. The main performance implication is that any component using useContext will re-render whenever the context value changes, regardless of which part of the value changed. You can't selectively subscribe to parts of the context. This can cause unnecessary re-renders in large apps. Common solutions include splitting contexts by concern, using useMemo to stabilize context values, or using state management libraries designed for performance like Zustand. Context is great for truly global or semi-global data like themes or auth, but I'm careful about putting frequently-changing data in context.",
            "useContext consumes context values without prop drilling. The performance catch: all consumers re-render when context changes, with no partial subscription. Split contexts by concern to minimize blast radius. Memoize context values to prevent object identity changes. For frequently-changing data, consider Zustand or Jotai instead. Context works great for static or rarely-changing data like themes, auth, or feature flags. Combine with memoization strategies for better performance."],
    },
    {
        text: "What is useReducer and when would you use it over useState?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks, ValidTag.enum.useReducer, ValidTag.enum.useState],
        answers: ["useReducer is an alternative to useState for managing complex state. You provide a reducer function and initial state, and it returns the current state and a dispatch function. It's better than useState when you have complex state logic involving multiple sub-values, when the next state depends on the previous one, or when you want to optimize performance by passing dispatch down instead of callbacks. It's also more testable since the reducer is a pure function. I reach for useReducer for form state with multiple fields and validation, complex UI state machines, or when state updates depend on multiple actions. For simple boolean toggles or single values, useState is cleaner. It's similar to Redux but at the component level.",
            "useReducer manages state through actions and a reducer function. Benefits: testable pure reducers, complex state transitions, stable dispatch identity. Use over useState for multi-field forms, state machines, or when updates depend on current state in complex ways. Dispatch can be passed to children without memoization worries. It's component-level Redux. For simple state, useState is cleaner. The pattern works well for anything with distinct actions like forms or multi-step flows."],
    },
    {
        text: "What is useId and why was it introduced?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["useId generates unique IDs that are stable across server and client renders, solving a major issue with server-side rendering. Before useId, generating unique IDs was tricky because IDs generated on the server needed to match those on the client to avoid hydration mismatches. It's primarily used for accessibility attributes that require unique IDs, like linking labels to inputs with htmlFor and id, or aria attributes. You call it with no arguments and it returns a unique string. It's important to note that it's not for generating keys in lists - those should be based on your data. The IDs it generates aren't sequential or pretty, but they're guaranteed to be unique and consistent across renders. I use it whenever I need to connect related elements for accessibility.",
            "useId generates unique IDs stable across server and client, preventing hydration mismatches. Pre-useId, random IDs differed between server and client renders. Use for accessibility: connecting labels to inputs, aria attributes. Not for list keys - those should come from data. Call without arguments for a unique string. The IDs look random but are deterministic. Use multiple times in a component for multiple IDs. Essential for accessible form components in SSR applications."],
    },

    // State Management
    {
        text: "What is Redux and how does the data flow work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.redux, ValidTag.enum.react],
        answers: ["Redux is a predictable state management library based on a single source of truth - the store. The data flow is unidirectional: components dispatch actions, which are plain objects describing what happened. Reducers, which are pure functions, take the current state and an action, and return a new state. The store updates with this new state and notifies subscribed components to re-render. This pattern makes state changes predictable and debuggable. You can see exactly what changed, when, and why. Redux DevTools let you time-travel through state changes. The downside is boilerplate - you need action creators, reducers, and often middleware for async operations. Redux Toolkit significantly reduces this. I use Redux for complex applications with lots of shared state, but for simpler apps, Context or Zustand might be sufficient.",
            "Redux provides predictable state with unidirectional flow: dispatch action, reducer returns new state, store notifies subscribers. Benefits: single source of truth, time-travel debugging, predictable changes. Redux Toolkit reduces traditional boilerplate significantly with createSlice. Use for complex apps with significant shared state. For simpler needs, Zustand or Context suffice. The pattern shines in large teams where predictability and debugging matter. Middleware handles async logic like API calls."],
    },
    {
        text: "What is Zustand and why would you choose it over Redux?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.zustand, ValidTag.enum.react],
        answers: ["Zustand is a small, fast state management library with a much simpler API than Redux. You create a store with a single function call, and state updates are straightforward - you just mutate a draft using Immer under the hood or return new state. Components can subscribe to specific slices of state, preventing unnecessary re-renders. Unlike Context, it doesn't require providers wrapping your app. The main advantages over Redux are less boilerplate, smaller bundle size, better performance with selective subscriptions, and no need for actions or reducers. The tradeoff is less structure and fewer guardrails. I prefer Zustand for most projects now because it's so much simpler while still providing good developer experience. Redux is still valuable for very large apps where you want the strict patterns and extensive ecosystem.",
            "Zustand offers minimal API state management: create store in one call, no providers needed. Components subscribe to specific slices, preventing unnecessary re-renders. Much less boilerplate than Redux, smaller bundle, great TypeScript support. Use selectors for performance. Immer integration allows mutable-style updates that produce immutable state. I reach for Zustand by default now - it's simple yet capable. Redux still has value for very large apps needing strict patterns, but Zustand covers most needs."],
    },
    {
        text: "What are the performance implications of React Context?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum["context-api"], ValidTag.enum.performance],
        answers: ["The main performance issue with Context is that every component consuming the context re-renders whenever the context value changes, even if it only uses a small part of that value. There's no way to selectively subscribe to portions of the context. This can cause cascading re-renders in large component trees. To mitigate this, you can split contexts by concern so changes to one don't affect others, use useMemo to prevent creating new context values on every render, or leverage composition to limit which components actually consume the context. For frequently changing data, Context might not be the best choice - libraries like Zustand or Jotai provide more granular subscriptions. Context is great for relatively static data like themes, auth state, or configuration, but be cautious with rapidly changing application state.",
            "Context causes all consumers to re-render on any value change - no selective subscription. This cascades through component trees. Mitigate by splitting contexts by domain, memoizing provider values, and using composition to minimize consumers. For frequent updates, use Zustand or Jotai with granular subscriptions. Context fits static or slow-changing data: themes, auth, locale. Don't put rapidly changing state like form values in context. Profile re-renders to identify context performance issues."],
    },
    {
        text: "How do you structure global state vs server state vs local state?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react],
        answers: ["I think about these as distinct categories that need different solutions. Local state is component-specific data like form inputs or UI toggles - useState or useReducer handles this perfectly. Global client state is application-level data that's not from the server, like UI preferences, modal state, or selected themes - I use Context, Zustand, or Redux depending on complexity. Server state is data from your backend, which is fundamentally different because it's cached, can be stale, and needs refetching - TanStack Query or SWR are purpose-built for this. The key insight is that server state shouldn't go into Redux or Context; it needs different handling for caching, revalidation, and synchronization. I've seen many projects mix these concerns and end up with complicated state management when they just needed to separate server state properly.",
            "Categorize state by origin. Local: component-specific UI state, use useState/useReducer. Client global: app-wide UI state like modals or preferences, use Context/Zustand. Server: remote data requiring caching and synchronization, use React Query/SWR. Mixing server state into Redux complicates things - server state has unique needs like staleness, refetching, and cache invalidation. Proper categorization simplifies architecture significantly. Most 'complex state' problems are actually just server state mixed into client stores."],
    },
    {
        text: "How do you handle derived state?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.react, ValidTag.enum.state],
        answers: ["Derived state is data you can compute from existing state, and the key principle is to avoid storing it as separate state. Instead, calculate it during render or use useMemo if the calculation is expensive. For example, if you have an array of items and need the filtered count, don't store both the array and the count - just calculate filteredItems.length. Storing derived state separately leads to synchronization bugs where the states get out of sync. If you're tempted to use useEffect to keep derived state in sync, that's a code smell - you should usually just compute it directly. The exception is when the computation is truly expensive and you're seeing performance issues, then useMemo is appropriate. This keeps your state minimal and your components easier to reason about.",
            "Derived state should be computed, not stored. If you can calculate a value from existing state, do it during render. Don't store both items array and itemCount - compute items.length. Storing derived state creates sync bugs. useEffect to sync derived state is a code smell - compute directly instead. useMemo handles expensive computations. This keeps state minimal and prevents the entire category of 'states got out of sync' bugs. Store source of truth only."],
    },

    // TanStack Query
    {
        text: "What is TanStack Query and what problems does it solve?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.react],
        answers: ["TanStack Query, formerly React Query, is a data-fetching and caching library that handles server state. It solves the problems of fetching, caching, synchronizing, and updating server data. Without it, you'd manually handle loading states, error states, caching, deduplication, background refetching, and stale data. TanStack Query does all this automatically. It caches query results, deduplicates identical requests, automatically refetches in the background to keep data fresh, handles pagination and infinite scrolling, and provides a great DevTools experience. It shifts your mental model from thinking about when to fetch to describing what data you need. The really powerful part is how it handles cache invalidation and refetching. I use it on every project now - it dramatically reduces boilerplate and makes data fetching predictable.",
            "TanStack Query manages server state: caching, synchronization, background refetching, and deduplication. Describe what data you need and it handles when and how. Features: automatic caching, request deduplication, stale-while-revalidate, optimistic updates, infinite queries, and great DevTools. It replaces manual loading/error state management with a declarative approach. I consider it essential for any data-heavy React app. The mental shift from imperative fetching to declarative data requirements is transformative."],
    },
    {
        text: "What is the difference between useQuery and useMutation?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.react],
        answers: ["useQuery is for fetching data - it runs automatically, returns cached data, and manages refetching. You use it for GET requests that read data. It handles loading, error, and success states, and the data is cached by the query key. useMutation is for modifying data - POST, PUT, DELETE requests. It doesn't run automatically; you call the mutate function when you want to perform the operation. Mutations don't cache results but can invalidate or update query caches after success. A typical pattern is using useQuery to fetch a list, useMutation to add an item, and then invalidating the list query in the mutation's onSuccess callback so the list refetches. I use queries for all my data reads and mutations for writes, and they work together to keep the UI in sync with the server.",
            "useQuery reads data automatically with caching and refetching. useMutation writes data on demand without caching. Query for GET requests; mutation for POST/PUT/DELETE. Mutations trigger via mutate() function and commonly invalidate queries onSuccess to refetch updated data. Pattern: query fetches list, mutation adds item, onSuccess invalidates list query. They complement each other: queries declare what data you need, mutations handle changes and trigger appropriate refetches."],
    },
    {
        text: "What is stale-while-revalidate and how does TanStack Query implement it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.caching],
        answers: ["Stale-while-revalidate is a caching strategy where you immediately return cached data even if it's stale, then refetch in the background and update when fresh data arrives. This provides instant UI updates while ensuring data freshness. TanStack Query implements this through staleTime and cacheTime configs. StaleTime determines how long data is considered fresh - during this period, no refetch happens. After it becomes stale, the cached data is still returned, but a background refetch is triggered. CacheTime determines how long unused data stays in cache. This means users see instant responses from cache while getting fresh data shortly after. It's a great UX pattern - no loading spinners for cached data, but data stays up to date. I typically set different stale times based on how frequently data changes.",
            "Stale-while-revalidate shows cached data instantly, refetches in background, then updates UI when fresh data arrives. Best of both: instant response plus eventual freshness. staleTime controls freshness duration - fresh data won't refetch. After staleTime, cached data shows while background refetch runs. gcTime (formerly cacheTime) controls how long inactive queries persist. Configure based on data volatility: longer staleTime for stable data, shorter for frequently changing data."],
    },
    {
        text: "How does query caching work in TanStack Query?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum["query-cache"], ValidTag.enum.caching],
        answers: ["TanStack Query caches query results in memory, keyed by the query key. When you call useQuery with a key that's already cached, it returns the cached data immediately while potentially refetching in the background based on your staleness config. Each query has status tracking - fresh, stale, or fetching. Fresh queries won't refetch, stale queries will refetch in certain situations like window focus or mount. The cache persists as long as there are active subscribers or until cacheTime expires for inactive queries. You can manually manipulate the cache with invalidation to mark data as stale, or direct updates to modify cached data without refetching. This intelligent caching means the same data across your app shares a single cache entry, preventing duplicate requests and keeping everything in sync.",
            "Query keys uniquely identify cached entries. Same key returns cached data instantly, with background refetch based on staleness. Cache survives while queries are subscribed or until gcTime expires. Invalidation marks entries stale for refetch. setQueryData directly updates cache for optimistic updates. All components using the same key share one cache entry - no duplicate requests. The DevTools visualize cache state. This automatic deduplication and sharing is a huge benefit over manual fetch management."],
    },
    {
        text: "What are query keys and how should you structure them?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum["query-keys"]],
        answers: ["Query keys uniquely identify queries for caching purposes. They can be strings or arrays, and TanStack Query uses them to determine if cached data exists and to match queries for invalidation. Best practice is to structure them as arrays hierarchically, from least to most specific. For example, ['todos'] for all todos, ['todos', 'list', { filter: 'completed' }] for filtered todos, and ['todos', 'detail', id] for a specific todo. This structure lets you invalidate at any level - invalidating ['todos'] refetches all todo-related queries, while ['todos', 'detail', 5] only refetches that specific todo. Include all variables that affect the data in the key. I treat query keys as dependencies - if any parameter changes, it should be in the key to trigger a new fetch.",
            "Query keys identify and organize cached queries. Structure hierarchically: ['entity', 'scope', variables]. Example: ['users', 'list'], ['users', 'detail', userId]. Hierarchical keys enable targeted invalidation - invalidating ['users'] hits all user queries; ['users', 'detail', 5] hits only that specific user. Include all variables affecting data in the key. Create query key factories for consistency: queryKeys.users.detail(id). Treat keys like effect dependencies - anything that changes the query result belongs in the key."],
    },
    {
        text: "What is query invalidation and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Query invalidation marks cached data as stale and triggers a refetch for active queries. You use queryClient.invalidateQueries() with a query key or pattern. The most common use case is after mutations - when you create, update, or delete data, you invalidate related queries so they refetch and show the updated data. For example, after adding a todo, you'd invalidate the todos list query. You can invalidate exact matches or use partial keys to invalidate multiple related queries. It's more efficient than manually updating the cache for complex data relationships. Sometimes I combine approaches - optimistically update the cache for instant UI feedback, then invalidate to refetch and ensure consistency. Invalidation is your primary tool for keeping the UI in sync with the server after changes.",
            "Invalidation marks queries stale and triggers refetch for active ones. Call queryClient.invalidateQueries({ queryKey: ['todos'] }) after mutations to sync UI with server. Partial keys invalidate matching queries - ['todos'] invalidates all todo queries. Combine with optimistic updates: update cache immediately for UX, then invalidate to confirm with server. For complex entities, invalidation is simpler than manually updating all affected cache entries. It's the primary mechanism for post-mutation data synchronization."],
    },
    {
        text: "How do you handle dependent queries?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"]],
        answers: ["Dependent queries are when one query needs data from another query to run. TanStack Query handles this with the enabled option. You set enabled to false until you have the required data. For example, if you need a user ID from a user query before fetching that user's projects, you'd do 'enabled: !!userId'. The query won't execute until enabled becomes true. This prevents unnecessary requests and errors from missing parameters. You can also chain data - the second query can use data from the first query's result. Another pattern is using query keys with the dependent data, so when that data changes, the dependent query automatically refetches. This keeps everything reactive and in sync. I use dependent queries frequently when building features that require sequential data fetching.",
            "Use the enabled option for dependent queries: 'enabled: !!userId' prevents execution until userId exists. The query stays in loading state until enabled becomes true. Include dependent values in query keys so changes trigger refetch. Pattern: first query provides userId, second query enabled on !!userId and includes userId in its key. This chains queries safely without null errors. The dependency pattern is common for user-then-user-data or category-then-products fetching sequences."],
    },
    {
        text: "How do you handle error and loading states?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["tanstack-query"], ValidTag.enum.react],
        answers: ["TanStack Query provides comprehensive state through the query result object. You get isLoading for initial load, isFetching for any fetch including background refetches, isError and error for error states, and isSuccess with data for successful states. For most UIs, I check isLoading to show a skeleton, isError to show an error message, and then render data. For more sophisticated UIs, I might show background refetch indicators using isFetching. You can also use status which is 'loading', 'error', or 'success' as a string. Error handling can be global through QueryClient defaults or per-query with onError callbacks. I typically combine local error displays with global error boundaries. The status booleans are mutually exclusive which makes conditional rendering straightforward.",
            "Query results include isLoading (initial fetch), isFetching (any fetch), isError/error, isSuccess/data. Pattern: isLoading shows skeleton, isError shows error message, otherwise render data. isFetching indicates background refresh for subtle loading indicators. Mutually exclusive booleans simplify conditionals. Handle errors globally via QueryClient or per-query with callbacks. Error boundaries can wrap query-using components. isPending in v5 replaces isLoading for initial state without data."],
    },

    // Next.js
    {
        text: "What is Next.js and what problems does it solve over plain React?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.react],
        answers: ["Next.js is a React framework that adds server-side rendering, static site generation, routing, and other production features on top of React. Plain React is client-side only - the browser downloads JavaScript, executes it, and builds the page, which hurts SEO and initial load time. Next.js solves this by rendering pages on the server, sending fully formed HTML to the browser, then hydrating it with React. This improves performance and SEO dramatically. It also provides file-based routing, API routes, automatic code splitting, image optimization, and built-in CSS support. You get a lot of configuration and tooling out of the box that you'd otherwise need to set up yourself with webpack, Babel, routing libraries, etc. For any production React app, especially content sites, Next.js saves tons of setup time.",
            "Next.js adds SSR, SSG, routing, and optimizations to React. Plain React is client-only, hurting SEO and initial load. Next.js pre-renders HTML on server, improving SEO and performance. Features include file-based routing, API routes, image optimization, and automatic code splitting. It handles production concerns like bundling and caching out of the box. I reach for Next.js on every new React project - it's the standard for production React apps."],
    },
    {
        text: "What is the difference between SSR, SSG, and ISR?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.ssr, ValidTag.enum.ssg, ValidTag.enum.isr],
        answers: ["SSR (Server-Side Rendering) generates HTML on every request. It's always fresh but slower since the server does work on each request. Use it for personalized or frequently changing data. SSG (Static Site Generation) generates HTML at build time. It's incredibly fast since it's just serving static files, but data can be stale. Perfect for blogs, docs, or marketing pages. ISR (Incremental Static Regeneration) is the sweet spot - it serves static pages like SSG but regenerates them in the background after a specified interval. You get the speed of static sites with fresher data. After the revalidation period, the next visitor triggers a rebuild in the background while still seeing the stale page. I use SSG for mostly static content, ISR for content that changes occasionally, and SSR only when data must be fresh on every request.",
            "SSG: HTML at build time - fastest but stale. SSR: HTML per request - fresh but slower. ISR: static pages that regenerate in background on schedule. Choose based on data freshness needs. SSG for blogs, docs. ISR for product pages with hourly price updates. SSR for personalized dashboards. ISR is often the sweet spot, combining static speed with reasonable freshness. The App Router adds more granular revalidation controls."],
    },
    {
        text: "When would you use server-side rendering vs client-side rendering?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.ssr, ValidTag.enum.csr],
        answers: ["Server-side rendering is better for SEO-critical content, when you need fast initial page loads, or when serving users with slower devices or connections. The initial HTML is ready immediately. Client-side rendering is fine for authenticated dashboards, internal tools, or highly interactive apps where SEO doesn't matter. It's simpler and reduces server load. The tradeoff is SSR adds server complexity and costs, while CSR means slower initial loads and poor SEO. In practice, I often use a hybrid approach - SSR for public pages that need SEO like landing pages and blogs, and CSR for the authenticated portions of the app. Next.js makes this easy since you can mix rendering strategies per page. The right choice depends on your audience and whether search engines need to see your content.",
            "SSR for SEO-critical public pages and fast initial loads. CSR for authenticated apps, dashboards, and tools where SEO doesn't matter. Hybrid approach is common: SSR for marketing pages, CSR for the app behind login. Consider: does Google need to crawl it? Is initial load critical? SSR adds server costs but improves SEO and perceived performance. CSR is simpler but requires JavaScript to render anything."],
    },
    {
        text: "How does Next.js handle routing?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.routing],
        answers: ["Next.js uses file-based routing where the file structure in the pages directory maps to routes. A file at pages/about.js becomes /about, pages/blog/index.js becomes /blog, and so on. Dynamic routes use square brackets like [id].js which matches any value and provides it as a query parameter. You can have catch-all routes with [...slug].js. The router is available through next/router with push, replace, and query. The Link component handles client-side navigation with prefetching. This file-based approach eliminates the need for route configuration files and makes the structure intuitive. With the new App Router, it's now folder-based with more features, but the Pages Router is still widely used. I find file-based routing much simpler than manually configuring React Router.",
            "File structure equals routes. pages/about.js becomes /about. Dynamic routes use brackets: [id].js. Catch-all routes: [...slug].js. Link component for navigation with automatic prefetching. useRouter hook provides programmatic navigation. No route config files needed - intuitive and maintainable. The App Router uses folders with page.js files, adding layouts and server components. File-based routing is cleaner than manual route configuration."],
    },
    {
        text: "What is the _app.js file and what is it used for?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"]],
        answers: ["The _app.js file is the root component that wraps all pages in a Next.js application. It's where you put anything that should persist across page changes - layout components, global styles, theme providers, authentication wrappers, state management providers, or analytics initialization. Every page is passed as the Component prop along with pageProps. You can override _app to customize the initialization process, add global error boundaries, or track page views. Common use cases include wrapping the app in a Redux Provider, adding a persistent navigation bar, or including global CSS imports. Anything you put in _app wraps every page, so be mindful of performance. I typically keep _app minimal and compose layout components at the page level when possible.",
            "_app.js wraps all pages - the root component. Add providers, global styles, persistent layouts, and analytics here. Receives Component and pageProps props. Persists between page navigations, unlike regular page components that remount. Keep it lean since it affects every page. Common additions: Redux/Query providers, auth wrappers, global CSS. In App Router, this role splits between layout.js for UI and providers.js for context."],
    },
    {
        text: "How does Next.js handle image optimization?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.performance],
        answers: ["The Next.js Image component automatically optimizes images by serving them in modern formats like WebP when supported, lazy loading by default, preventing layout shift with automatic sizing, and resizing images based on the device. It serves responsive images with the right size for each viewport. Images are optimized on-demand when requested, not at build time, so it works for any number of images. The component requires width and height props to prevent layout shift, or you can use fill for responsive sizing. You can configure custom image loaders for CDNs. This automatic optimization is huge for performance - images are often the largest assets on web pages. I use next/image for every image now because it handles all the optimization best practices automatically.",
            "next/image optimizes images automatically: modern formats like WebP, responsive sizes, lazy loading, blur placeholders. Requires width/height to prevent layout shift, or use fill prop. On-demand optimization at request time. Huge performance win since images are often the heaviest assets. I use it for all images. Configure remote domains in next.config.js. Priority prop loads above-fold images eagerly. The automatic optimization handles what would otherwise require manual tooling."],
    },
    {
        text: "What is next/head and how do you manage metadata?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.seo],
        answers: ["next/head is a component that lets you modify the document head for each page, adding title tags, meta descriptions, Open Graph tags, or any other head elements. You import Head from next/head and use it anywhere in your component. This is crucial for SEO since each page can have unique metadata. If multiple Head components specify the same tag, the last one wins, so you can have defaults in _app and override them per page. For the App Router, this is replaced by the metadata export or generateMetadata function which is more ergonomic. I always set title, description, and Open Graph tags for public pages. This proper metadata is essential for search rankings and social media sharing.",
            "next/head modifies document head per page for SEO metadata. Set title, description, Open Graph tags. Later Head components override earlier ones, enabling defaults with page-level overrides. Essential for SEO and social sharing. In App Router, export metadata object or generateMetadata function instead. Every public page needs unique, descriptive metadata. Dynamic titles for detail pages improve SEO significantly."],
    },
    {
        text: "How do you handle environment variables in Next.js?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs],
        answers: ["Next.js loads environment variables from .env.local for local development and .env.production for production builds. Variables are automatically available in Node.js code through process.env. For client-side code, you must prefix variables with NEXT_PUBLIC_ to expose them - this is a security feature to prevent accidentally exposing server secrets. Variables without the prefix are only available server-side. You can also have .env.development for development-specific values. The precedence is .env.local overrides .env, and specific environments override general. Never commit .env.local to version control - use .env for defaults that can be committed. I keep API keys and secrets in .env.local, and only expose public API endpoints with NEXT_PUBLIC_ prefix for client-side use.",
            "NEXT_PUBLIC_ prefix exposes variables to client - security feature. Server-only variables have no prefix. .env.local for secrets (gitignored), .env for defaults (committed). Precedence: .env.local > .env.production/.env.development > .env. Never expose API keys to client. Only NEXT_PUBLIC_ variables bundle into client JavaScript. Common pattern: server-side API keys for getServerSideProps, public API endpoints for client."],
    },
    {
        text: "What is the public folder and how does static file serving work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs],
        answers: ["The public folder in Next.js is for static assets that should be served as-is without processing. Files in public are available at the root URL path - so public/logo.png is accessible at /logo.png. This is perfect for robots.txt, favicons, images, fonts, or any other static files. Important: don't use 'public' in the path when referencing these files, just start with a slash. These files are served directly and don't go through webpack, so they won't be optimized. For images you want optimized, you should still use the Image component and can reference public folder images. I use public for assets that must have specific URLs or filenames like robots.txt, and for assets that are already optimized and don't need processing.",
            "public/ serves static files at root URL path: public/logo.png becomes /logo.png. Don't include 'public' in paths. Use for robots.txt, favicons, fonts, pre-optimized images. Files bypass webpack - no processing or optimization. For image optimization, still use next/image component with public folder images. Good for assets needing specific URLs or already-optimized files."],
    },
    {
        text: "What is getStaticProps and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.ssg],
        answers: ["getStaticProps is a function you export from a page that runs at build time to fetch data for static generation. It runs on the server, never on the client, so you can directly query databases or access the filesystem. The data it returns as props is baked into the HTML at build time. Use it for pages with data that doesn't change often or can be the same for all users - blogs, marketing pages, documentation. You can add a revalidate value to enable ISR, so the page regenerates in the background after that interval. The advantage is performance - the HTML is pre-generated and served instantly. The tradeoff is the data is only as fresh as the last build or revalidation. I use it for most content-driven pages.",
            "getStaticProps runs at build time on server, returning props baked into static HTML. Direct database/filesystem access since it's server-only. Add revalidate for ISR. Use for content that's same for all users: blogs, docs, product pages. Incredibly fast since serving static files. Trade-off is staleness without ISR. I default to getStaticProps with revalidate for most content pages."],
    },
    {
        text: "What is getServerSideProps and what are its performance implications?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum.ssr, ValidTag.enum.performance],
        answers: ["getServerSideProps runs on every request, fetching data server-side and rendering the page with that data before sending it to the client. Use it when data must be fresh on every request or is user-specific. It has access to the request object so you can check cookies, headers, or query params. The performance implication is that it's slower than static generation - every request waits for the server to fetch data and render. This adds server load and costs. It can't be cached by CDNs in the same way. Time to first byte is slower. I only use it when absolutely necessary - for authenticated, personalized data or when data changes so frequently that ISR won't work. Often, client-side fetching with SWR or TanStack Query is a better choice if the initial SEO HTML doesn't need the data.",
            "getServerSideProps runs per request - always fresh but slower. Access to request object for cookies, headers, auth. Performance cost: server work on every request, no CDN caching, slower TTFB. Use only when necessary: personalized data, auth-required pages, real-time data where ISR won't suffice. Often client-side fetching is better if SEO doesn't need the data. I minimize GSSP usage due to performance implications."],
    },
    {
        text: "What is getStaticPaths and how does dynamic routing work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum["dynamic-routes"]],
        answers: ["getStaticPaths is required when you have dynamic routes with getStaticProps. It tells Next.js which dynamic pages to pre-render at build time. You return an array of params objects representing each page to generate. For a blog, you'd fetch all post IDs and return paths for each. The fallback option controls what happens for paths not returned - false means 404, true shows a fallback while generating, and 'blocking' waits for generation. For thousands of pages, you might only pre-render popular pages and set fallback to 'blocking' to generate others on-demand. This combines static generation's speed with the flexibility of dynamic content. I use getStaticPaths for blogs and product pages - pre-render at build time but handle new content gracefully.",
            "getStaticPaths tells Next.js which dynamic routes to pre-render. Return array of params for each path. fallback: false = 404 for unknown paths, 'blocking' = generate on demand, true = show loading then generate. For large sites, pre-render popular pages, use fallback for the long tail. Pattern: fetch all IDs at build, generate paths. Combines static performance with dynamic flexibility."],
    },
    {
        text: "How do API routes work in the Pages Router?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum["pages-router"], ValidTag.enum["api-routes"]],
        answers: ["API routes let you build API endpoints inside your Next.js app. Files in pages/api map to /api/* endpoints. You export a function that receives req and res objects, similar to Express. These run as serverless functions and can access databases, external APIs, or the filesystem. They're great for hiding API keys, handling authentication, or creating simple backends without deploying a separate server. You can use them for webhooks, form submissions, or proxy requests. They support dynamic routes just like pages. The limitation is they're serverless, so not ideal for long-running processes or WebSockets. I use them for simple backend logic, authentication endpoints, and proxying third-party APIs to hide credentials.",
            "pages/api files create serverless API endpoints. Express-like req/res handlers. Perfect for: hiding API keys, auth endpoints, webhooks, proxying third-party APIs. Serverless means no long-running processes or WebSockets. Dynamic routes work like pages. Runs server-side, can access databases directly. I use for form handlers, auth, and API proxying. App Router uses route.js with Response object instead."],
    },
    {
        text: "What is ISR (Incremental Static Regeneration) and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.nextjs, ValidTag.enum.isr, ValidTag.enum.performance],
        answers: ["ISR lets you update static pages after build without rebuilding the entire site. You add a revalidate value in seconds to getStaticProps. After that time, the next visitor gets the cached stale page, but triggers a regeneration in the background. Once regenerated, subsequent visitors get the fresh page. This gives you the performance of static sites with content that updates periodically. It's perfect for content that changes regularly but doesn't need to be real-time - product listings, blog posts, news articles. The sweet spot is content that changes hourly or daily. I might use 60 seconds for product prices, 3600 for blog posts. The beautiful part is it scales infinitely since you're mostly serving cached pages, but content stays reasonably fresh.",
            "ISR bridges the gap between static and dynamic content. You set a revalidate time in getStaticProps, and after that period expires, the next request serves the stale page while regenerating a fresh version in the background. Future visitors then get the updated page. It's ideal for content like product catalogs or blog posts that change periodically but don't need real-time updates. You get static performance with reasonably fresh content. I use shorter revalidate times for frequently changing data like prices, longer times for blog content. The App Router has on-demand revalidation too, letting you trigger updates programmatically when content changes."],
    },

    // React Native
    {
        text: "What is React Native and how does it differ from React?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.react],
        answers: ["React Native is a framework for building native mobile apps using React. The core difference is the rendering target - React renders to the DOM, while React Native renders to native platform components. Instead of div and span, you use View and Text. Instead of CSS, you use StyleSheet which is similar but not identical. The JavaScript runs in a JS engine, but the UI is actual native components, not a WebView. This gives you native performance and feel. You write mostly the same React code with hooks and components, but the platform APIs and components are different. The development experience is similar - hot reloading, component-based architecture. You can share business logic between web and mobile but not UI code. It's a great choice when you want native apps without learning Swift or Kotlin.",
            "React Native lets you build native iOS and Android apps using React and JavaScript. Unlike web React which renders to HTML elements, React Native renders to actual native platform components. You use View instead of div, Text instead of span, and StyleSheet for styling. The JavaScript logic is the same React you know with components and hooks, but the output is native UI, not a web view. This gives you native performance and platform-specific look and feel. The big advantage is code sharing across platforms and using your existing React skills. I find the development experience excellent with hot reloading, and you can share business logic between mobile and web apps."],
    },
    {
        text: "What is the difference between React Native and Expo?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["React Native is the core framework, while Expo is a set of tools and services built on top of it. Expo provides a managed workflow where you don't deal with native code - you just write JavaScript and Expo handles the native layer. It includes a bunch of commonly needed APIs like camera, location, and notifications out of the box. The Expo Go app lets you preview on real devices without building. The downside is you're limited to Expo's supported modules - you can't add arbitrary native code. Expo has a bare workflow that gives you more control while keeping some Expo tools. Plain React Native requires managing iOS and Android projects and writing native modules when needed. I start with Expo for most projects and only eject if I need custom native functionality.",
            "Expo is essentially a platform and set of tools built on React Native that simplifies mobile development. With Expo's managed workflow, you write JavaScript without touching native iOS or Android code directly. It comes with many common APIs built in like camera, push notifications, and file system access. Expo Go lets you instantly test on devices without building. The tradeoff is less flexibility for custom native modules, though Expo's dev clients and config plugins have expanded what's possible. Plain React Native gives full native access but requires managing Xcode and Android Studio projects. I recommend starting with Expo for most projects since it's much faster to get started, then ejecting only if you hit limitations."],
    },
    {
        text: "What are the core components in React Native?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["The fundamental components are View (like div, a container), Text (for displaying text - all text must be in Text components), Image (for images), ScrollView (scrollable container), TextInput (for user input), and TouchableOpacity or Pressable (for touchable elements). View is your main layout building block using flexbox. Text is required for any text content. ScrollView makes content scrollable but renders everything at once. For long lists, you use FlatList or SectionList which are optimized with virtualization. Button is basic but limited, so you often use Touchables to make custom buttons. SafeAreaView handles notches and safe areas. Modal creates overlays. These primitives combine to build complex UIs just like HTML elements, but they map to native components.",
            "The essential building blocks are View for containers and layout, Text for all text content, Image for displaying images, ScrollView for scrollable areas, TextInput for user input, and Pressable or TouchableOpacity for touch interactions. View uses flexbox for layout, which is the default layout model. Importantly, all text must be wrapped in Text components. For lists, FlatList and SectionList provide virtualized rendering for performance. SafeAreaView handles device notches and safe areas automatically. These core components map directly to native views on each platform, which is why you get native performance. I combine these primitives to build complex interfaces, similar to how you'd compose HTML elements on the web."],
    },
    {
        text: "What is the difference between StyleSheet and inline styles in React Native?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.performance],
        answers: ["StyleSheet is React Native's API for creating optimized style objects. You call StyleSheet.create() with an object of styles, and it returns an optimized reference. Inline styles are plain JavaScript objects passed directly to the style prop. The key difference is performance - StyleSheet objects are created once and reused, while inline styles create new objects on every render, potentially causing unnecessary re-renders. StyleSheet also validates styles in development, catching typos. However, inline styles are fine for dynamic styles that actually change, like interpolated values in animations. I use StyleSheet for static styles defined once, and inline styles only when I need to compute styles based on props or state. The performance difference isn't huge but it's a best practice.",
            "StyleSheet.create() creates optimized, reusable style objects that are processed once and referenced by ID internally. Inline styles are regular JavaScript objects created on each render. The performance benefit of StyleSheet is that the style objects are created once at module load rather than on every render, avoiding unnecessary object allocations. StyleSheet also validates your styles during development, catching typos and invalid properties. I use StyleSheet for all static styles and reserve inline styles for truly dynamic values that depend on state or props. You can combine both by passing an array to the style prop, like style={[styles.base, { opacity: fadeValue }]}, getting the best of both approaches."],
    },
    {
        text: "How do you handle platform-specific code in React Native?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"]],
        answers: ["React Native provides several ways to handle platform-specific code. The Platform module has a Platform.OS property that's 'ios' or 'android', so you can write conditional logic. Platform.select() is cleaner for choosing between values. For larger differences, you can use file extensions like Component.ios.js and Component.android.js, and React Native automatically imports the right one. This is great when entire components need to differ. You can also use platform-specific native modules when JavaScript isn't enough. I use Platform.select for small style differences, separate files when the components diverge significantly, and try to keep most code shared. The goal is to maximize code reuse while handling the inevitable platform differences gracefully.",
            "There are multiple approaches depending on how different the code needs to be. For small differences, Platform.OS gives you 'ios' or 'android' for conditionals. Platform.select() is cleaner for choosing values: Platform.select({ ios: 10, android: 12 }). For larger differences, use platform-specific file extensions like Button.ios.js and Button.android.js, and the bundler automatically picks the right one. This keeps platform code cleanly separated. I typically use Platform.select for minor style adjustments like shadows, separate files when components need significantly different implementations, and keep business logic shared. The goal is maximizing code reuse while respecting platform conventions."],
    },
    {
        text: "What is the difference between FlatList and ScrollView?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.performance],
        answers: ["ScrollView renders all children immediately, even if they're off-screen. This is fine for short lists but terrible for performance with hundreds of items. FlatList only renders items currently visible on screen plus a small window around them, using virtualization. As you scroll, it unmounts off-screen items and mounts new ones. This keeps memory usage constant regardless of list length. FlatList is built for data arrays - you pass data and a renderItem function. It also has built-in optimizations like item keys and performance props. Use ScrollView for small, known-size content. Use FlatList for any data-driven list, especially if it could grow. SectionList is similar to FlatList but supports sections with headers. I default to FlatList for any list of data.",
            "The key difference is virtualization. ScrollView renders all its children at once regardless of visibility, which works for limited content but causes memory and performance issues with long lists. FlatList uses windowing to only render visible items plus a buffer, recycling items as you scroll. This keeps memory usage constant even with thousands of items. FlatList takes a data array and renderItem function, handles keys, and provides scroll performance optimizations. I use ScrollView for forms, settings screens, or any small fixed content. For any dynamic list, especially from an API, FlatList is essential. SectionList adds section support with headers for grouped data."],
    },
    {
        text: "How do you debug React Native applications?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.debugging],
        answers: ["React Native has several debugging options. The Dev Menu (shake device or Cmd+D) gives access to features like remote debugging with Chrome DevTools, enabling Fast Refresh, and showing the element inspector. You can use console.log which appears in Metro bundler logs. React DevTools work for inspecting the component tree. For JavaScript debugging, you can use Flipper, which is the recommended debugger with network inspection, layout inspection, and logs. Reactotron is another popular tool. For native crashes, you use Xcode or Android Studio debuggers. The network inspector shows API calls. For performance, you can enable performance monitor to see FPS. I usually start with console.log, use React DevTools for component issues, and Flipper for network and comprehensive debugging.",
            "Debugging in React Native uses multiple tools. The Dev Menu, accessed by shaking the device or Cmd+D in simulator, provides options like Fast Refresh, element inspector, and remote debugging. Console.log output appears in Metro bundler logs or terminal. React DevTools connects to inspect the component tree and props. Flipper is a comprehensive debugging platform with network inspection, layout tools, and plugin support. For native-level issues, you use Xcode for iOS and Android Studio for Android. The built-in performance monitor shows FPS. I typically start with console.log for quick debugging, use React DevTools for component hierarchy issues, and reach for Flipper when I need network inspection or more advanced tooling."],
    },
    {
        text: "How do you handle navigation in React Native?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum["react-native"], ValidTag.enum.routing],
        answers: ["React Navigation is the most popular navigation library for React Native. It provides several navigator types - Stack for screen stacks like iOS navigation, Tab for bottom tabs, Drawer for side menus, and you can combine them. You define a navigation structure, and it handles transitions, gestures, and state. Each screen gets a navigation prop for navigating and passing params. Stack navigation feels native with platform-appropriate transitions. React Navigation is JavaScript-based, so it's highly customizable. There's also React Native Navigation which uses native navigation primitives for better performance but is less flexible. Expo Router is a newer file-based routing solution. I typically use React Navigation for most apps since it covers 95% of navigation needs with great DX.",
            "React Navigation is the standard navigation solution. It offers Stack Navigator for push/pop screen stacks, Tab Navigator for bottom tabs, and Drawer Navigator for side menus. You can nest these for complex navigation patterns. Each screen receives navigation and route props for navigating and accessing parameters. The library handles transitions, gestures, deep linking, and state persistence. It's JavaScript-based, making it highly customizable. Expo Router offers file-based routing as a newer alternative. React Native Navigation by Wix uses native navigation controllers for potentially better performance but less flexibility. For most apps, React Navigation covers everything needed with excellent developer experience."],
    },

    // Accessibility
    {
        text: "What is accessibility and why does it matter?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.a11y],
        answers: ["Accessibility, often abbreviated a11y, means making your application usable by everyone, including people with disabilities. This includes visual impairments requiring screen readers, motor impairments needing keyboard navigation, hearing impairments needing captions, and cognitive impairments benefiting from clear interfaces. It matters ethically because everyone deserves access to digital content. Legally, many jurisdictions require it. Practically, it improves UX for everyone - keyboard shortcuts help power users, captions help in noisy environments, clear labels reduce confusion. About 15% of the world has some disability, so you're excluding a significant audience without accessibility. Plus, many accessibility practices overlap with SEO. I consider accessibility from the start rather than retrofitting, and it's usually not much extra work if you do it right.",
            "Accessibility means ensuring everyone can use your application, regardless of ability. This includes supporting screen readers for vision impairments, keyboard navigation for motor impairments, captions for hearing impairments, and clear design for cognitive accessibility. It matters ethically because digital access is increasingly essential. It also matters legally since many countries require accessible websites, and practically because it improves usability for everyone. Captions help in loud environments, keyboard navigation helps power users, and clear labels reduce confusion for all. About 15% of people have some disability, so inaccessible sites exclude a significant audience. I build accessibility in from the start since retrofitting is much harder."],
    },
    {
        text: "What are WCAG guidelines?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.wcag],
        answers: ["WCAG (Web Content Accessibility Guidelines) are the international standard for web accessibility, created by W3C. They're organized around four principles: Perceivable, Operable, Understandable, and Robust. There are three conformance levels - A (basic), AA (mid-range, most commonly targeted), and AAA (highest). WCAG covers things like providing text alternatives for images, ensuring sufficient color contrast, making all functionality keyboard accessible, giving users enough time to read content, and avoiding content that could cause seizures. Version 2.1 is current, with 2.2 adding more criteria. Most legal requirements reference WCAG AA compliance. I use it as a checklist for accessibility - things like 4.5:1 contrast ratio for normal text, alt text for images, and proper heading structure are all WCAG requirements.",
            "WCAG stands for Web Content Accessibility Guidelines, the W3C's international accessibility standard. They're built on four principles called POUR: Perceivable, Operable, Understandable, and Robust. There are three conformance levels: A is basic, AA is standard for most legal requirements, and AAA is the highest. Specific criteria include 4.5:1 color contrast for text, alt text for images, keyboard accessibility for all functions, sufficient time limits, and avoiding seizure-inducing content. WCAG 2.1 and 2.2 are current versions with mobile and cognitive additions. I target WCAG AA compliance and use it as my accessibility checklist. Most accessibility laws reference these guidelines."],
    },
    {
        text: "What is the difference between ARIA roles and semantic HTML?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum.aria, ValidTag.enum["semantic-html"]],
        answers: ["Semantic HTML uses elements that convey meaning - button, nav, article, header. These have built-in accessibility features and roles. ARIA (Accessible Rich Internet Applications) is a set of attributes you add to HTML to provide additional accessibility information, like role='button' or aria-label. The key principle is: use semantic HTML first, ARIA only when semantic HTML isn't sufficient. A real button is better than a div with role='button' because it's keyboard accessible by default and has expected behavior. ARIA is for complex widgets that don't have native HTML equivalents, like tabs or accordions, or to provide extra context like aria-live regions. Misusing ARIA can make things worse for screen reader users. I use semantic HTML whenever possible and ARIA to fill gaps.",
            "Semantic HTML elements like button, nav, and header have built-in accessibility. Screen readers understand their purpose, they're keyboard accessible by default, and they behave as expected. ARIA attributes add accessibility information to elements, like role='button' or aria-label for context. The first rule of ARIA is: don't use ARIA if you can use native HTML. A real button element beats a div with role='button' because it handles keyboard events, focus states, and screen reader announcements automatically. ARIA is valuable for complex widgets like tabs, modals, or autocompletes where native HTML doesn't exist. I reach for semantic HTML first and use ARIA to enhance when native elements aren't sufficient."],
    },
    {
        text: "What is focus management and why does it matter?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.accessibility, ValidTag.enum["focus-management"]],
        answers: ["Focus management is controlling which element has keyboard focus and ensuring a logical focus order. It's critical for keyboard and screen reader users who navigate by focus. Focus should follow a logical order matching the visual layout, usually top to bottom, left to right. When you open a modal, focus should move into it and trap there until closed, then return to the trigger. Skip links let users bypass repetitive navigation. You shouldn't remove focus outlines unless replacing them with equally visible indicators. Custom components need proper tabIndex and focus handling. When content changes, like after deleting an item, focus should move somewhere logical. I test all interactions with keyboard-only navigation to ensure focus is always visible and logical. React refs and focus management is key for modals and dynamic content.",
            "Focus management controls which element has keyboard focus as users navigate. It's essential for keyboard and screen reader users who can't use a mouse. Focus should flow logically through the page matching visual order. Key scenarios include: moving focus into modals when opened and trapping it there, returning focus to triggers when dialogs close, moving focus to meaningful places after dynamic content changes. Never remove focus outlines without providing visible alternatives. Skip links help users bypass navigation. I test all features with keyboard only to verify focus is always visible and behaves logically. In React, refs and useEffect handle programmatic focus for dynamic content."],
    },

    // Testing
    {
        text: "What is React Testing Library and how does it differ from Enzyme?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["React Testing Library is a testing library focused on testing components the way users interact with them, rather than implementation details. Enzyme, the older alternative, provides utilities to test component internals like state and props. The philosophy difference is key - RTL encourages testing behavior and output, while Enzyme encourages testing implementation. RTL doesn't let you access state or simulate lifecycle methods directly. Instead, you query by text, labels, roles - things users see. This makes tests more resilient to refactoring since changing how something works internally won't break tests as long as the output is the same. Enzyme tests often break when you refactor. RTL has become the standard and is recommended by the React team. I use RTL exclusively now for this reason.",
            "RTL and Enzyme represent different testing philosophies. Enzyme lets you test component internals like state, props, and lifecycle methods directly. React Testing Library intentionally hides these, forcing you to test behavior instead of implementation. With RTL, you query elements the way users find them, by text, labels, and roles, then assert on visible outcomes. This means refactoring components doesn't break tests as long as behavior stays the same. Enzyme tests often break on internal changes even when functionality works. RTL is now the React team's recommendation and the community standard. I switched to RTL because tests are more maintainable and give more confidence that features actually work for users."],
    },
    {
        text: "What are the guiding principles of React Testing Library?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["The core principle is: test your software the way users use it. This means querying elements the way users find them - by label text, displayed text, or role, not by implementation details like class names or component state. Tests should resemble how your app is used. This makes tests more maintainable because they don't break when you refactor implementation. Another principle is avoiding testing implementation details - don't test state, props, or component methods directly. Focus on inputs and outputs. Also, find elements by accessibility attributes when possible, which encourages accessible markup. The philosophy is that if your tests use the app like a user, they'll be valuable and maintainable. This has completely changed how I write tests for the better.",
            "The main principle is testing behavior from the user's perspective. Query elements the way users find them: by labels, text, and ARIA roles rather than class names, test IDs, or component structure. Don't test implementation details like internal state or methods. Focus on what the component does, not how it does it internally. Tests should resemble actual usage scenarios. This approach makes tests resilient to refactoring since internal changes don't break tests when behavior stays the same. It also encourages accessible markup since you query by accessibility attributes. These principles have made my tests more valuable because they catch real bugs while remaining stable during refactors."],
    },
    {
        text: "What are the different query methods and when do you use each?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl],
        answers: ["RTL has three types of queries: getBy, queryBy, and findBy, each with variants like getByRole, getByText, etc. getBy throws an error if the element isn't found - use it for elements that should be in the document. queryBy returns null if not found - use it to assert something doesn't exist. findBy returns a promise and waits for the element to appear - use it for async elements that show up after data loads or user interaction. Then there are AllBy variants for multiple matches. For query types, prefer getByRole for accessibility, getByLabelText for form fields, getByPlaceholderText as a last resort for inputs, and getByText for non-interactive text. getByTestId is an escape hatch when nothing else works. I follow the priority order: role, label, placeholder, text, test ID.",
            "There are three query variants: getBy throws if not found, use for elements that should exist. queryBy returns null if not found, use to assert absence. findBy returns a promise and waits, use for async content. Each has AllBy versions for multiple matches. The query types follow a priority based on accessibility: getByRole is preferred since it mirrors how assistive tech works, then getByLabelText for form fields, getByText for visible text, getByPlaceholderText, and getByTestId as a last resort. I use getBy for most assertions, queryBy when checking something shouldn't render, and findBy after async operations like API calls complete. Following the priority order also improves accessibility."],
    },
    {
        text: "How do you test user interactions?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["You use the userEvent library or fireEvent from RTL. userEvent is preferred because it simulates complete user interactions more accurately - like typing triggers keydown, keypress, and keyup events. fireEvent just dispatches single events. For clicks, you do userEvent.click(button). For typing, userEvent.type(input, 'text'). You can simulate hover, tab navigation, selecting options, and more. The pattern is: render the component, find elements using queries, interact with them using userEvent, then assert the result. For async results, you use waitFor or findBy queries to wait for changes. Always wrap assertions about async changes in waitFor. I use userEvent for all interactions because it better matches real user behavior and catches more potential bugs.",
            "Use userEvent from @testing-library/user-event over fireEvent. userEvent simulates real user interactions including all the events that would fire, like typing triggering focus, keydown, input, and keyup. fireEvent just dispatches a single event. The pattern is: render, query for elements, interact with userEvent, then assert results. userEvent.click() for clicks, userEvent.type() for typing, userEvent.selectOptions() for selects. For async results after interactions, use findBy queries or wrap assertions in waitFor to wait for DOM updates. I always use userEvent since it catches bugs that fireEvent misses by more accurately simulating how users actually interact with the UI."],
    },
    {
        text: "How do you test components with context?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react],
        answers: ["You wrap the component in the necessary context providers when rendering in tests. RTL's render function accepts a wrapper option where you can provide a component that wraps your test component. You can create a custom render function that includes all your common providers - theme, auth, router, etc. This keeps tests clean. If you need different context values per test, you pass them as props to your wrapper. For testing the context provider itself, you render a consumer component inside it and assert the values are correct. Mock the context values when you want to test different states. I create a custom render that includes all standard providers, then override specific values per test when needed.",
            "Wrap components in their required providers when testing. RTL's render accepts a wrapper option for this. The common pattern is creating a custom render function that includes standard providers like router, theme, and auth, then importing that instead of RTL's render directly. For testing different context states, pass values to your wrapper or mock the context entirely. When testing the provider itself, render a consumer component inside it and verify the values. I set up a test-utils file with a custom render that includes all app providers, making individual tests clean while ensuring components have the context they need."],
    },
    {
        text: "How do you test components with hooks?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.testing, ValidTag.enum.rtl, ValidTag.enum.react, ValidTag.enum.hooks],
        answers: ["For components using hooks, you test them like any other component - render, interact, assert. RTL doesn't care about implementation details like hooks. For testing custom hooks in isolation, you use renderHook from @testing-library/react. It renders a test component that calls your hook and returns the result. You can access result.current to get the hook's return value, and rerender to trigger updates. For hooks with side effects, use waitFor for async assertions. You can pass props to test different inputs. The pattern is similar to component testing but focused on the hook's interface. I usually test custom hooks in isolation to verify their logic, then test components using those hooks to verify integration.",
            "Components with hooks are tested like any other component since RTL focuses on behavior, not implementation. For custom hooks specifically, use renderHook from @testing-library/react. It wraps your hook in a test component and returns result.current with the hook's return value. Call rerender() to test updates, and use waitFor for async behavior. You can pass initialProps and update them on rerender. I test custom hooks in isolation when they contain complex logic worth testing independently, then also test through components that use them. Most hooks are simple enough that testing through components provides sufficient coverage."],
    },

    // Git
    {
        text: "What is git reflog and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git reflog is a record of where your HEAD and branch references have been, even including commits that are no longer reachable from any branch. It's like a safety net for your local repository. I use it when I accidentally reset to the wrong commit, delete a branch, or mess up a rebase. You can find the commit SHA you need and recover it with git reset or git cherry-pick. Reflog entries expire after 90 days by default. A common scenario is accidentally doing a hard reset and needing to get back to where you were - reflog shows all the previous HEAD positions so you can restore them. It's purely local and not shared with the remote. It's saved me many times when I've made mistakes with Git operations.",
            "Reflog tracks where HEAD has pointed, including commits no longer on any branch. It's your safety net when Git operations go wrong. If you accidentally reset, delete a branch, or mess up a rebase, reflog shows previous HEAD positions so you can recover. Run git reflog to see the history, find the commit you need, then git reset or git checkout to that SHA. Entries expire after 90 days by default. It's local only, not shared with remotes. I've used it many times to recover from mistakes. Knowing reflog exists lets you be more confident with Git since you can usually undo accidents."],
    },
    {
        text: "What is the difference between git reset and git revert?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git reset moves the current branch pointer to a different commit, potentially removing commits from the branch history. It rewrites history, which is dangerous for shared branches. Git revert creates a new commit that undoes the changes from a previous commit, preserving history. Reset is like going back in time and pretending something never happened, while revert is like creating an undo operation that shows you reverted something. Use reset for local commits you haven't pushed yet. Use revert for commits that are already on shared branches because it doesn't rewrite history. I use reset to clean up my local work before pushing, and revert when I need to undo something that's already been shared with the team.",
            "Reset moves your branch pointer backward, effectively erasing commits from history. Revert creates a new commit that undoes changes while preserving history. The key difference is history rewriting: reset is destructive to history, revert is additive. Use reset for unpushed local commits where you want to redo something. Use revert for shared commits on team branches since it doesn't require force pushing or cause problems for others. I use reset to clean up local work before pushing, like squashing messy commits. I use revert to undo something that's already on main or a shared branch."],
    },
    {
        text: "What are the different modes of git reset (soft, mixed, hard)?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git reset has three modes that differ in what they do with your changes. Soft reset moves the branch pointer but keeps your changes staged and in the working directory - use it when you want to redo a commit with different changes. Mixed reset, the default, moves the pointer and unstages changes but keeps them in your working directory - use it when you want to recommit with different staging. Hard reset moves the pointer and discards all changes in staging and working directory - use it when you want to completely throw away work. A common use for soft is squashing commits by resetting back and recommitting. Mixed is good for unstaging everything. Hard is dangerous but useful when you want to match a remote state exactly. I use hard reset carefully since it destroys work.",
            "The three modes control what happens to your changes. Soft keeps everything staged, just moves the branch pointer, useful for squashing commits or redoing a commit message. Mixed, the default, unstages changes but keeps them in your working directory, good for re-organizing what goes in each commit. Hard discards everything and makes your working directory match the target commit exactly. I use soft to squash multiple commits into one. Mixed when I want to restage things differently. Hard only when I want to completely abandon work and match a clean state, and I'm very careful with it since it's destructive."],
    },
    {
        text: "What is cherry-picking?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum["cherry-pick"]],
        answers: ["Cherry-picking is applying a specific commit from one branch to another. You use git cherry-pick with a commit SHA to copy that commit's changes onto your current branch as a new commit. It's useful when you need a specific fix from another branch but don't want to merge the whole branch. For example, if you have a bug fix on a feature branch but need it on main immediately, you can cherry-pick just that commit. The downside is it creates duplicate commits with different SHAs, which can complicate history. It's better to use proper merging or rebasing when possible. I use cherry-pick sparingly, mainly for hotfixes that need to go to multiple branches or when extracting specific work from a messy branch.",
            "Cherry-pick copies a specific commit from one branch to another, creating a new commit with the same changes. It's useful when you need just one commit without merging an entire branch. Common scenarios: applying a hotfix from a feature branch to main, or backporting a fix to a release branch. Run git cherry-pick <sha> to apply a commit to your current branch. The downside is duplicate commits with different SHAs, which can cause confusion if you later merge those branches. I use it sparingly, mainly for urgent hotfixes or extracting specific changes from branches, preferring regular merges when possible."],
    },
    {
        text: "What is GitFlow vs trunk-based development?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git, ValidTag.enum.branching],
        answers: ["GitFlow is a branching model with long-lived branches like develop and main, plus feature, release, and hotfix branches. It's structured but complex. Trunk-based development has everyone committing to a single main branch frequently, using short-lived feature branches or feature flags. GitFlow works well for scheduled releases and when you need to support multiple versions. Trunk-based development is better for continuous deployment and faster feedback. GitFlow can lead to merge hell with long-lived branches. Trunk-based requires good testing and CI/CD since main is always deployable. Modern development trends toward trunk-based with continuous deployment. I prefer trunk-based for web apps with continuous deployment, but GitFlow can make sense for mobile apps with release schedules and app store reviews.",
            "GitFlow uses long-lived branches: main for production, develop for integration, plus feature, release, and hotfix branches. It's structured but can lead to complex merges. Trunk-based development keeps everyone on one main branch with very short-lived feature branches or feature flags, requiring good CI/CD since main is always deployable. GitFlow suits scheduled releases and supporting multiple versions, like mobile apps with app store review cycles. Trunk-based is ideal for web apps with continuous deployment since you get faster feedback and simpler history. Modern practice trends toward trunk-based. I use trunk-based for web development and reserve GitFlow for projects with specific release schedules."],
    },
    {
        text: "What are Git hooks?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.git],
        answers: ["Git hooks are scripts that Git executes before or after events like commit, push, or merge. They live in .git/hooks and can be written in any scripting language. Common hooks include pre-commit for running linters or tests before committing, commit-msg for validating commit message format, pre-push for running tests before pushing, and post-merge for updating dependencies after pulling. Hooks are local by default, but tools like Husky let you share them via package.json. I use pre-commit hooks to run prettier and eslint, ensuring code is formatted and linted before committing. Pre-push hooks can run tests to prevent broken code from being pushed. The downside is they can slow down Git operations if they do too much work.",
            "Git hooks are scripts that run automatically at specific Git events. They're stored in .git/hooks and can be any executable script. Common hooks: pre-commit runs before commits, great for linting and formatting; commit-msg validates commit message format; pre-push runs tests before pushing; post-merge can install dependencies after pulls. Hooks are local by default, so teams use Husky to share them via package.json. I use pre-commit with lint-staged to run formatting and linting only on staged files, keeping it fast. The tradeoff is slower Git operations, so keep hooks focused on what matters most."],
    },

    // Performance
    {
        text: "How would you optimize a slow loading page?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.debugging],
        answers: ["I'd start by measuring to identify bottlenecks using Lighthouse or Chrome DevTools Performance panel. Common issues and fixes include: large bundle sizes - implement code splitting and lazy loading; unoptimized images - compress and use modern formats like WebP with proper sizing; too many network requests - bundle assets, use HTTP/2, implement caching; blocking JavaScript - defer non-critical scripts, async load when possible; slow API responses - implement caching, consider SSR or SSG; no CDN - serve static assets from a CDN; layout shifts - define dimensions for images and dynamic content. I prioritize based on impact - often images are the biggest win. The key is measuring first, fixing the biggest issues, then measuring again. Every optimization should be based on data, not assumptions.",
            "First, measure with Lighthouse and DevTools to identify actual bottlenecks. Common fixes: for large bundles, add code splitting and lazy loading; for images, compress them, use WebP, and set proper dimensions; for render blocking, defer non-critical JavaScript; for slow APIs, add caching or move to SSR/SSG; ensure static assets use a CDN. I prioritize by impact, usually images are the quick win. Check Core Web Vitals: LCP for load speed, CLS for visual stability, INP for interactivity. The key is measuring before and after each change since assumptions are often wrong. Target the biggest bottleneck first."],
    },
    {
        text: "What are Chrome DevTools and what are the key panels?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["Chrome DevTools is the built-in debugging and profiling toolset in Chrome. The key panels are: Elements for inspecting and modifying the DOM and CSS in real-time; Console for JavaScript errors, logs, and running code; Sources for debugging JavaScript with breakpoints and step-through; Network for analyzing requests, timing, and waterfall charts; Performance for recording and analyzing runtime performance; Application for inspecting storage, service workers, and cache; and Lighthouse for auditing performance, accessibility, and SEO. I use Elements constantly for CSS debugging, Network to identify slow requests, Console for quick debugging, and Performance when investigating slowness. The Coverage panel is also useful for finding unused JavaScript and CSS. DevTools is essential for frontend development - I have it open basically all the time.",
            "DevTools is Chrome's built-in suite for debugging and profiling. Key panels: Elements for DOM and CSS inspection and live editing; Console for logs, errors, and running JavaScript; Sources for debugging with breakpoints; Network for request analysis and timing waterfalls; Performance for runtime profiling and frame analysis; Application for storage, service workers, and cache inspection; Lighthouse for audits. I use Elements constantly for styling issues, Network to debug API calls and find slow requests, Console for quick debugging, and Performance when troubleshooting jank or slowness. It's essential to frontend development and I always have it open."],
    },
    {
        text: "What is the Network panel and how do you use it to debug?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["The Network panel shows all network requests with timing, size, and status. The waterfall chart visualizes when requests start and how long they take. I use it to identify slow API calls, large assets, failed requests, and caching issues. You can filter by type, search, and throttle network speed to test on slower connections. Click a request to see headers, payload, response, timing breakdown, and initiator. Common debugging scenarios: check if an API call is happening and what it returns, find which requests are slow or blocking, verify caching headers are correct, identify unnecessary requests, and debug CORS issues. The timing tab shows DNS, connection, waiting, and download times separately. I also use it to ensure resources are being served from CDN and check if compression is enabled.",
            "The Network panel displays all HTTP requests with status, size, and timing. The waterfall visualization shows request timing and blocking relationships. I use it to check if API calls are firing correctly, inspect request and response payloads, identify slow or failed requests, verify caching headers, and debug CORS issues. Filter by type like XHR, JS, or CSS. Throttle to simulate slow connections. Click any request to see headers, response body, timing breakdown showing DNS, connection, and download phases. The Initiator tab shows what triggered each request. I check this panel regularly to understand what my app is fetching and how long it takes."],
    },
    {
        text: "What is Lighthouse and how do you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum.lighthouse],
        answers: ["Lighthouse is an automated auditing tool built into Chrome DevTools that analyzes performance, accessibility, best practices, SEO, and PWA compliance. It loads your page and measures metrics like First Contentful Paint, Largest Contentful Paint, Time to Interactive, and Cumulative Layout Shift. Each category gets a score 0-100 with specific recommendations. I use it to identify performance bottlenecks, accessibility issues, and SEO problems. Run it in incognito mode to avoid extension interference. The performance score focuses on Core Web Vitals which affect SEO rankings. Common issues it catches include unoptimized images, render-blocking resources, poor contrast ratios, and missing alt text. I run Lighthouse regularly during development and aim for scores above 90, though I prioritize the specific recommendations over the score itself.",
            "Lighthouse audits pages for performance, accessibility, best practices, SEO, and PWA features. It measures Core Web Vitals like LCP, CLS, and INP, giving 0-100 scores with specific recommendations. Run it from DevTools or as a CLI tool. I use it to catch unoptimized images, render-blocking scripts, accessibility issues like low contrast, and SEO problems. Run in incognito to avoid extension interference. The recommendations are more valuable than the scores themselves. I run Lighthouse throughout development to catch issues early. It's particularly useful for checking Core Web Vitals which affect SEO. I aim for 90+ scores but focus on the actionable fixes it suggests."],
    },
    {
        text: "What is tree shaking and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum["tree-shaking"]],
        answers: ["Tree shaking is the process of eliminating dead code from your JavaScript bundle. Modern bundlers like Webpack and Rollup analyze ES module imports to determine what code is actually used and remove what isn't. It only works with ES modules because their static structure allows the bundler to determine dependencies at build time. For example, if you import one function from lodash-es, tree shaking ensures you only bundle that function, not the entire library. For it to work effectively, your dependencies need to use ES modules and your code must use import/export syntax. Side effects can prevent tree shaking - you can specify sideEffects in package.json to help the bundler. I make sure to import only what I need and use libraries that support tree shaking for smaller bundle sizes.",
            "Tree shaking removes unused code from bundles. Bundlers analyze ES module imports to detect dead code and exclude it from the final bundle. It only works with ES modules since their static structure allows compile-time analysis. If you import one function from a library, only that function gets bundled, not the whole thing. For it to work well, use ES module versions of libraries like lodash-es instead of lodash. Avoid side effects in modules, or declare them in package.json sideEffects field. I always use named imports and choose tree-shakeable libraries to keep bundles small."],
    },
    {
        text: "What is code splitting and how do you implement it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum["code-splitting"]],
        answers: ["Code splitting divides your bundle into smaller chunks that can be loaded on demand, reducing initial load time. In React, you implement it with dynamic imports and React.lazy. Instead of a regular import, you use React.lazy with a dynamic import function, then wrap the component in a Suspense boundary that shows a fallback while loading. Modern bundlers automatically split these into separate chunks. You can split at route level so each page loads its own code, or split large dependencies like charting libraries. Next.js does route-based splitting automatically. The benefit is faster initial page loads since users only download code for what they're viewing. I use it for routes, large dependencies, and below-the-fold content. The tradeoff is managing loading states and potentially more network requests.",
            "Code splitting breaks your bundle into smaller chunks loaded on demand. In React, use dynamic imports with React.lazy: const Component = React.lazy(() => import('./Component')). Wrap lazy components in Suspense with a fallback for loading states. Bundlers automatically create separate chunk files. Split by routes so each page loads its own code, or split heavy dependencies like charts or rich text editors. Next.js and other frameworks do route-based splitting automatically. The benefit is smaller initial downloads since users only fetch code for what they're viewing. I code split routes, large dependencies, and below-the-fold features. The tradeoff is managing loading states and additional network requests."],
    },
    {
        text: "What is lazy loading and when should you use it?",
        level: Level.enum.mid,
        category: Category.enum.frontend,
        tags: [ValidTag.enum.performance, ValidTag.enum["lazy-loading"]],
        answers: ["Lazy loading defers loading resources until they're actually needed, typically when they're about to enter the viewport. For images, you can use the native loading='lazy' attribute on img tags, or intersection observer for more control. For JavaScript, you use code splitting with dynamic imports. For components, React.lazy loads them on demand. Use lazy loading for below-the-fold images, off-screen content, modal content, tabs, and route components. It dramatically improves initial page load by reducing the amount of data downloaded upfront. The downside is potential layout shift if you don't reserve space, and slight delay when content loads. I lazy load images by default, especially on content-heavy pages. For critical above-the-fold content, load it eagerly. The key is balancing initial performance with user experience.",
            "Lazy loading defers resource loading until needed, improving initial page load. For images, use the native loading='lazy' attribute or Intersection Observer for more control. For JavaScript components, use React.lazy with dynamic imports. Apply it to below-the-fold images, off-screen content, modals, and route components. The main benefit is reduced initial download size since users only load what they're about to see. Set explicit dimensions on lazy images to prevent layout shifts. Keep critical above-the-fold content eager. I lazy load most images and heavy components by default. The tradeoff is a slight delay when content appears, so balance performance with perceived smoothness."],
    },
];
