import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.behavioral,
    typeof Level.enum.mid
>[] = [
    {
        text: "What's the most complicated feature or project you've ever worked on?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["The most complicated project I worked on was building a real-time data processing pipeline that handled millions of events per day. The complexity came from multiple factors - we had to ensure data consistency across distributed systems, handle backpressure when downstream services were slow, and maintain exactly-once delivery semantics. The technical challenges included coordinating Kafka consumers, implementing circuit breakers for failing services, and building a sophisticated monitoring system to track data flow. What made it particularly challenging was that we had to migrate from the legacy system while it was still running, so we built a dual-write system with reconciliation logic. It took about four months with a team of five engineers, but we successfully launched it with zero downtime and it's been running smoothly ever since."],
    },
    {
        text: "What's your proudest accomplishment (of something you built)?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I'm most proud of building an accessibility-first component library that became the foundation for our entire product suite. What made this special was that I didn't just focus on making components that looked good - I worked closely with our accessibility team to ensure every component met WCAG 2.1 AA standards from the ground up. This included proper ARIA labels, keyboard navigation, screen reader support, and focus management. The impact was significant - we saw a 40% reduction in accessibility bugs reported in production, and we received positive feedback from users who rely on assistive technologies. It also sped up our development process since teams could now build accessible features by default. What I'm most proud of is that this became a cultural shift in how we approach development, not just a technical achievement."],
    },

    // AI in Development
    {
        text: "How do you use AI tools like GitHub Copilot or ChatGPT in your workflow?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["I use AI tools strategically throughout my workflow. GitHub Copilot is great for autocompleting boilerplate code, writing tests, and suggesting implementation patterns when I'm working in familiar territory. For example, when writing repetitive CRUD operations or test cases, Copilot can speed things up significantly. I use ChatGPT more for exploration and learning - like when I'm working with a new library and need to understand different approaches, or when I need help debugging a tricky issue. I'll also use it to generate initial documentation or explain complex code. That said, I never blindly accept AI suggestions. I always review the code carefully, ensure it fits our coding standards, and verify it actually solves the problem correctly. I see these tools as productivity boosters that handle the mundane stuff, freeing me up to focus on the more interesting architectural and problem-solving aspects of development."],
    },
    {
        text: "What are the limitations of AI code generation tools?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["AI tools have several important limitations. First, they lack understanding of your specific business context and codebase architecture - they might suggest code that works in isolation but doesn't fit with your existing patterns or conventions. Second, they can generate code that looks correct but contains subtle bugs or security vulnerabilities, especially around edge cases. I've seen AI suggest SQL queries that are vulnerable to injection or authentication logic with security flaws. Third, they struggle with complex architectural decisions that require deep understanding of trade-offs and long-term maintainability. Fourth, they're trained on public code, which means they might suggest outdated patterns or even code with licensing issues. And finally, they can make developers over-reliant, potentially weakening problem-solving skills if used as a crutch rather than a tool. That's why code review and testing are still essential, regardless of how the code was generated."],
    },
    {
        text: "How do you evaluate AI-generated code for quality and security?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral, ValidTag.enum.security],
        answers: ["I treat AI-generated code the same way I'd treat code from a junior developer - it needs thorough review. First, I read through it line by line to understand what it's actually doing, not just what it's supposed to do. I check for common security issues like SQL injection vulnerabilities, XSS risks, hardcoded secrets, or improper input validation. I also verify it follows our security best practices, like proper error handling that doesn't leak sensitive information. For quality, I check if it aligns with our coding standards, is properly typed if we're using TypeScript, has appropriate error handling, and includes edge case handling. I always write or run tests to verify the behavior, especially around boundary conditions. If it's touching sensitive areas like authentication or data access, I'm extra cautious and often ask for a second pair of eyes during code review. The key is never assuming AI code is correct just because it looks plausible."],
    },
    {
        text: "What is prompt engineering?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai],
        answers: ["Prompt engineering is the practice of crafting effective inputs to get better outputs from AI models. It's about being strategic with how you ask questions or frame requests to guide the AI toward giving you more useful, accurate, and relevant responses. For example, instead of asking 'write a function to sort data,' a better prompt might be 'write a TypeScript function that sorts an array of user objects by last name, handling null values by placing them at the end, and include JSDoc comments.' The more specific and contextual you are, the better the results. It also involves understanding techniques like few-shot learning, where you provide examples of what you want, or chain-of-thought prompting, where you ask the AI to explain its reasoning. In development, good prompt engineering can be the difference between getting generic boilerplate and getting code that actually fits your use case. It's becoming an important skill as AI tools become more integrated into our workflows."],
    },
    {
        text: "How has AI changed your development process?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["AI has significantly changed how I work, mostly for the better. The biggest change is in how quickly I can handle routine tasks - writing boilerplate, generating test cases, creating initial documentation, or scaffolding out standard CRUD operations. What used to take an hour might now take 15 minutes, which lets me spend more time on the interesting problems. It's also changed how I learn new technologies. Instead of spending hours reading documentation, I can ask targeted questions and get working examples quickly, then dive deeper into the docs for nuance. However, it's also made me more deliberate about code review. I've had to develop a more critical eye because I'm reviewing more code volume, and some of it comes from AI. I've also noticed it's changed team dynamics - we have to be more explicit about coding standards and patterns because AI needs to be guided toward our specific practices. Overall, I'd say it's made me more productive on execution but hasn't replaced the need for strong fundamentals and critical thinking."],
    },
    {
        text: "What tasks are AI tools good/bad at?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["AI tools excel at repetitive and well-defined tasks. They're great at generating boilerplate code, writing unit tests for straightforward functions, creating mock data, converting between formats, writing regex patterns, and explaining what existing code does. They're also good at suggesting standard implementations for common patterns like pagination or authentication flows. On the other hand, they struggle with tasks that require deep context and nuance. They're bad at making architectural decisions that involve trade-offs specific to your business needs, understanding complex domain logic, debugging issues that require knowledge of your infrastructure, and maintaining consistency across a large codebase over time. They also struggle with novel problems that don't have established patterns in their training data. And critically, they're bad at understanding security implications in context - they might generate code that's functionally correct but has subtle vulnerabilities. The key is knowing when to lean on AI for efficiency and when to rely on human judgment."],
    },
    {
        text: "How do you balance using AI tools with learning fundamentals?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["I think of AI tools as a calculator - they're incredibly useful, but you still need to understand the underlying math. My approach is to use AI for acceleration, not replacement. When I'm learning something new, I intentionally avoid using AI for the first implementation. I want to struggle through it, read the docs, and build that foundational understanding. Once I understand the concepts, then I'll use AI to speed up repetitive applications of that knowledge. For example, when I was learning React hooks, I wrote my first few custom hooks completely manually. Now that I understand the patterns, I'm comfortable using AI to scaffold similar hooks. I also make it a point to always read and understand AI-generated code rather than just copying it blindly. If I don't understand what the code is doing or why it works, that's a signal I need to learn more about that area. I also regularly challenge myself with side projects where I intentionally build things from scratch without AI assistance, just to keep those problem-solving muscles strong."],
    },
    {
        text: "What ethical considerations exist when using AI in development?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.ai, ValidTag.enum.behavioral],
        answers: ["There are several important ethical considerations. First is code licensing and intellectual property - AI models are trained on public code that has various licenses, and there's an ongoing debate about whether AI-generated code could inadvertently violate those licenses. Second is data privacy - you need to be careful about not feeding proprietary code or sensitive data into AI tools, especially cloud-based ones. Some companies have policies against this. Third is attribution and transparency - there's a question about whether we should disclose when significant portions of code are AI-generated, particularly in open source projects. Fourth is the impact on junior developers and learning - if we over-rely on AI, are we creating an environment where new developers can't develop fundamental skills? And fifth is bias - AI models can perpetuate biases from their training data, which could manifest in generated code or algorithms. I think it's important to be thoughtful about these issues and follow your company's policies while the industry figures out best practices around AI usage."],
    },

    // Behavioral/Project-Specific
    {
        text: "Tell me about a challenging project you worked on",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["One of my most challenging projects was migrating a monolithic application to microservices while the system was actively being used by thousands of customers. The challenge wasn't just technical - it was also organizational. We had to coordinate across multiple teams, maintain backward compatibility, and ensure zero downtime during the migration. Technically, we used the strangler fig pattern, gradually routing traffic to new services while keeping the monolith as a fallback. We built extensive monitoring and feature flags so we could roll back quickly if issues arose. The hardest part was handling distributed transactions and maintaining data consistency across services. We ended up implementing a saga pattern with compensating transactions. It took about eight months, and there were definitely some tense moments - like when we discovered a race condition in production that only appeared under high load. But we worked through it systematically, and the migration was ultimately successful. The system is now much more scalable and easier to maintain."],
    },
    {
        text: "How do you handle conflicts in a team?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I try to address conflicts early and directly, but with empathy. My approach is to first understand all perspectives - often conflicts arise from miscommunication or different context rather than actual disagreement. I'll have one-on-one conversations to understand each person's viewpoint before trying to solve anything. When it's a technical disagreement, I focus on objective criteria - what does the data show, what are the trade-offs, what aligns with our goals. I try to remove ego from the equation and frame it as 'we're all trying to solve the same problem.' For example, I once had a conflict with a senior engineer about whether to use REST or GraphQL for a new API. Instead of arguing, I suggested we prototype both approaches for a small feature and compare them based on our specific needs - performance, developer experience, and maintenance burden. We presented both to the team and made a data-driven decision together. If it's an interpersonal conflict, I try to facilitate open communication in a safe environment, and if needed, I'll involve a manager or mediator. The key is addressing it rather than letting it fester."],
    },
    {
        text: "How do you prioritize tasks when everything is urgent?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["When everything feels urgent, I step back and evaluate based on impact and actual urgency. I ask questions like: What's the cost of delay? How many users are affected? Is this a security issue? Does this block other work? I use a mental framework that considers both business impact and technical dependencies. For example, a production bug affecting 10% of users takes priority over a feature request from a single customer, even if that customer is being loud about it. I also communicate proactively - if I have five 'urgent' things, I'll talk to stakeholders to align on priorities rather than making assumptions. Sometimes what seems urgent to one person isn't actually critical when you understand the full context. I also look for quick wins - if I can knock out a couple small urgent items in 30 minutes, that might free up mental space to focus on a bigger problem. And honestly, I'm not afraid to push back when something isn't truly urgent. I'll explain the trade-offs and what else would be delayed. Usually, that conversation helps clarify what's really important."],
    },
    {
        text: "Tell me about a time you had to learn a new technology quickly",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I had to learn Kubernetes quickly when our team decided to containerize our applications and we had a tight deadline to migrate off our legacy infrastructure. I had worked with Docker before, but Kubernetes was completely new to me, and I had about three weeks to get up to speed. My approach was to combine hands-on practice with structured learning. I started with the official Kubernetes tutorials and documentation to understand the core concepts like pods, services, and deployments. Then I set up a local cluster with Minikube and started deploying simple applications to understand how everything connected. I also found a few good courses and blog posts that explained common patterns and pitfalls. The key was to focus on what we actually needed - I didn't try to learn everything about Kubernetes, just the parts relevant to our use case. I also reached out to colleagues at other companies who had done similar migrations and learned from their experiences. Within two weeks, I was comfortable enough to start migrating our staging environment, and by the deadline, we had successfully moved to production."],
    },
    {
        text: "How do you stay up-to-date with new technologies?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I use a mix of passive and active learning. For passive learning, I follow a curated set of newsletters like JavaScript Weekly and Hacker News, and I'm part of a few Discord communities where developers discuss new tools and trends. I also listen to podcasts during my commute. But I'm selective - I used to try to follow everything, and it was overwhelming. Now I focus on areas relevant to my work and interests. For active learning, I dedicate time to side projects where I can experiment with new technologies in a low-stakes environment. For example, I recently rebuilt a personal project using Next.js App Router to understand React Server Components. I also attend local meetups when I can, and I'll occasionally take a focused online course if I want to go deep on something. At work, I try to incorporate new learnings when appropriate - like suggesting we try a new testing library during a refactor. The key is being intentional about what I learn rather than chasing every shiny new thing."],
    },
    {
        text: "What's your approach to code review?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral, ValidTag.enum["code-review"]],
        answers: ["I approach code review as both a quality gate and a learning opportunity. First, I try to understand the context - I read the PR description and related tickets to understand what problem is being solved. Then I look at the code with several lenses. I check for correctness - does it actually solve the problem? I look for potential bugs, edge cases, and security issues. I consider maintainability - is the code clear and well-structured? I also think about testing - are there appropriate tests, and do they cover the important scenarios? When I leave comments, I try to be specific and constructive. Instead of just saying 'this is wrong,' I'll explain why and suggest alternatives. I also distinguish between blockers and suggestions - not everything needs to be fixed before merging. I try to recognize good code too, not just point out problems. When receiving reviews, I assume good intent and see feedback as an opportunity to improve. If I disagree with a comment, I'll discuss it rather than just making the change, because that conversation often leads to better solutions."],
    },
    {
        text: "How do you handle technical debt?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral, ValidTag.enum["technical-debt"]],
        answers: ["I think of technical debt as a strategic tool, not something inherently bad. Sometimes taking on debt is the right choice to ship faster or validate an idea. The key is being intentional about it. When I'm writing code, I try to distinguish between 'we need to move fast here' versus 'we're being sloppy.' If we're consciously taking on debt, I document it - either in code comments or in our issue tracker - so we know what needs to be addressed later. For existing debt, I advocate for addressing it incrementally rather than waiting for a mythical 'refactor sprint.' When I'm working in an area with debt, I try to leave it a bit better than I found it - the Boy Scout rule. I also try to prioritize debt that's actually causing problems - slowing down development, causing bugs, or making the system hard to maintain. Not all debt needs to be paid immediately. When I pitch paying down debt to stakeholders, I frame it in terms of business value - this refactor will reduce bugs, or this cleanup will let us ship features faster. That usually resonates better than 'the code is messy.'"],
    },
    {
        text: "Tell me about a mistake you made and how you handled it",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["Early in my career, I pushed a database migration to production without properly testing the rollback script. The migration ran fine, but later that day we discovered a critical bug in the new code and needed to roll back. The rollback script failed because I had made an assumption about the data structure that wasn't true in production. We had about an hour of downtime while I frantically fixed the rollback script and restored the data from backups. As soon as the incident was resolved, I took full ownership. I wrote up a detailed post-mortem explaining what happened, why it happened, and what I learned. I didn't try to shift blame or make excuses. Then I worked on preventive measures - I created a checklist for migrations that included testing rollback scripts against production-like data, and I advocated for adding automated testing of rollback procedures to our CI pipeline. I also started doing dry runs of risky migrations in staging. The experience taught me that testing the happy path isn't enough - you need to test failure scenarios too. My manager appreciated the honesty and the systematic approach to preventing it from happening again."],
    },
    {
        text: "How do you mentor junior developers?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I try to strike a balance between giving guidance and letting people learn through doing. When a junior developer asks for help, my first instinct is to ask questions rather than give answers - 'What have you tried? What do you think might work?' - to help them develop problem-solving skills. But I also recognize when someone is truly stuck and needs more direct help. I like to pair program for complex tasks, walking through my thought process out loud so they can see how I approach problems. During code reviews, I try to be educational - explaining not just what to change but why it matters and linking to resources for deeper learning. I also advocate for giving junior developers meaningful work, not just bug fixes. I'll break down larger projects into smaller chunks that they can own, and I make myself available for questions without hovering. Outside of code, I try to share career advice and help them navigate team dynamics. I also encourage them to share what they're learning - sometimes teaching something solidifies your own understanding. The goal is to help them become independent, confident developers."],
    },
    {
        text: "What's your ideal development environment/workflow?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["My ideal environment balances speed and quality. I like having a fast feedback loop - quick local builds, hot reloading in development, and fast-running unit tests that I can run frequently. I use VS Code with a carefully curated set of extensions - things like ESLint, Prettier, and language-specific tools that catch issues as I type. I'm a big fan of type checking and linting in my editor so I can fix problems before they even make it to a commit. For workflow, I like starting with the problem - understanding requirements and edge cases before writing code. I'll often write tests first or at least think through test cases, because it helps clarify what I'm building. I commit frequently with clear messages so it's easy to track changes and roll back if needed. I use feature flags for larger changes so I can merge early without exposing incomplete work. I also value good tooling for common tasks - things like automated formatting, pre-commit hooks, and CI/CD that catches issues before they reach production. And honestly, I like having uninterrupted focus time - too many meetings fragment my day and make it hard to get into flow."],
    },
    {
        text: "How do you handle tight deadlines?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["When facing tight deadlines, I focus on ruthless prioritization and clear communication. First, I make sure I understand what's actually needed - often stakeholders ask for everything when they really only need a subset of features to solve the immediate problem. I'll have a conversation about what's the MVP versus what's nice to have. Then I break down the work and identify the critical path - what absolutely has to be done and what can be deferred. I'm honest about trade-offs - if we want to hit this deadline, we might need to take on some technical debt or skip certain nice-to-have features. I communicate risks early so there are no surprises. During execution, I focus on making consistent progress rather than perfect code. I'll write tests for critical paths but might skip them for less important features. I minimize scope creep by pushing back on new requirements that come up mid-sprint. And I'm proactive about asking for help or flagging if I don't think we'll make it. After the deadline, I make sure to schedule time to address any shortcuts we took - paying down that technical debt before it becomes a bigger problem."],
    },
    {
        text: "Tell me about a time you had to make a technical decision with incomplete information",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["We needed to choose a database for a new service, but we didn't have clear requirements on the scale or query patterns yet - the product was still being defined. I had to make a recommendation quickly because it was blocking other work. My approach was to gather what information I could and make a reversible decision. I talked to the product team to understand the general direction, even if specifics weren't finalized. Based on that, I identified key constraints - we needed strong consistency, the data was relational, and we anticipated moderate but not massive scale. I evaluated a few options - Postgres, MySQL, and a couple of NoSQL databases. Given the uncertainty, I recommended Postgres because it's flexible, well-supported, handles relational data well, and can scale reasonably if needed. I made it clear to the team that this was based on current understanding and we should revisit if requirements changed significantly. I also structured our data access layer to be somewhat database-agnostic, so we could migrate if needed. Two years later, Postgres is still working great for us. The lesson was that sometimes you have to make a call with imperfect information, but you can reduce risk by choosing flexible options and building in the ability to change course."],
    },
    {
        text: "How do you approach performance optimization?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral, ValidTag.enum.performance],
        answers: ["My approach is to measure first, optimize second. It's tempting to optimize based on intuition, but that often leads to wasted effort on things that don't matter. I start by identifying if there's actually a performance problem - are users complaining, are metrics showing issues, or are we failing SLAs? Then I measure to find the bottleneck. For frontend work, I use tools like Chrome DevTools, Lighthouse, and performance profiling to identify slow renders or large bundles. For backend, I look at APM tools, database query logs, and profiling data. Once I know where the problem is, I prioritize based on impact. Sometimes the fix is simple - adding a database index, memoizing an expensive calculation, or lazy loading a component. Other times it requires architectural changes. I also think about trade-offs - optimization can make code more complex and harder to maintain, so I balance performance gains against code clarity. After making changes, I measure again to confirm the improvement and watch for regressions. And I try to build performance monitoring into our regular workflow so we catch issues before they become critical."],
    },
    {
        text: "What's your debugging process?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral, ValidTag.enum.debugging],
        answers: ["I follow a systematic approach rather than randomly trying things. First, I reproduce the bug consistently - if I can't reproduce it, it's hard to know if I've fixed it. I gather information about the context - what are the error messages, what changed recently, what's the state of the system. Then I form hypotheses about what could be causing it. I like to use the scientific method - come up with a theory, test it, and either confirm or rule it out. I'll use debugging tools strategically - setting breakpoints, inspecting variables, checking logs, or using network tools to see what's happening. I try to narrow down where the problem is by working backwards from the symptom to the cause. Sometimes I'll add extra logging or use binary search to isolate the issue - commenting out half the code to see if the problem persists. I also leverage version control - if it worked before, git bisect can help find when it broke. When I'm stuck, I take a break or explain the problem to someone else, which often helps me see it differently. Once I find the root cause, I fix it and verify the fix doesn't introduce new issues."],
    },
    {
        text: "How do you ensure code quality in your projects?",
        level: Level.enum.mid,
        category: Category.enum.behavioral,
        tags: [ValidTag.enum.behavioral],
        answers: ["I use a combination of automated tools and human processes. On the automation side, I'm a big believer in linting, formatting, and static analysis tools that catch common issues before code even gets to review. Type checking with TypeScript or similar tools prevents whole classes of bugs. I write comprehensive tests - unit tests for business logic, integration tests for how components work together, and end-to-end tests for critical user flows. I also set up CI pipelines that run all these checks automatically on every PR. On the human side, code review is crucial - having another set of eyes catch things that automated tools miss, like logic errors or design issues. I advocate for clear coding standards and patterns documented in a style guide, so the team has shared expectations. I also think about quality proactively during design - discussing edge cases, error handling, and testing strategy before writing code. And I pay attention to metrics - if we're seeing lots of bugs in a particular area, that's a signal we need better tests or a refactor. Quality isn't just one thing - it's a mindset and a set of practices applied consistently."],
    },
];
