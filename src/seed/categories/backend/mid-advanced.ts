import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const midAdvanced: QuestionForCategoryAndLevel<
    typeof Category.enum.backend,
    typeof Level.enum["mid-advanced"]
>[] = [
    // Node.js Advanced
    {
        text: "How do you profile and debug memory leaks in Node.js?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.debugging, ValidTag.enum.performance],
        answers: ["There are several approaches I typically use for tracking down memory leaks in Node.js. First, I'll use the built-in --inspect flag to connect Chrome DevTools and take heap snapshots at different points in time. By comparing snapshots, you can see which objects are growing unexpectedly. I also rely on tools like clinic.js or the Node.js memory profiler to visualize memory usage over time. For production environments, I'll use process.memoryUsage() to monitor heap usage and set up alerts. The key is to look for objects that keep growing - often it's event listeners that weren't removed, closures holding references, or caching without bounds. Once you identify the leak, fixing it usually involves proper cleanup, removing event listeners, and implementing cache size limits.",
            "I approach memory leaks in stages. First, confirm it's actually a leak by monitoring process.memoryUsage().heapUsed over time - if it grows without bound and garbage collection doesn't reclaim it, you have a leak. For investigation, I run Node with --inspect and connect Chrome DevTools. I take heap snapshots before and after suspicious operations, then compare them in the Allocation tab to see what's being retained. The 'objects allocated between snapshots' view is particularly useful. Common culprits: event listeners added but never removed, closures capturing large objects, unbounded caches or arrays, timers that reference objects. I use clinic.js doctor and heapprofile for automated analysis. For production, I set up memory alerts and sometimes enable core dumps on out-of-memory. Once identified, fixes are usually straightforward: use WeakMap or WeakSet for caches, always removeEventListener, clear timers, bound cache sizes with LRU eviction. Prevention is better - code reviews should check for these patterns."],
    },

    // GraphQL Advanced
    {
        text: "How do you handle file uploads in GraphQL?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["File uploads in GraphQL are typically handled using the GraphQL multipart request specification. The most common approach is to use a library like graphql-upload on the server side. You define an Upload scalar type in your schema, then in your mutation you can accept that type as an argument. On the client side, you send a multipart form request where the file is one part and the GraphQL operation is another. The server then maps the file to the appropriate resolver argument. An alternative approach some teams use is to separate file uploads from GraphQL entirely - you upload the file to a separate endpoint or directly to S3, get back a URL or ID, and then pass that through your GraphQL mutation. This can be cleaner for very large files and gives you more control over the upload process.",
            "There are two main approaches. The GraphQL multipart request spec extends GraphQL to handle file uploads directly. Libraries like graphql-upload add an Upload scalar type. Your mutation accepts Upload arguments, and the client sends multipart/form-data with the file and a map linking it to the GraphQL variables. The server streams the file to your resolver where you handle it - typically uploading to S3 or processing. This integrates files into your GraphQL schema cleanly. The alternative is separating uploads from GraphQL entirely. The client first uploads to a dedicated endpoint or directly to S3 using presigned URLs, gets back a file ID or URL, then calls a GraphQL mutation with that reference. This approach is simpler for large files, gives you more control over uploads, avoids the multipart complexity, and lets you leverage existing upload infrastructure. I typically use the separate approach for production - it's more robust, easier to scale, and doesn't couple file handling to GraphQL."],
    },
    {
        text: "How do you handle caching in GraphQL?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.caching],
        answers: ["Caching in GraphQL is more nuanced than REST because every request can be unique. On the server side, I typically implement field-level caching using DataLoader to batch and cache database queries within a single request - this prevents the N+1 query problem. For longer-term caching, you can use Redis or similar to cache resolver results with appropriate TTLs. The key is to cache at the resolver level based on the arguments. On the client side, libraries like Apollo Client provide normalized caching where objects are stored by their ID and typename, so when you fetch the same object in different queries, it's automatically deduplicated. You can also use HTTP caching with GET requests and cache-control headers if you structure your queries to be cacheable. For mutations, you need to be thoughtful about cache invalidation - either by refetching affected queries, updating the cache manually, or using subscriptions to keep data fresh.",
            "Caching in GraphQL happens at multiple layers. Request-level: DataLoader batches and caches database calls within a single request, solving the n+1 problem. Server-side caching: cache resolver results in Redis with TTLs. The cache key is typically the resolver name plus serialized arguments. You can use the @cacheControl directive to declare cache policies per field. CDN caching: some teams use persisted queries where queries are pre-registered and called by hash - this lets you use GET requests with standard HTTP caching. Client-side: Apollo Client normalizes responses by ID and typename, so the same User appears once in cache even if fetched via different queries. This deduplicates automatically. Cache invalidation is the hard part - after mutations, you either refetch affected queries, manually update the cache with the mutation result, or use subscriptions for real-time updates. I set cache policies based on data sensitivity and staleness tolerance: static data gets long TTLs, user-specific data gets short TTLs or no caching."],
    },

    // gRPC
    {
        text: "What is gRPC and how does it differ from REST?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum["rest-api"]],
        answers: ["gRPC is a high-performance RPC framework developed by Google that uses HTTP/2 and Protocol Buffers by default. The main differences from REST are quite significant. First, gRPC uses binary serialization with Protocol Buffers instead of JSON, which makes it much faster and more bandwidth-efficient. Second, it runs on HTTP/2, so you get features like multiplexing, bidirectional streaming, and header compression out of the box. Third, gRPC is contract-first - you define your service methods in a .proto file, which generates client and server code automatically, whereas REST is more flexible but requires more manual work. Fourth, gRPC supports different communication patterns like server streaming, client streaming, and bidirectional streaming, not just request-response. The tradeoff is that gRPC is less human-readable since it's binary, and browser support isn't as straightforward - you need gRPC-web. REST is still better for public APIs and when you need broad compatibility, but gRPC excels for internal microservices communication.",
            "gRPC is Google's RPC framework using HTTP/2 and Protocol Buffers. Compared to REST, the differences are fundamental. Protocol: gRPC uses HTTP/2 with multiplexed streams, header compression, and bidirectional communication. REST typically uses HTTP/1.1 with separate connections. Serialization: gRPC uses binary Protocol Buffers - smaller and faster than JSON. Schema: gRPC is contract-first - you define services in .proto files, generate type-safe client/server code. REST is more ad-hoc. Communication patterns: gRPC supports unary, server streaming, client streaming, and bidirectional streaming natively. REST is request-response only. Performance: gRPC is significantly faster due to binary format and HTTP/2. Tradeoffs: gRPC isn't browser-friendly without gRPC-Web, isn't human-readable for debugging, and has a learning curve. REST is universal, cacheable, and simpler. I use gRPC for internal service-to-service communication where performance matters. REST for public APIs needing broad compatibility."],
    },
    {
        text: "What are Protocol Buffers?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum["protocol-buffers"]],
        answers: ["Protocol Buffers, or protobufs, are a language-agnostic binary serialization format developed by Google. You define your data structures in .proto files using a simple interface definition language, and then you use a compiler to generate code for your target programming language. The main advantages over JSON are size and speed - protobufs are typically much smaller and faster to serialize and deserialize because they're binary and the schema is known ahead of time. They're also strongly typed and provide built-in versioning through field numbers, which makes it safer to evolve your API over time. The schema-first approach also serves as documentation and ensures consistency between client and server. The downside is they're not human-readable like JSON, which makes debugging a bit harder, and you need the schema to decode the messages. But for high-performance systems and internal service communication, protobufs are usually the better choice.",
            "Protocol Buffers are a binary serialization format and schema language from Google. You define messages in .proto files: message User { string name = 1; int32 age = 2; }. The numbers are field tags used in the binary encoding. The protoc compiler generates code for any target language - strongly typed classes with serialize/deserialize methods. Advantages over JSON: binary encoding is typically 3-10x smaller and faster to parse. Strong typing catches errors at compile time. Field numbers enable backward-compatible evolution - add fields without breaking old clients, remove fields safely. The schema is documentation that's always accurate. Downsides: not human-readable, you need the schema to decode, debugging is harder. You can't just curl an endpoint and see the response. For debugging, I use tools like grpcurl or protoc --decode. Protobufs excel for internal services where performance matters. For public APIs where human readability and universal compatibility matter, JSON is still preferable."],
    },
    {
        text: "What are the different types of gRPC calls (unary, server streaming, client streaming, bidirectional)?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc],
        answers: ["gRPC supports four different communication patterns. Unary is the simplest - it's just like a regular function call where the client sends one request and gets one response back. Server streaming is when the client sends a single request but the server responds with a stream of messages, which is useful for things like tailing logs or pushing real-time updates. Client streaming is the opposite - the client sends a stream of messages and the server responds with a single message when it's done processing everything, which is great for things like uploading chunks of a file. Bidirectional streaming is where both client and server can send streams of messages to each other independently, useful for things like chat applications or real-time collaboration. The key thing is that these are all built into the protocol itself, so you get proper backpressure handling and flow control that you'd have to implement yourself with other approaches.",
            "gRPC defines four RPC types in your .proto service definitions. Unary: rpc GetUser(GetUserRequest) returns (User) - simple request-response, like a function call. Server streaming: rpc WatchEvents(WatchRequest) returns (stream Event) - client sends one request, server sends many responses. Use for log tailing, real-time feeds, long-polling replacement. Client streaming: rpc UploadFile(stream FileChunk) returns (UploadResult) - client sends many messages, server responds once when done. Good for file uploads, batch processing. Bidirectional streaming: rpc Chat(stream Message) returns (stream Message) - both sides stream independently. Chat apps, collaborative editing, real-time sync. All streaming types use HTTP/2's multiplexing. You get flow control and backpressure built in - if the receiver is slow, the sender automatically slows down. This is powerful but adds complexity. Most APIs only need unary calls; streaming is for specific use cases where the communication pattern genuinely fits."],
    },
    {
        text: "When would you choose gRPC over REST or GraphQL?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum["rest-api"], ValidTag.enum.graphql],
        answers: ["I'd choose gRPC primarily for internal microservices communication where performance is critical. If you have services that need to talk to each other with low latency and high throughput, gRPC's binary format and HTTP/2 multiplexing really shine. It's also great when you need streaming capabilities - if you're doing real-time data processing, bidirectional communication, or need to push updates from server to client efficiently. The strong typing and code generation are huge benefits for internal APIs because they catch errors at compile time and make refactoring safer. I'd stick with REST for public APIs where you need broad compatibility and human-readable responses, or when you're working with clients that don't have good gRPC support. GraphQL is better when you have diverse clients with different data needs and you want to avoid over-fetching - it gives clients more flexibility. But for service-to-service communication in a microservices architecture, especially when performance matters, gRPC is hard to beat.",
            "I choose gRPC when: high performance is essential - service-to-service calls at scale where every millisecond and byte matters. Streaming is needed - log tailing, real-time updates, bidirectional communication. Strong contracts are valued - code generation catches breaking changes at compile time, not runtime. The team controls both ends - we can generate clients and servers from shared protos. I stick with REST when: it's a public API needing universal compatibility - browsers, curl, any HTTP client. Human readability matters for debugging. Simple CRUD operations where REST's conventions fit naturally. Caching at the HTTP layer is important. I choose GraphQL when: diverse clients need different data shapes - mobile wants minimal data, web wants more. Rapid iteration is needed - clients can change queries without backend changes. The data is highly interconnected - graph-like relationships are natural. For pure microservice-to-microservice communication inside my infrastructure, gRPC usually wins on performance and type safety."],
    },
    {
        text: "How does gRPC handle errors?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum["error-handling"]],
        answers: ["gRPC has a well-defined error model with a set of standard status codes similar to HTTP status codes, but more specific to RPC semantics. When an error occurs, the server returns a status code like INVALID_ARGUMENT, NOT_FOUND, PERMISSION_DENIED, or INTERNAL, along with an optional error message and metadata. On the client side, you catch these errors and check the status code to handle different error cases appropriately. What's nice is that error handling is consistent across all languages that gRPC supports. You can also use the richer error details mechanism where you attach structured error information using Google's error details protos - things like BadRequest, RetryInfo, or custom error details. This gives clients more context about what went wrong and how to fix it. For streaming calls, errors can be sent at any point in the stream, and the client needs to handle both per-message errors and stream-level errors.",
            "gRPC uses a status-based error model. Every response includes a status code from a fixed set: OK, CANCELLED, UNKNOWN, INVALID_ARGUMENT, NOT_FOUND, PERMISSION_DENIED, UNAUTHENTICATED, RESOURCE_EXHAUSTED, INTERNAL, UNAVAILABLE, and more. These are consistent across all gRPC implementations in any language. The server sets the status code and an optional error message. The client catches this as a language-appropriate exception or error type and checks the code. For richer errors, gRPC supports error details - structured data attached to errors using well-known types like BadRequest (which fields failed validation), RetryInfo (when to retry), QuotaFailure (which quota was exceeded). You pack these into the status using the Any type. This is much more informative than just a code and message. For streaming, errors can terminate the stream at any point. The client must handle both successful messages and the terminal error. Best practice: use appropriate status codes, not just INTERNAL for everything. Return actionable error details when possible."],
    },
    {
        text: "How do you handle authentication in gRPC?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum.auth],
        answers: ["Authentication in gRPC is typically handled through metadata, which is similar to HTTP headers. The most common approach is to use SSL/TLS for the transport layer security, and then pass authentication tokens in the metadata. For example, you might send a JWT or API key in the authorization metadata field with each request. gRPC has built-in support for different authentication mechanisms through its credentials system. You can use channel credentials for transport-level security like SSL, and call credentials for request-level authentication like tokens. On the server side, you typically implement an interceptor that extracts and validates the credentials from the metadata before the request reaches your actual handler. For service-to-service authentication, mutual TLS is common where both client and server authenticate each other using certificates. You can also integrate with external auth systems - for example, using OAuth2 tokens or integrating with service meshes like Istio that handle authentication at the infrastructure level.",
            "gRPC authentication works at two levels. Transport-level: TLS encrypts the connection. For mutual TLS (mTLS), both client and server present certificates - common for service-to-service auth where you trust the client's certificate. Per-call: tokens passed in metadata, gRPC's equivalent of headers. Clients attach JWTs, API keys, or OAuth tokens to the authorization metadata. gRPC's credentials system combines these: channel credentials for transport security, call credentials for per-call auth. You compose them together when creating a channel. On the server, an interceptor runs before each handler, extracts credentials from metadata, validates them, and populates context with user identity. If validation fails, return UNAUTHENTICATED status. For microservices, mTLS is excellent - the infrastructure handles identity, no application-level tokens needed. Service meshes like Istio or Linkerd can inject this transparently. For user-facing APIs behind a gateway, JWTs work well - the gateway validates tokens and forwards authenticated identity."],
    },
    {
        text: "How do you version gRPC APIs?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.grpc, ValidTag.enum["api-versioning"]],
        answers: ["There are a few approaches to versioning gRPC APIs. The most common is to include the version in the package name of your proto files, like 'package myapp.v1' or 'package myapp.v2'. This keeps different versions completely separate and allows you to run multiple versions simultaneously. When you need to make breaking changes, you create a new version of the service and gradually migrate clients over. Another approach is to make backward-compatible changes only - Protocol Buffers are designed to support this through field numbers. You can add new fields, deprecate old ones, and clients using older versions will still work. For this to work, you need to follow rules like never reusing field numbers and making new fields optional. Some teams also version at the method level by creating new methods like 'GetUserV2' while keeping the old ones around. The key is to be thoughtful about breaking changes and give clients time to migrate. In a microservices environment, you often run multiple versions side by side and route traffic based on client version.",
            "Versioning strategy depends on how breaking your changes are. Package-level versioning: include version in package name - package myapi.v1, package myapi.v2. Each is a complete, independent service definition. You run both versions, migrate clients gradually, retire old versions. This is cleanest for major breaking changes. Backward-compatible evolution: Protocol Buffers support this natively. Add new optional fields - old clients ignore them. Deprecate fields but don't remove them. Never reuse field numbers. Old and new clients work with the same service. This works well for continuous evolution without hard version bumps. Method-level versioning: add GetUserV2() alongside GetUser() in the same service. Simpler than full package versioning but messier over time. Best practice: prefer backward-compatible changes. Reserve new version numbers for truly breaking changes. Run versions in parallel during migration windows. Have clear deprecation timelines. Track which clients use which versions to know when you can retire old ones."],
    },

    // WebSockets Advanced
    {
        text: "How do you scale WebSockets horizontally?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets, ValidTag.enum.scalability],
        answers: ["Scaling WebSockets horizontally is challenging because they're stateful, long-lived connections. The main issue is that if a user is connected to server A, and you need to send them a message but it originates from server B, you need a way to route that message. The typical solution is to use a pub-sub system like Redis. When a message needs to be sent to a user, you publish it to a channel, and all servers subscribe to that channel. The server that has the actual WebSocket connection with the user picks up the message and sends it down. You also need sticky sessions or consistent hashing in your load balancer so that a user's connection attempts go to the same server. Another approach is to use a service mesh or a specialized WebSocket proxy that handles the routing for you. For very large scale, some teams separate the WebSocket servers from the application servers entirely - the WebSocket servers just handle connections and message routing, while the application logic runs separately. Tools like Socket.io have built-in support for this with their Redis adapter.",
            "WebSockets are stateful - each connection is bound to a specific server. Scaling horizontally requires solving the routing problem: user A is connected to server 1, but server 2 needs to send them a message. Solution: pub-sub with Redis. All servers subscribe to relevant channels. When sending to a user, publish to their channel. The server holding their connection receives it and delivers. For rooms/groups, publish to the room channel. Socket.io has a Redis adapter that handles this transparently. Load balancing: use sticky sessions or consistent hashing so reconnections go to the same server. Otherwise, connection state is lost on every reconnect. For massive scale, separate concerns: connection servers just maintain WebSocket connections and forward messages. Separate application servers handle business logic and publish through Redis. This lets you scale connection capacity independently. Alternative: use managed WebSocket services like AWS API Gateway WebSocket APIs or Pusher that handle the scaling infrastructure for you."],
    },
    {
        text: "How do you handle message ordering and delivery guarantees?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["WebSockets themselves only guarantee in-order delivery within a single connection, but they don't guarantee that messages are actually received or processed. To add stronger guarantees, you typically need to build them into your application layer. For ordering, you can add sequence numbers to your messages - the client keeps track of the last sequence number it processed and can detect gaps or out-of-order messages. For delivery guarantees, you implement an acknowledgment system where the client confirms receipt of each message, and the server retries if it doesn't get an ack within a timeout. You might also persist unacknowledged messages in a database or queue. Some libraries like Socket.io provide built-in support for this with delivery guarantees and automatic reconnection. For critical applications, you might implement an at-least-once delivery pattern where you store messages until they're acknowledged, which means you also need idempotency on the receiving side to handle duplicates. The key is understanding that WebSockets are just the transport - you need to add your own reliability layer if you need strong guarantees.",
            "WebSockets over TCP guarantee in-order delivery within a connection, but not cross-connection or after disconnects. For stronger guarantees, build application-level protocols. Ordering: include sequence numbers in messages. The receiver tracks the last sequence processed. If it receives sequence 5 before 4, it buffers 5 until 4 arrives. On reconnection, the client tells the server its last sequence to resume correctly. Delivery: implement acknowledgments. Each message has an ID. The receiver sends an ack. The sender retries unacked messages after timeout. Store pending messages durably so they survive server restarts. At-least-once delivery: persist messages until acked. This means the receiver might see duplicates, so make processing idempotent. At-most-once: don't retry, accept some loss. Exactly-once: combine at-least-once with deduplication on the receiver. Socket.io has built-in ack callbacks. For critical systems like financial transactions, I persist messages to a queue and use explicit acks. The transport is just the pipe - reliability is your responsibility."],
    },

    // Pagination Advanced
    {
        text: "What are the different pagination strategies?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination],
        answers: ["There are three main pagination strategies, each with different tradeoffs. Offset-based pagination is the simplest - you use LIMIT and OFFSET in your SQL queries, like 'get me 20 items starting at position 40'. It's easy to implement and allows jumping to any page, but it has performance issues with large datasets and can show duplicates or skip items if data changes between requests. Cursor-based pagination uses a pointer to a specific record, usually based on a unique, sequential field like an ID or timestamp. You return a cursor with each page, and the next request uses that cursor to fetch the next set of results. This is much more efficient and handles data changes gracefully, but you can't jump to arbitrary pages. Keyset pagination is similar to cursor-based but uses the actual values of the ordering columns as the cursor. For example, 'give me items where created_at > last_timestamp ORDER BY created_at'. This is the most performant for large datasets but requires indexed columns and doesn't handle arbitrary ordering well. For infinite scroll and real-time feeds, cursor-based is usually best. For traditional page navigation, offset can work if your dataset isn't huge.",
            "Three main strategies with different tradeoffs. Offset: LIMIT 20 OFFSET 40 - simple to implement, allows jumping to any page. Problems: performance degrades with large offsets, data shifts cause duplicates or skipped items. Cursor-based: use an opaque token encoding position, typically based on ID or timestamp. Query with WHERE id > cursor LIMIT 20. Constant performance regardless of position, handles real-time data changes well. Can't jump to arbitrary pages - only next/previous. Keyset: similar to cursor but uses actual column values. WHERE (created_at, id) > (last_created_at, last_id) ORDER BY created_at, id. Most performant, but requires indexed columns and stable sort order. Which to use: offset for admin interfaces with small datasets where page jumping matters. Cursor-based for feeds, infinite scroll, real-time data, large datasets. Keyset when you need maximum performance and control the sort columns. For APIs, cursor-based is usually the right default - it's efficient and handles the common cases well."],
    },
    {
        text: "What is offset pagination and what are its drawbacks?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination],
        answers: ["Offset pagination is the traditional approach where you use LIMIT and OFFSET in your queries - for example, to get page 3 with 20 items per page, you'd do LIMIT 20 OFFSET 40. It's intuitive and easy to implement, and it allows users to jump to any page number, which is nice for UI. The main drawback is performance - as the offset grows, the database still has to scan through all those rows before returning results, so page 1000 will be much slower than page 1. The second major issue is consistency - if items are added or deleted between page requests, you can see duplicates or skip items entirely. For example, if you're on page 2 and someone deletes an item from page 1, when you go to page 3 you'll skip an item. Similarly, if items are added, you might see the same item twice. For small datasets or admin interfaces where these issues don't matter much, offset pagination is fine. But for large datasets, public APIs, or real-time feeds, you generally want to use cursor-based pagination instead.",
            "Offset pagination uses SQL's LIMIT and OFFSET: SELECT * FROM posts ORDER BY id LIMIT 20 OFFSET 100 gets items 101-120. Simple to implement, maps directly to page numbers, allows jumping to any page. The drawbacks are significant though. Performance: OFFSET doesn't skip rows efficiently - the database scans through all offset rows, discarding them. OFFSET 1000000 scans a million rows to return 20. This gets slower linearly with offset size. Instability: if data changes between requests, you get inconsistencies. Delete an item from page 1? Everything shifts - page 2 items move to page 1, you skip something. Add an item? You might see duplicates. This breaks the user experience for real-time data. Use offset when: dataset is small and bounded, page numbers are required for UI, data doesn't change frequently. Avoid for: large tables, feeds with real-time updates, infinite scroll, public APIs. Most modern APIs use cursor-based pagination to avoid these problems."],
    },
    {
        text: "What is cursor-based pagination and when would you use it?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination],
        answers: ["Cursor-based pagination uses an opaque cursor token that points to a specific position in your dataset, usually based on a unique identifier or timestamp. Instead of saying 'give me page 3', you say 'give me 20 items after this cursor'. The cursor is typically an encoded or hashed value that contains the ID or timestamp of the last item from the previous page. The big advantages are consistent performance regardless of how deep you paginate, and it handles real-time data changes gracefully - you won't see duplicates or skip items. You'd use cursor-based pagination for infinite scroll interfaces, real-time feeds like social media timelines, or any large dataset where offset pagination would be too slow. It's also the standard for most modern APIs. The main limitation is you can't jump to arbitrary pages - you can only go forward or backward from where you are, which is fine for most use cases but not great if you need traditional page numbers. Implementation-wise, you typically include next and previous cursor tokens in your API response, and the client includes the cursor in the next request.",
            "Cursor-based pagination uses a pointer to a specific record rather than a numeric position. The cursor is typically an encoded value containing the last item's ID or sort key. Query: WHERE id > cursor ORDER BY id LIMIT 20. The response includes the new cursor for the next page. Advantages: constant performance - whether you're on page 1 or page 10000, the query uses an index seek, not a scan. Stability - even if items are added or deleted, you continue from where you left off without duplicates or gaps. Efficient for infinite scroll and real-time data. Limitations: can't jump to arbitrary page numbers. You navigate forward/backward from your current position. Implementation: base64-encode the cursor value to make it opaque. Include hasNextPage/hasPreviousPage and cursors in responses. Validate and decode cursors on the server. Use for: infinite scroll, feeds, timelines, any large or real-time dataset. It's the modern standard for APIs - Twitter, GitHub, Stripe all use cursor-based pagination."],
    },
    {
        text: "How do you implement infinite scroll?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination],
        answers: ["Infinite scroll is best implemented with cursor-based pagination on the backend. You start by loading an initial batch of items, and with the response you include a cursor that points to the last item. On the frontend, you detect when the user is approaching the bottom of the page using either scroll event listeners or, better yet, the Intersection Observer API which is more performant. When the user gets close to the bottom, you make another API request including the cursor from the previous response to get the next batch of items. The server uses that cursor to fetch items after that point and returns them along with a new cursor. You append the new items to your existing list and update the cursor. You also need to handle loading states, show a spinner while fetching, and handle the case when there are no more items to load. On the backend, make sure you're using indexed columns for your cursor to keep queries fast. Some nice touches are prefetching the next page before the user reaches the bottom, and implementing a 'back to top' button once they've scrolled far enough. Libraries like React Query or TanStack Query make this easier by handling the caching and state management for you.",
            "Backend: use cursor-based pagination. Return items with a cursor and hasMore flag. API returns { items: [...], nextCursor: 'abc123', hasMore: true }. The cursor encodes the last item's position. Frontend: detect when user approaches bottom. Intersection Observer is best - place a sentinel element at the bottom and observe when it enters viewport. When triggered, fetch next page with the cursor. Append items to existing list, update cursor. State management: track items array, current cursor, loading state, hasMore flag. Show spinner during load. When hasMore is false, stop requesting. Libraries help: React Query's useInfiniteQuery handles pagination state, caching, and deduplication. TanStack Virtual handles rendering thousands of items efficiently by only mounting visible ones. Performance tips: prefetch next page slightly before user reaches bottom. Use skeleton loaders instead of spinners. Implement 'back to top' button. Virtualize long lists to avoid DOM overload. Handle errors with retry logic."],
    },
    {
        text: "How do you handle pagination in GraphQL?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination, ValidTag.enum.graphql],
        answers: ["GraphQL has a well-established pattern for pagination called Connections, which comes from the Relay specification. The basic idea is that instead of returning a list directly, you return a connection object that contains edges and pageInfo. Each edge wraps a node with a cursor, and pageInfo tells you whether there are more pages and provides cursors for the next and previous pages. So your schema might look like: users(first: 10, after: cursor) returns a UserConnection with edges, where each edge has a cursor and node. This gives you cursor-based pagination out of the box. You can query 'first: 10' to get the first 10 items, then use the endCursor from pageInfo to get the next batch with 'first: 10, after: endCursor'. The pattern also supports backward pagination with 'last' and 'before'. While the schema looks verbose, it's very powerful and flexible. Most GraphQL libraries have utilities to implement connections easily. You can also implement simpler offset-based pagination if you want, just accepting 'limit' and 'offset' arguments, though cursor-based is generally preferred for the same performance and consistency reasons as with REST APIs.",
            "The Relay Connection specification is the standard for GraphQL pagination. Instead of returning [User], you return a UserConnection with structure: { edges: [{ node: User, cursor: String }], pageInfo: { hasNextPage, hasPreviousPage, startCursor, endCursor } }. Arguments: first/after for forward pagination, last/before for backward. Query: users(first: 20, after: 'cursor123') returns the next 20 users after that cursor. Each edge has its own cursor for maximum flexibility. pageInfo.endCursor and hasNextPage tell you how to fetch the next page. This pattern is verbose but powerful: consistent across all paginated fields, supports bidirectional navigation, each item carries its own cursor. Most GraphQL libraries have helpers: graphql-relay-js provides connectionFromArray and connectionDefinitions. Apollo Server has similar utilities. Alternatively, implement simpler pagination with just limit/offset arguments if Connections is overkill for your use case. But Connections is the community standard and clients like Relay expect it."],
    },
    {
        text: "What are the performance implications of different pagination strategies?",
        level: Level.enum["mid-advanced"],
        category: Category.enum.backend,
        tags: [ValidTag.enum.pagination, ValidTag.enum.performance],
        answers: ["The performance differences can be huge, especially at scale. Offset pagination degrades linearly with the offset size because the database has to scan through all the skipped rows even though it doesn't return them. So OFFSET 10000 LIMIT 20 might scan 10,020 rows to return 20. This gets really slow on large tables. Cursor-based and keyset pagination, on the other hand, maintain constant performance regardless of how deep you paginate because they use WHERE clauses on indexed columns - the database can jump directly to the right position using the index. The query is essentially the same whether you're getting items 1-20 or items 10,001-10,020. The key is that your cursor column needs to be indexed. In terms of database load, offset pagination also tends to produce more variable query plans which can be harder for the database to optimize. Memory usage is similar across strategies for the returned results, but offset can cause more disk I/O. For large datasets - think millions of rows - the difference between offset and cursor-based can be orders of magnitude. That's why social media feeds, which might have billions of posts, always use cursor-based pagination.",
            "Performance differs dramatically at scale. Offset: OFFSET N forces the database to scan N rows before returning results. OFFSET 1000000 scans a million rows. This is O(N) where N is the offset - linear degradation. Deep pagination becomes unusably slow. Cursor/Keyset: WHERE id > cursor uses an index seek. The database jumps directly to the right position. O(log N) lookup regardless of position. Page 1 and page 10000 have identical performance. The requirement: cursor column must be indexed. Composite cursors like (created_at, id) need a composite index. Real numbers: on a table with 10 million rows, offset pagination for page 50000 might take 5+ seconds. Cursor-based takes 5ms. In practice, offset pagination is fine for small datasets (under 10k rows) where performance isn't noticeable. For anything large, real-time, or public-facing, cursor-based is essential. Database-side: cursor queries produce consistent execution plans, easier to cache. Offset queries may have variable plans depending on offset size. Always analyze your specific queries with EXPLAIN."],
    },
];
