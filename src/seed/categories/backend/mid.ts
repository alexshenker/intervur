import { Category, Level, ValidTag } from "../../../db";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.backend,
    typeof Level.enum.mid
>[] = [
    // Node.js
    {
        text: "What is Node.js and how does its event-driven architecture work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-driven"]],
        answers: ["Node.js is a JavaScript runtime built on Chrome's V8 engine that lets us run JavaScript on the server side. What makes it really powerful is its event-driven, non-blocking I/O model. Basically, instead of waiting for operations like file reads or database queries to complete, Node.js uses callbacks and the event loop to handle multiple operations concurrently. When you make an async call, Node.js registers a callback and moves on to the next task. When the operation completes, it emits an event that triggers the callback. This architecture makes Node.js incredibly efficient for I/O-heavy applications because a single thread can handle thousands of concurrent connections without blocking."],
    },
    {
        text: "What is the difference between process.nextTick() and setImmediate()?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-loop"]],
        answers: ["This is a subtle but important distinction in the Node.js event loop. process.nextTick() callbacks are executed immediately after the current operation completes, before the event loop continues. They're processed before any I/O events or timers. On the other hand, setImmediate() is designed to execute a callback on the next iteration of the event loop, after I/O events. In practice, if you want something to run as soon as possible but after the current code completes, use process.nextTick(). If you want to yield to the event loop and let I/O operations process first, use setImmediate(). I generally prefer setImmediate() in most cases because excessive use of process.nextTick() can starve the event loop."],
    },
    {
        text: "What are streams in Node.js and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.streams],
        answers: ["Streams are one of the most powerful features in Node.js. They let you process data piece by piece instead of loading everything into memory at once. There are four types: readable, writable, duplex, and transform streams. I use streams whenever I'm dealing with large files or data sets. For example, if you're reading a 2GB log file, using fs.createReadStream() lets you process it chunk by chunk rather than loading the entire file into memory. You can also pipe streams together, which is really elegant for things like reading a file, compressing it, and writing it to another location. Streams are essential for building scalable applications because they keep memory usage constant regardless of data size."],
    },
    {
        text: "What is the difference between spawning a child process and using worker threads?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["child-processes"]],
        answers: ["Child processes and worker threads solve different problems. Child processes are completely separate instances of the V8 engine with their own memory space. They're great when you need to run external programs or isolate potentially unstable code. The downside is they have higher overhead and communication happens through IPC. Worker threads, on the other hand, run in the same process but in separate threads, sharing memory. They're perfect for CPU-intensive JavaScript operations like image processing or complex calculations. I use worker threads when I need to parallelize JavaScript code without blocking the main thread, and child processes when I need complete isolation or to run non-Node programs."],
    },
    {
        text: "How do you handle uncaught exceptions in Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["error-handling"]],
        answers: ["Handling uncaught exceptions is critical for application stability. I always set up a process-level handler using process.on('uncaughtException') to log the error and gracefully shut down the application. The key thing to understand is that once an uncaught exception occurs, your application is in an undefined state and you should not continue running. I also use process.on('unhandledRejection') for promises that reject without a catch handler. In production, I use a process manager like PM2 that automatically restarts the application after a crash. The best approach though is preventing uncaught exceptions in the first place by proper error handling with try-catch blocks, promise error handling, and wrapping async operations appropriately."],
    },
    {
        text: "What is backpressure and how do you handle it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.streams],
        answers: ["Backpressure happens when data is being produced faster than it can be consumed. It's a really common issue with streams. For example, if you're reading from a fast source and writing to a slow destination, the data can pile up in memory. Node.js streams have built-in backpressure handling. When you call write() on a writable stream, it returns false if the internal buffer is full, signaling you to pause. The proper way to handle this is to stop reading until the stream emits a 'drain' event. If you use the pipe() method, this is all handled automatically, which is why I prefer using pipe() when possible. The key is respecting these signals to prevent memory issues and ensure smooth data flow."],
    },
    {
        text: "What are buffers in Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.buffers],
        answers: ["Buffers are Node.js's way of handling binary data. Before we had typed arrays in JavaScript, buffers were the only way to work with raw binary data. They're fixed-size chunks of memory allocated outside the V8 heap. You use buffers when working with streams, file systems, or network protocols - basically anything that deals with binary data. For example, when reading an image file or handling TCP sockets, you're working with buffers. You can convert between buffers and strings using different encodings like UTF-8, Base64, or hex. One thing to watch out for is that buffers are mutable and have a fixed size, so you need to allocate the right amount of memory upfront."],
    },
    {
        text: "What is the difference between require and import?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.modules],
        answers: ["require is the CommonJS module system that Node.js originally used, while import is the ES6 module syntax. The main differences are that require is synchronous and happens at runtime, while import is asynchronous and happens at parse time. This means with import you can do static analysis and tree-shaking. require gives you the entire module at once, whereas import lets you selectively import just what you need. In modern Node.js, you can use ES modules by setting type: 'module' in your package.json or using .mjs extensions. I tend to prefer ES modules for new projects because they're the standard and offer better tooling support, but require is still widely used and perfectly fine, especially in existing codebases."],
    },
    {
        text: "How does the module resolution algorithm work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.modules],
        answers: ["When you require a module, Node.js follows a specific algorithm to find it. First, it checks if it's a core module like 'fs' or 'http' - those are loaded directly. If it starts with './' or '../', it treats it as a file or directory relative to the current file. For file paths, it tries adding .js, .json, and .node extensions. For directories, it looks for package.json and loads the file specified in the 'main' field, or defaults to index.js. If the module doesn't start with a path indicator, Node.js treats it as a package and searches up the directory tree in node_modules folders. This search continues all the way to the root. Understanding this is helpful for debugging module not found errors and organizing your project structure."],
    },
    {
        text: "What is the cluster module and how do you scale Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.cluster, ValidTag.enum.scalability],
        answers: ["The cluster module lets you spawn multiple Node.js processes to take advantage of multi-core systems. Since Node.js is single-threaded, one process can only use one CPU core. With clustering, you create a master process that forks worker processes, typically one per CPU core. All workers can share the same server port, and the master distributes incoming connections among them. I usually use PM2 in production, which handles clustering automatically and adds features like automatic restarts and monitoring. For scaling beyond a single machine, you'd use horizontal scaling with load balancers and multiple servers. The key is understanding that Node.js scales through multiple processes, not threads, which is different from traditional server platforms."],
    },
    {
        text: "What are environment variables and how do you manage them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs],
        answers: ["Environment variables are key-value pairs that configure your application differently across environments like development, staging, and production. In Node.js, you access them through process.env. I typically use the dotenv package for local development, which loads variables from a .env file. For production, I set them through the hosting platform or container orchestration system. The key principle is never committing sensitive data like API keys or database passwords to version control. I usually have a .env.example file in the repo with placeholder values so other developers know what variables are needed. For complex configurations, I sometimes create a config module that validates and provides type-safe access to environment variables."],
    },
    {
        text: "What is the event emitter pattern?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-emitter"]],
        answers: ["The EventEmitter is a core pattern in Node.js for handling asynchronous events. It's basically a pub-sub system where objects can emit named events and listeners can subscribe to them. Most of Node.js core APIs use this pattern - streams, HTTP servers, and more all extend EventEmitter. You can create your own by extending the EventEmitter class. For example, you might emit a 'data' event when new data arrives and have multiple listeners react to it. I use this pattern when I need loose coupling between components or when one action needs to trigger multiple side effects. The key methods are on() or addListener() to subscribe, emit() to trigger events, and once() for one-time listeners. It's a powerful way to build event-driven architectures."],
    },
    {
        text: "What is libuv and how does it relate to Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-loop"]],
        answers: ["libuv is the C library that provides Node.js with the event loop and handles all the async I/O operations. It's what makes Node.js cross-platform, abstracting away the differences between Windows, Linux, and macOS. libuv manages the thread pool that handles file system operations and other blocking tasks that can't be done asynchronously at the OS level. When you do an async file read, libuv offloads that to the thread pool. It also handles TCP and UDP sockets, child processes, and timers. Understanding libuv helps you grasp why certain operations in Node.js are truly async while others are simulated async using the thread pool. It's the foundation that makes the whole non-blocking I/O model work."],
    },

    // Express
    {
        text: "How do you handle errors in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum["error-handling"]],
        answers: ["Express has a specific pattern for error handling. You define error-handling middleware with four parameters - err, req, res, and next - and place it after all your other middleware and routes. When you call next() with an argument, Express skips to the error handlers. For async route handlers, I wrap them in a try-catch and pass errors to next(), or use a wrapper function that catches promise rejections automatically. In the error handler, I check the error type and send appropriate HTTP status codes and messages. I typically have different error classes for different scenarios like ValidationError or NotFoundError. In production, I'm careful not to expose stack traces or sensitive details. Having a centralized error handler makes your error responses consistent across the entire application."],
    },
    {
        text: "How do you structure a large Express application?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["For large Express apps, I follow a layered architecture pattern. I separate concerns into routes, controllers, services, and models or repositories. Routes define the endpoints and use Express routers to modularize by feature or resource. Controllers handle the HTTP layer - parsing requests and sending responses. Services contain the business logic and are framework-agnostic, making them easier to test and reuse. I also create separate directories for middleware, utilities, and config. A typical structure might have folders like routes/, controllers/, services/, models/, middleware/, and utils/. I use dependency injection where possible to make testing easier. The key is keeping each layer focused on its responsibility and avoiding mixing business logic with HTTP concerns."],
    },
    {
        text: "How do you implement rate limiting in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum["rate-limiting"]],
        answers: ["I typically use the express-rate-limit middleware for basic rate limiting. You configure it with a time window and maximum number of requests, and it tracks requests by IP address. For production apps, I often use Redis as the store instead of in-memory storage, especially when running multiple server instances. This ensures rate limits work correctly across all servers. You can apply rate limiting globally or to specific routes. I usually have stricter limits on authentication endpoints to prevent brute force attacks and more lenient limits on regular API endpoints. For more sophisticated needs, I might implement token bucket or sliding window algorithms. The key is finding the right balance between protecting your resources and not frustrating legitimate users."],
    },
    {
        text: "What are routers in Express and how do you modularize routes?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["Express routers are mini-applications that handle routing for a specific part of your app. Instead of defining all routes on the main app object, you create router instances for different features or resources. For example, I might have separate routers for users, posts, and comments. Each router is defined in its own file, and you mount them on the main app with a prefix like app.use('/api/users', userRouter). This keeps your code organized and makes it easier to manage as the application grows. Routers can also have their own middleware that only applies to their routes. I often organize my routes folder by resource, with each file exporting a router. This modular approach makes the codebase much more maintainable and testable."],
    },
    {
        text: "How do you handle file uploads in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["For file uploads, I use middleware like multer, which handles multipart/form-data. You configure it with options like destination, file size limits, and file name handling. I typically validate the file type and size on both the client and server. For small applications, I might store files on the local filesystem, but for production, I usually upload directly to cloud storage like S3 or Google Cloud Storage. With multer, you can process the file in memory and stream it to cloud storage, which is more efficient than writing to disk first. I always implement proper error handling for cases like oversized files or invalid file types, and I'm careful about security - never trust the client-provided filename and always validate file content, not just the extension."],
    },
    {
        text: "What is CORS middleware and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum.cors],
        answers: ["CORS middleware handles Cross-Origin Resource Sharing, which is the browser security feature that blocks requests from different origins. In Express, I use the cors package. For development, I might allow all origins, but in production, I configure it to only allow specific origins. You can set allowed methods, headers, and whether to allow credentials. I typically configure it with an object specifying origin, methods, and credentials. For APIs with multiple frontend clients, I maintain a whitelist of allowed origins. You can also configure CORS per-route rather than globally if different endpoints have different requirements. Understanding CORS is crucial because misconfiguration is a common source of bugs, and being too permissive can create security vulnerabilities."],
    },

    // REST APIs
    {
        text: "How do you version APIs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["api-versioning"]],
        answers: ["There are a few common approaches to API versioning. The most popular is URL versioning, like /api/v1/users and /api/v2/users. It's explicit and easy to understand. Another approach is using headers, either a custom version header or the Accept header with content negotiation. Some teams use query parameters like /api/users?version=2. I personally prefer URL versioning because it's the most visible and makes it easy to route to different implementations. The key is to have a versioning strategy from the start and maintain backward compatibility as long as possible. When you do introduce a new version, clearly document the changes and give clients plenty of time to migrate. I also try to avoid versioning too frequently - only for breaking changes, not new features."],
    },
    {
        text: "What is HATEOAS and when is it useful?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"]],
        answers: ["HATEOAS stands for Hypermedia as the Engine of Application State. It's a principle of REST where the server includes links to related resources in its responses, so clients can discover available actions dynamically rather than having URLs hardcoded. For example, a user resource might include links to update, delete, or view related resources. The idea is that the API becomes self-documenting and more flexible. Honestly, full HATEOAS is pretty rare in practice because it adds complexity and most modern APIs work fine with documentation and conventions. I've seen it used in enterprise systems where the API needs to be highly discoverable and evolvable. For most projects, including some basic links to related resources is a good middle ground without going full HATEOAS."],
    },
    {
        text: "How do you design error responses?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["error-handling"]],
        answers: ["Good error responses should be consistent and informative. I always use appropriate HTTP status codes - 400 for client errors, 500 for server errors, and so on. The response body typically includes an error code or type, a human-readable message, and sometimes additional details. For validation errors, I include which fields failed and why. I follow a consistent structure across all endpoints, something like { error: { code: 'VALIDATION_ERROR', message: 'Invalid input', details: [...] } }. In production, I'm careful not to expose sensitive information or stack traces. For client developers, I might include a correlation ID they can reference when contacting support. The key is making errors actionable - the client should understand what went wrong and how to fix it."],
    },
    {
        text: "How do you handle pagination and what are the tradeoffs between offset and cursor-based?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum.pagination],
        answers: ["Offset-based pagination uses page numbers and limits, like ?page=2&limit=20. It's simple and lets users jump to any page, but it has issues with data consistency if items are added or deleted while paginating. Cursor-based pagination uses a pointer to a specific item, usually an ID or timestamp. It's more stable and efficient for large datasets because it uses indexed lookups, but you can't jump to arbitrary pages. I use offset for smaller datasets where users need random access, like admin dashboards. For feeds or large datasets, I prefer cursor-based pagination. I also always include metadata in the response like total count, next cursor, or has more pages, so clients can build proper UI. The key is matching the pagination strategy to your use case."],
    },
    {
        text: "What is rate limiting and how would you implement it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["rate-limiting"]],
        answers: ["Rate limiting restricts how many requests a client can make in a given time period. It protects your API from abuse and ensures fair usage. The simplest approach is a fixed window - count requests per IP in a time window, say 100 requests per hour. I typically implement this with Redis, storing a counter with an expiration. For more sophisticated needs, I use sliding window or token bucket algorithms, which provide smoother rate limiting. I include rate limit headers in responses so clients know their limits and remaining quota. Different endpoints might have different limits - stricter for expensive operations or auth endpoints. For authenticated APIs, I rate limit by user rather than IP. The implementation depends on your scale - simple in-memory storage works for single servers, but distributed systems need Redis or similar."],
    },
    {
        text: "How do you document APIs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["api-design"]],
        answers: ["I use OpenAPI (formerly Swagger) specification for API documentation. You can either write the spec by hand or generate it from code annotations. I prefer tools like swagger-jsdoc that let me document endpoints with comments, keeping docs close to the code. This generates interactive documentation where developers can try endpoints directly in the browser. I make sure to document all endpoints, parameters, request bodies, response formats, and error codes. For more complex APIs, I supplement with written guides covering authentication flows, common use cases, and best practices. The key is keeping documentation in sync with the code - I often automate this with CI checks that fail if the spec doesn't match the implementation. Good documentation significantly reduces support burden and helps developers integrate faster."],
    },
    {
        text: "How do you handle filtering and sorting?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"]],
        answers: ["For filtering, I use query parameters that map to the fields being filtered, like ?status=active&category=tech. For complex filters, I might support operators like ?price[gte]=100&price[lte]=500. Sorting typically uses a sort parameter with the field name, like ?sort=createdAt, and a minus sign for descending order, like ?sort=-createdAt. I validate all filter and sort parameters against a whitelist to prevent injection attacks and ensure users can only filter on indexed fields for performance. On the backend, I parse these parameters and build database queries safely. I'm careful to handle edge cases like invalid field names or malformed values. The API should return clear errors if someone tries to filter or sort on unsupported fields. This approach is intuitive and scales well."],
    },
    {
        text: "What is content negotiation?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"]],
        answers: ["Content negotiation is how clients and servers agree on the format of data being exchanged. The client uses the Accept header to specify what formats it can handle, like application/json or application/xml. The server responds with the appropriate format and sets the Content-Type header. In practice, most modern APIs just use JSON, but content negotiation is useful if you need to support multiple formats. For example, an API might return JSON for web clients and XML for legacy systems. You can also use it for versioning by having custom media types like application/vnd.myapi.v2+json. Express has built-in support with res.format(). While it's a nice REST principle, I find it's overkill for most APIs where JSON is the standard."],
    },
    {
        text: "What are ETags and conditional requests?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum.etag, ValidTag.enum.caching],
        answers: ["ETags are identifiers for specific versions of a resource, usually a hash of the content. The server includes an ETag header in the response. The client can then make a conditional request with the If-None-Match header containing that ETag. If the resource hasn't changed, the server returns 304 Not Modified without the body, saving bandwidth. Similarly, If-Match headers are used for safe updates - the server only processes the request if the resource hasn't changed since the client last saw it, preventing lost updates. This is great for caching and concurrency control. I implement ETags by hashing response bodies or using database version fields. Express has middleware that can handle this automatically, though for APIs I often implement it manually for better control over what's cached."],
    },

    // GraphQL
    {
        text: "What is GraphQL and how does it differ from REST?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["rest-api"]],
        answers: ["GraphQL is a query language for APIs where clients specify exactly what data they need. Unlike REST where you have multiple endpoints, GraphQL typically has a single endpoint and clients send queries describing their data requirements. The big advantage is it solves over-fetching and under-fetching - clients get exactly what they need in one request instead of making multiple calls or getting excessive data. REST is resource-based with fixed endpoints, while GraphQL is schema-based with flexible queries. GraphQL is great for complex, interconnected data and when you have diverse clients with different needs. REST is simpler and works well for straightforward CRUD operations. I choose GraphQL when the frontend needs a lot of flexibility, and REST when simplicity and caching are priorities."],
    },
    {
        text: "What are queries, mutations, and subscriptions?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["These are the three main operation types in GraphQL. Queries are for reading data - they're like GET requests in REST. You specify what fields you want and GraphQL returns just that data. Mutations are for modifying data - creating, updating, or deleting. They're similar to POST, PUT, and DELETE in REST. Mutations explicitly signal that something is changing, unlike queries. Subscriptions are for real-time updates. The client subscribes to events and the server pushes updates when data changes. This is typically implemented with WebSockets. In my schema, I define separate root types for each: Query, Mutation, and Subscription. I use queries for all read operations, mutations for writes, and subscriptions when clients need live updates, like in chat applications or live dashboards."],
    },
    {
        text: "What is the GraphQL schema and type system?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["The GraphQL schema is a contract between the client and server that defines what data is available and how to request it. It's written in the GraphQL Schema Definition Language. You define object types with fields, like a User type with name, email, and posts fields. The type system is strongly typed with built-in scalars like String, Int, Boolean, and you can define custom scalars. Fields can be required with the ! operator or lists with brackets. The schema also defines the root Query, Mutation, and Subscription types that serve as entry points. This strong typing enables powerful developer tools like autocomplete and validation. I write the schema to model my domain, and it serves as living documentation that's always in sync with the implementation."],
    },
    {
        text: "How would you handle the n+1 problem in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["n-plus-one"], ValidTag.enum.performance],
        answers: ["The n+1 problem happens when you fetch a list and then make separate database queries for related data for each item. In GraphQL, this is really common because of how resolvers work. The solution is DataLoader, a batching and caching utility. Instead of making individual queries in each resolver, DataLoader batches them together and executes them in a single query. For example, if you're fetching users and their posts, DataLoader will collect all the user IDs that need posts and fetch them in one query. It also caches results within a single request. I create DataLoader instances per request and pass them through the context. This dramatically improves performance without changing your resolver structure. It's pretty much essential for any production GraphQL API."],
    },
    {
        text: "What are resolvers and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Resolvers are functions that return data for each field in your GraphQL schema. When a query comes in, GraphQL executes the resolver for each requested field. A resolver receives four arguments: parent (the result from the parent resolver), args (arguments passed to the field), context (shared data like the current user or database connection), and info (metadata about the query). For simple fields, resolvers can just return the property from the parent object. For complex fields, they might fetch from a database or call another service. Resolvers are executed in a specific order following the query structure. I organize my resolvers by type and keep them focused on data fetching, putting business logic in separate service layers. This separation makes testing easier and keeps resolvers clean."],
    },
    {
        text: "What is the DataLoader pattern?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.performance],
        answers: ["DataLoader is a pattern and library for batching and caching data fetches. It solves the n+1 query problem by collecting all the data requests made during a single tick of the event loop and batching them into one request. You give DataLoader a batch function that takes an array of keys and returns a promise of values in the same order. When resolvers call dataLoader.load(id), those calls are collected and the batch function is called once with all the IDs. DataLoader also provides per-request caching, so if the same ID is requested multiple times in one query, it only fetches once. I create new DataLoader instances per request and pass them through context. This is crucial for GraphQL performance and has become a standard pattern in the ecosystem."],
    },
    {
        text: "How do you handle authentication and authorization in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.auth, ValidTag.enum.authorization],
        answers: ["Authentication typically happens before the GraphQL layer. I verify the token in middleware and attach the authenticated user to the context. Then all resolvers can access the current user from context. For authorization, I check permissions within resolvers before returning data. Some people use directive-based authorization like @auth or @hasRole, which is declarative and clean. Others prefer explicit checks in resolver code. I usually combine both - directives for simple role checks and manual checks for complex business logic. Field-level authorization is important in GraphQL since clients can request any fields. I make sure to check permissions on individual fields, not just top-level queries. The key is never trusting the client and always validating access at the resolver level."],
    },
    {
        text: "What are fragments and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Fragments are reusable pieces of GraphQL queries. Instead of repeating the same field selections in multiple places, you define a fragment and reference it. For example, you might have a UserBasicInfo fragment with name and email fields that you use across different queries. Fragments are client-side constructs - they make your queries more maintainable but don't affect the server. There are also inline fragments for querying fields on specific types in a union or interface. I use fragments to keep queries DRY, especially in frontend code where the same data structure is needed in multiple components. In tools like Apollo Client, fragments can be colocated with components, which creates a nice component-driven data fetching pattern. They're essential for organizing complex GraphQL applications."],
    },
    {
        text: "What are directives in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Directives are a way to modify the execution of queries and schemas. The built-in ones are @include and @skip for conditional field inclusion, and @deprecated for marking fields as deprecated. You can also create custom directives for things like formatting, authorization, or caching. On the client side, directives control query execution. On the server, schema directives let you add behavior to your types and fields. For example, you might have a @auth directive that checks permissions or a @cacheControl directive that sets cache hints. I use them sparingly - they're powerful but can make the schema harder to understand if overused. Custom directives are great for cross-cutting concerns that would otherwise require a lot of repetitive code in resolvers."],
    },
    {
        text: "How do you handle errors in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["error-handling"]],
        answers: ["GraphQL has a unique approach to errors. Responses always return 200 OK with both data and errors fields. When a resolver throws an error, it's caught and added to the errors array, while other resolvers continue executing. This is different from REST where an error typically fails the whole request. I create custom error classes for different error types and format them in the GraphQL error formatter. You can add extensions to errors with additional context like error codes or validation details. For user-facing errors, I make sure the message is clear. For security, I avoid exposing stack traces or internal details in production. Some teams use union types for errors as part of the schema, treating errors as data, which gives more type safety on the client side."],
    },
    {
        text: "What is introspection and when should you disable it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.security],
        answers: ["Introspection is a feature that lets clients query the schema itself to discover available types, fields, and operations. This powers tools like GraphiQL and GraphQL Playground. Clients send special queries starting with __schema or __type to get schema information. It's incredibly useful for development and documentation, but it's a security consideration in production because it exposes your entire API structure to anyone. Many teams disable introspection in production to prevent attackers from easily mapping the API surface. However, this doesn't really provide security through obscurity since a determined attacker could still probe the API. I typically leave it enabled but rely on proper authentication, authorization, and rate limiting for actual security. If you do disable it, make sure your authenticated developers can still access it."],
    },
    {
        text: "What are input types vs output types?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["In GraphQL, output types are what you return from queries and mutations - things like object types, unions, and interfaces. Input types are what you pass as arguments to fields and mutations. They're defined with the input keyword and can only contain scalars, enums, and other input types - not object types. This separation exists because inputs and outputs serve different purposes and have different validation needs. For mutations, I create input types for complex arguments instead of having many scalar parameters. For example, CreateUserInput with name, email, and password fields. This makes mutations cleaner and easier to evolve. The naming convention is usually to suffix input types with Input. You can't use the same type for both input and output, which sometimes means duplicating similar structures."],
    },
    {
        text: "How do you implement pagination in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.pagination],
        answers: ["GraphQL has a standard pagination pattern called Relay cursor-based pagination. It uses a connection pattern with edges and nodes. Each edge has a cursor and the node, plus there's a pageInfo object with hasNextPage, hasPreviousPage, and start/end cursors. Clients pass first/after for forward pagination or last/before for backward pagination. This is more complex than simple offset pagination but handles edge cases better and is more performant. For simpler use cases, I sometimes just use offset and limit arguments, especially for internal APIs where the Relay pattern would be overkill. The connection pattern is great for infinite scrolling and bidirectional pagination. I implement it by encoding cursors from unique identifiers like IDs or timestamps, and using those for database queries."],
    },

    // WebSockets
    {
        text: "What are WebSockets and how do they differ from HTTP?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["WebSockets provide full-duplex, bidirectional communication over a single TCP connection. Unlike HTTP where the client makes a request and the server responds, WebSockets keep a persistent connection open so both sides can send messages at any time. HTTP is request-response and stateless - each request is independent. WebSockets are stateful and persistent, which makes them perfect for real-time applications like chat, live notifications, or collaborative editing. The overhead is much lower than HTTP for frequent messages since you don't have to re-establish connections or send headers with every message. However, WebSockets are more complex to implement and scale than HTTP. I use them when I need real-time bidirectional communication, but stick with HTTP for traditional request-response patterns."],
    },
    {
        text: "What is the WebSocket handshake?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["The WebSocket handshake is how the connection gets established. It starts as a regular HTTP request with an Upgrade header asking to switch to the WebSocket protocol. The request includes a Sec-WebSocket-Key header with a random value. If the server supports WebSockets, it responds with 101 Switching Protocols status and a Sec-WebSocket-Accept header that's computed from the client's key. This handshake confirms both sides speak the WebSocket protocol. After the handshake completes, the connection switches from HTTP to WebSocket protocol and remains open for bidirectional messages. The handshake being over HTTP is nice because it works through existing infrastructure like load balancers and proxies that understand HTTP, though you need to ensure they also support the protocol upgrade."],
    },
    {
        text: "What is Socket.io and how does it differ from native WebSockets?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Socket.io is a library that provides real-time communication with fallbacks and extra features. While it uses WebSockets when available, it can fall back to HTTP long-polling if WebSockets aren't supported. Socket.io adds features like automatic reconnection, rooms and namespaces for organizing connections, event-based messaging instead of raw data frames, and broadcasting to multiple clients easily. Native WebSockets are lower level - you send and receive raw messages and have to implement reconnection and other features yourself. Socket.io is easier to use and more reliable across different environments, but it's heavier and requires Socket.io on both client and server. I use Socket.io for complex real-time apps where I want those extra features, and native WebSockets for simpler use cases or when I need to integrate with non-Socket.io clients."],
    },
    {
        text: "How do you handle WebSocket authentication?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets, ValidTag.enum.auth],
        answers: ["WebSocket authentication is trickier than HTTP because you can't easily send headers after the initial handshake. I typically authenticate during the handshake by passing a token as a query parameter or in the initial HTTP headers. Once connected, I verify the token and store the user identity with the connection. Some people send an authentication message immediately after connecting instead. For session-based auth, cookies work since they're sent with the handshake. I always validate tokens on the server and handle expired tokens by closing the connection. With Socket.io, I use middleware to authenticate during the handshake. It's important to re-validate periodically for long-lived connections and handle token refresh gracefully. The key is ensuring only authenticated users can establish and maintain connections."],
    },
    {
        text: "What are rooms and namespaces in Socket.io?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Namespaces and rooms are Socket.io features for organizing connections. Namespaces are separate communication channels with their own event handlers, like different apps sharing the same Socket.io server. You might have /chat and /notifications namespaces. Clients connect to specific namespaces. Rooms are groups within a namespace that you can broadcast to. For example, in a chat app, each conversation might be a room. Sockets can join and leave rooms dynamically. When you emit to a room, only sockets in that room receive it. I use namespaces to separate different features or applications, and rooms to group related clients within a feature. For example, a multiplayer game might have a /game namespace with a room for each match. This makes it easy to send messages to specific groups without manually tracking connections."],
    },
    {
        text: "What is the difference between WebSockets and Server-Sent Events?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Server-Sent Events (SSE) are a simpler alternative to WebSockets for server-to-client streaming. SSE is unidirectional - only the server can push data to the client. It uses regular HTTP, so it's easier to implement and works through proxies and firewalls better. SSE has automatic reconnection built-in and sends events as text. WebSockets are bidirectional and can send binary data, but are more complex. I use SSE for simple cases where I just need to push updates from server to client, like live news feeds or stock tickers. WebSockets are better when you need bidirectional communication, like chat or real-time collaboration. SSE is often overlooked but it's perfect for scenarios where you don't need full WebSocket complexity. The API is simpler and it's more reliable over unreliable networks."],
    },
    {
        text: "How do you handle reconnection and connection state?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Managing connection state is crucial for reliable WebSocket applications. On the client side, I implement automatic reconnection with exponential backoff - if the connection drops, wait a bit before retrying, and increase the wait time with each failed attempt. I track connection state (connecting, connected, disconnected) and show this to users. On the server, I clean up resources when connections close. For reliability, I often implement message acknowledgments so you know if messages were received. Socket.io handles a lot of this automatically. I also implement heartbeats to detect dead connections. When clients reconnect, they need to restore their state - rejoin rooms, resync data, etc. For message ordering, I sometimes add sequence numbers. The challenge is making reconnection seamless from a user perspective while handling all the edge cases on the technical side."],
    },
    {
        text: "What are heartbeats and ping/pong frames?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Heartbeats are a mechanism to keep connections alive and detect if they've died. The WebSocket protocol includes ping and pong frames for this purpose. One side sends a ping frame, and the other responds with a pong. If you don't get a pong back within a timeout, you know the connection is dead and can close it. This is important because connections can silently die due to network issues, and you might not know otherwise. TCP keepalive exists but operates at a lower level with longer timeouts. I typically implement application-level heartbeats every 30-60 seconds. On the server, if a client doesn't respond, I close the connection and clean up resources. This prevents zombie connections from accumulating. Many WebSocket libraries handle this automatically, but it's important to configure the intervals appropriately for your use case."],
    },

    // NestJS
    {
        text: "What is NestJS and how does it differ from Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.express],
        answers: ["NestJS is a framework built on top of Express (or optionally Fastify) that adds structure and features inspired by Angular. While Express is minimalist and unopinionated, NestJS is opinionated and provides a complete architecture out of the box. It uses TypeScript by default and leverages decorators for routing, dependency injection, and metadata. NestJS enforces a modular structure with controllers, services, and modules, whereas Express leaves architecture up to you. It includes built-in support for things like validation, serialization, WebSockets, and microservices. I use Express for simple APIs or when I want maximum flexibility, and NestJS for larger applications where the structure and built-in features save time. The learning curve is steeper with NestJS, but it scales better for complex applications with large teams."],
    },
    {
        text: "What are modules, controllers, and providers in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["These are the core building blocks of NestJS. Modules organize the application into cohesive units - each module groups related controllers and providers. You have a root module and can have feature modules for different parts of your app. Controllers handle incoming requests and return responses - they define your API endpoints using decorators like @Get() and @Post(). Providers are anything that can be injected as a dependency, typically services that contain business logic. You mark them with @Injectable(). The module decorator declares which controllers and providers belong to that module and which ones are exported for other modules to use. This structure enforces separation of concerns - controllers handle HTTP, services handle business logic, and modules organize everything. It makes the application very testable and maintainable."],
    },
    {
        text: "How does dependency injection work in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum["dependency-injection"]],
        answers: ["NestJS has a powerful dependency injection system. You mark classes with @Injectable() and register them as providers in a module. Then you can inject them into constructors of other classes, and NestJS automatically instantiates and provides them. The DI container manages the lifecycle of these instances. By default, providers are singletons scoped to the application, but you can also have request-scoped or transient providers. This makes testing easy because you can inject mock dependencies. It also promotes loose coupling - classes depend on interfaces rather than concrete implementations. I use custom providers when I need more control, like factory providers or using existing instances. The DI system is one of NestJS's best features, making the code more modular, testable, and maintainable than manually managing dependencies."],
    },
    {
        text: "What are guards and how do you implement authorization?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.guards, ValidTag.enum.authorization],
        answers: ["Guards are NestJS classes that determine whether a request should be handled by the route handler. They implement the CanActivate interface and return true or false. I use guards for authentication and authorization. For example, an AuthGuard checks if the user is logged in, and a RolesGuard checks if they have the required role. Guards execute before interceptors and pipes, making them perfect for access control. You can apply them globally, to controllers, or to individual routes using the @UseGuards() decorator. I often combine guards with custom decorators to extract user information from the request. For role-based access, I create a @Roles() decorator and a guard that reads it using reflection. Guards are cleaner than middleware for authorization because they have access to the execution context and fit into NestJS's architecture."],
    },
    {
        text: "What are interceptors and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.interceptors],
        answers: ["Interceptors let you transform the result or add extra logic before or after method execution. They implement the NestInterceptor interface and use RxJS operators since NestJS uses observables. I use interceptors for things like response transformation, logging, caching, and adding extra headers. For example, a logging interceptor can measure request duration, or a transform interceptor can wrap responses in a consistent format. You can bind interceptors globally, to controllers, or to specific routes. They're powerful for cross-cutting concerns that affect multiple routes. The ability to transform responses using RxJS operators like map is really useful. I've used interceptors to automatically serialize responses, add pagination metadata, and implement timeout logic. They're similar to middleware but operate at a higher level with access to the execution context and return values."],
    },
    {
        text: "What are pipes and how do you handle validation?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.pipes, ValidTag.enum.validation],
        answers: ["Pipes transform or validate input data before it reaches the route handler. NestJS provides built-in pipes like ValidationPipe and ParseIntPipe. The ValidationPipe works with class-validator decorators on DTOs - you define your data transfer objects with validation decorators like @IsString() or @IsEmail(), and the pipe automatically validates incoming data. If validation fails, it throws an error with details. I use the global ValidationPipe in most applications because it ensures all inputs are validated. You can also create custom pipes for specific transformations. Pipes can be applied globally, to controllers, to routes, or to specific parameters. For example, ParseIntPipe transforms a string parameter to a number. This declarative approach to validation is much cleaner than manual validation in route handlers."],
    },
    {
        text: "What are exception filters?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum["error-handling"]],
        answers: ["Exception filters handle errors thrown anywhere in the application. NestJS has a built-in global exception filter that handles standard HTTP exceptions, but you can create custom filters for specific error handling. They implement the ExceptionFilter interface and catch exceptions, allowing you to transform them into appropriate HTTP responses. I create custom exception filters for domain-specific errors or to format error responses consistently. For example, a database exception filter might catch database errors and return user-friendly messages. You can catch all exceptions or specific types. Filters can be global, controller-scoped, or method-scoped. They're the last line of defense in the request lifecycle, ensuring even unexpected errors are handled gracefully. I typically have a global filter for general errors and specific filters for things like validation errors or database errors."],
    },
    {
        text: "How do you handle configuration in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["NestJS has a dedicated @nestjs/config module built on dotenv. I import ConfigModule globally in the root module, usually with ConfigModule.forRoot(). It loads environment variables from .env files and makes them available through ConfigService, which you can inject anywhere. I create configuration namespaces for different parts of the app like database, auth, etc., using configuration factories. These return typed configuration objects, providing type safety. ConfigService has a get method to retrieve values, and you can provide default values. I validate configuration on startup using Joi or class-validator to fail fast if required variables are missing. For different environments, I use different .env files like .env.development and .env.production. This approach is much better than accessing process.env directly because it's testable, typed, and centralized."],
    },
    {
        text: "What is the difference between @Injectable() and @Controller()?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["These are different types of decorators that serve different purposes. @Injectable() marks a class as a provider that can be managed by NestJS's dependency injection container. You use it on services, repositories, or any class you want to inject elsewhere. Controllers use @Controller(), which marks the class as handling HTTP requests and binds it to a route. Controllers define your API endpoints using route decorators like @Get() and @Post(). Under the hood, controllers are also injectable, which is why you can inject services into them. But conceptually, controllers handle the HTTP layer while injectable services handle business logic. You declare controllers in the module's controllers array and providers in the providers array. The separation keeps concerns clear - controllers focus on request/response handling, and services focus on application logic."],
    },
    {
        text: "How do you implement WebSockets in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.websockets],
        answers: ["NestJS has great WebSocket support through the @nestjs/websockets and @nestjs/platform-socket.io packages. You create a gateway class decorated with @WebSocketGateway(), which is similar to a controller but for WebSocket events. Inside the gateway, you use @SubscribeMessage() decorators to handle specific events, just like route handlers. You can inject services into gateways and use all the NestJS features like guards and pipes. For Socket.io features, you use decorators like @ConnectedSocket() and @MessageBody() to access the socket and message data. Gateways also have lifecycle hooks like afterInit, handleConnection, and handleDisconnect. I typically create separate gateways for different namespaces or features. The pattern is very similar to HTTP controllers, making it intuitive if you already know NestJS."],
    },
    {
        text: "How do you structure a large NestJS application?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["I organize NestJS apps by feature modules rather than by layer. Each feature gets its own directory with its module, controllers, services, DTOs, and entities. For example, a users feature would have users.module.ts, users.controller.ts, users.service.ts, etc. Shared code goes in a common or shared module. I keep core infrastructure like database connections and configuration in a core module. Domain logic lives in services within feature modules, keeping controllers thin. For large apps, I might have sub-modules within features. I use barrel exports (index.ts files) to simplify imports. Database entities or schemas go with their feature. This structure scales well because features are isolated and teams can work independently on different modules. I also separate DTOs from entities to keep API contracts separate from database models."],
    },
    {
        text: "What are custom decorators and how do you create them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.decorators],
        answers: ["Custom decorators let you extract common patterns and make your code more declarative. You create them using the createParamDecorator function from @nestjs/common. For example, I often create a @CurrentUser() decorator that extracts the authenticated user from the request, so controllers can just use @CurrentUser() user: User instead of digging into the request object. You can also create metadata decorators using SetMetadata for things like @Roles(['admin']) that guards can read using Reflector. Decorators can be composed - you can combine multiple decorators into one using applyDecorators. I use custom decorators to reduce boilerplate, improve readability, and encapsulate common request data extraction. They make your route handlers cleaner and more focused on business logic rather than request manipulation."],
    },

    // Serverless
    {
        text: "What is serverless architecture and what are its tradeoffs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless means you write functions that run in managed compute environments without managing servers yourself. You're billed only for actual execution time, not idle time. The main benefits are automatic scaling, no server maintenance, and cost efficiency for variable workloads. The tradeoffs are cold starts causing latency, execution time limits, vendor lock-in, and statelessness requiring external storage for any state. Debugging and local development are harder. For applications with unpredictable traffic or event-driven workloads, serverless is great. For consistent high-traffic applications, traditional servers might be more cost-effective. I use serverless for APIs with sporadic traffic, background jobs, and webhooks. It's not great for long-running processes or latency-sensitive applications. The key is matching the architecture to your traffic patterns and requirements."],
    },
    {
        text: "What is cold start and how do you mitigate it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless, ValidTag.enum.lambda, ValidTag.enum.performance],
        answers: ["Cold start is the latency when a serverless function is invoked for the first time or after being idle. The platform needs to spin up a new container, load your code, and initialize it. This can add hundreds of milliseconds or more to the first request. To mitigate it, I keep functions small and minimize dependencies. Using compiled languages like Go instead of interpreted ones like Python helps. Provisioned concurrency keeps functions warm, though it costs more. I reuse connections by initializing them outside the handler function. For critical paths, scheduled pings can keep functions warm. Some platforms now have better cold start performance. I also design around it - using serverless for non-latency-critical paths and keeping time-sensitive operations on always-warm infrastructure. Understanding cold starts is crucial for realistic serverless performance expectations."],
    },
    {
        text: "How do you handle state in serverless functions?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless functions are stateless by design - each invocation might run in a different container. Any state needs to be stored externally. For session data, I use databases like DynamoDB or Redis. For file storage, I use S3 or similar object storage. You can use container reuse for temporary caching by storing data outside the handler function, but you can't rely on it being there. For state that needs to persist across invocations, external storage is the only option. I also use managed services like queues or event buses to coordinate between functions. The stateless nature is actually a benefit for scaling - any instance can handle any request. The key is designing your application to be inherently stateless and using appropriate external storage for different types of data."],
    },
    {
        text: "What is the difference between AWS Lambda and serverless containers?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless, ValidTag.enum.lambda, ValidTag.enum.aws],
        answers: ["AWS Lambda is a function-as-a-service with specific language runtimes, size limits, and execution time caps. You upload your code or a deployment package, and AWS manages everything. Serverless containers like AWS Fargate or Cloud Run let you deploy containerized applications that scale to zero. They give you more flexibility - any language, larger packages, longer execution times, and you control the entire runtime environment. Lambda is simpler and faster to deploy for standard use cases. Containers are better when you need custom dependencies, larger workloads, or want to use the same container locally and in production. Lambda cold starts are typically faster. I use Lambda for simple functions and event handlers, and serverless containers for existing applications or when I need more control over the environment."],
    },
    {
        text: "How do you handle database connections in serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Database connections are challenging in serverless because traditional connection pools don't work well. Each function instance creates its own connections, and at scale, you can exhaust database connection limits. I use a few strategies. First, create the connection outside the handler function to reuse it across invocations in the same container. Use connection poolers like RDS Proxy or PgBouncer that manage connections efficiently. Consider using HTTP-based databases or serverless-friendly databases like DynamoDB, Aurora Serverless, or FaunaDB that handle this better. Keep functions warm if database connection latency is critical. Set appropriate connection timeouts and cleanup. For APIs with unpredictable traffic, a connection pooler is almost mandatory to prevent overwhelming your database. The key is understanding that serverless scales quickly and your database needs to handle that."],
    },
    {
        text: "What are edge functions and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Edge functions run on CDN edge nodes close to users rather than in a central region. Services like Cloudflare Workers, Vercel Edge Functions, and Lambda@Edge deploy your code globally. The main benefit is extremely low latency because code runs near the user. They're great for things like authentication checks, redirects, header manipulation, A/B testing, and serving personalized content. The tradeoffs are limited execution time (usually milliseconds), smaller code size limits, and restricted APIs. You typically can't do heavy computation or access regional databases efficiently. I use edge functions for simple, latency-critical operations that benefit from being close to users. For example, checking auth tokens, rewriting URLs, or serving cached content with personalization. They complement regional functions rather than replacing them."],
    },
    {
        text: "How do you handle long-running tasks in serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless functions have execution time limits, so long-running tasks need different approaches. I break tasks into smaller chunks that can be chained together using queues or step functions. For example, AWS Step Functions orchestrate multiple Lambda functions into a workflow. I use queues to process items in batches asynchronously. For truly long tasks, I might trigger a serverless container or a traditional background job. Another approach is recursive function calls where a function processes part of the work and invokes itself with the remaining work. For data processing, I use stream processing to handle data incrementally. The key is designing tasks to be dividable and using orchestration services to coordinate the pieces. This actually makes tasks more resilient since each piece can retry independently."],
    },
    {
        text: "What is the serverless framework?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["The Serverless Framework is a popular tool for building and deploying serverless applications across different cloud providers. You define your functions, events, and resources in a serverless.yml file, and it handles deployment, packaging, and infrastructure provisioning. It supports AWS, Azure, Google Cloud, and others with a consistent interface. The framework includes plugins for extending functionality, local development tools, and environment management. I use it because it simplifies the deployment process and provides abstraction over cloud-specific details. Alternatives include AWS SAM, which is AWS-specific, or infrastructure-as-code tools like Terraform. The Serverless Framework is especially good for multi-cloud strategies and has a large ecosystem of plugins. It reduces the boilerplate of setting up serverless applications and makes deployments reproducible."],
    },
    {
        text: "How do you handle local development for serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Local development for serverless can be challenging since you're developing for a cloud environment. I use tools like serverless-offline or AWS SAM CLI that emulate the serverless environment locally. They spin up local API gateways and invoke functions locally, giving you a development experience similar to traditional apps. For AWS services, LocalStack emulates many AWS services locally. I also write functions to be testable independently of the serverless runtime by extracting business logic from handler functions. Unit tests don't need the full serverless environment. For integration testing, I sometimes use a dev stage in the cloud rather than local emulation. The key is separating business logic from the serverless handler code, which makes it easier to test and develop locally while still deploying to serverless infrastructure."],
    },
    {
        text: "What are the cost implications of serverless vs traditional servers?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["The cost model is completely different. Serverless charges per request and execution time, while traditional servers charge for uptime regardless of usage. For low-traffic applications or those with sporadic usage, serverless is usually cheaper because you only pay for what you use. There's a generous free tier too. For high-traffic applications with consistent load, traditional servers or containers often become more cost-effective because serverless per-request costs add up. The break-even point depends on your traffic patterns. I also factor in operational costs - serverless reduces DevOps overhead, which has value. Hidden costs in serverless include data transfer, API Gateway fees, and keeping functions warm. I analyze expected traffic patterns and calculate costs for both approaches. For startups or variable workloads, serverless often wins. For established apps with predictable high traffic, traditional infrastructure might be cheaper."],
    },
];
