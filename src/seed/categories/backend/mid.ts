import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.backend,
    typeof Level.enum.mid
>[] = [
    // Node.js
    {
        text: "What is Node.js and how does its event-driven architecture work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-driven"]],
        answers: ["Node.js is a JavaScript runtime built on Chrome's V8 engine that lets us run JavaScript on the server side. What makes it really powerful is its event-driven, non-blocking I/O model. Basically, instead of waiting for operations like file reads or database queries to complete, Node.js uses callbacks and the event loop to handle multiple operations concurrently. When you make an async call, Node.js registers a callback and moves on to the next task. When the operation completes, it emits an event that triggers the callback. This architecture makes Node.js incredibly efficient for I/O-heavy applications because a single thread can handle thousands of concurrent connections without blocking.",
            "Node.js lets us run JavaScript outside the browser, typically for building servers and APIs. Under the hood, it uses Chrome's V8 engine to execute JavaScript and libuv for its event loop and async I/O. The key insight is that Node is single-threaded but non-blocking. When you do something slow like reading a file or querying a database, Node doesn't wait around. It hands that operation off to the system, continues executing other code, and when the result is ready, a callback gets queued and executed. This event-driven model means one Node process can handle thousands of concurrent connections because it's never sitting idle waiting for I/O. That's why Node excels at real-time applications, APIs, and anything I/O-bound. For CPU-intensive work, you'd need worker threads or to offload to separate processes, since heavy computation would block the event loop."],
    },
    {
        text: "What is the difference between process.nextTick() and setImmediate()?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-loop"]],
        answers: ["This is a subtle but important distinction in the Node.js event loop. process.nextTick() callbacks are executed immediately after the current operation completes, before the event loop continues. They're processed before any I/O events or timers. On the other hand, setImmediate() is designed to execute a callback on the next iteration of the event loop, after I/O events. In practice, if you want something to run as soon as possible but after the current code completes, use process.nextTick(). If you want to yield to the event loop and let I/O operations process first, use setImmediate(). I generally prefer setImmediate() in most cases because excessive use of process.nextTick() can starve the event loop.",
            "Both schedule code to run asynchronously, but at different points in the event loop. process.nextTick() runs its callback at the end of the current phase, before moving to the next phase. It has the highest priority - it even runs before I/O callbacks and timers. setImmediate() runs in the check phase, after I/O callbacks have had a chance to execute. Think of nextTick as 'run this before anything else gets a chance' and setImmediate as 'run this after I/O but before the next timer tick.' The danger with nextTick is that if you recursively call it, you can starve the event loop and prevent I/O from ever being processed. That's why I default to setImmediate for most use cases - it's more cooperative with the rest of the system. I only use nextTick when I specifically need something to run before any I/O, like when I need to emit an event before returning from a synchronous function."],
    },
    {
        text: "What are streams in Node.js and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.streams],
        answers: ["Streams are one of the most powerful features in Node.js. They let you process data piece by piece instead of loading everything into memory at once. There are four types: readable, writable, duplex, and transform streams. I use streams whenever I'm dealing with large files or data sets. For example, if you're reading a 2GB log file, using fs.createReadStream() lets you process it chunk by chunk rather than loading the entire file into memory. You can also pipe streams together, which is really elegant for things like reading a file, compressing it, and writing it to another location. Streams are essential for building scalable applications because they keep memory usage constant regardless of data size.",
            "Streams let you work with data incrementally rather than loading it all at once. Instead of reading an entire file into memory, a readable stream gives you chunks as they become available. There are four types: readable streams for consuming data, writable streams for producing data, duplex streams that do both like TCP sockets, and transform streams that modify data as it passes through like compression. The power comes from piping - you can chain streams together. For example, read from a file, pipe through a gzip transform, pipe to an HTTP response. The data flows through without ever fully loading into memory. I use streams whenever I'm dealing with large data - file processing, video streaming, handling large API responses. Without streams, processing a 10GB file would require 10GB of RAM. With streams, you're just holding a small buffer at any time. They're also great for real-time data processing."],
    },
    {
        text: "What is the difference between spawning a child process and using worker threads?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["child-processes"]],
        answers: ["Child processes and worker threads solve different problems. Child processes are completely separate instances of the V8 engine with their own memory space. They're great when you need to run external programs or isolate potentially unstable code. The downside is they have higher overhead and communication happens through IPC. Worker threads, on the other hand, run in the same process but in separate threads, sharing memory. They're perfect for CPU-intensive JavaScript operations like image processing or complex calculations. I use worker threads when I need to parallelize JavaScript code without blocking the main thread, and child processes when I need complete isolation or to run non-Node programs.",
            "They're both ways to do work outside the main thread, but they operate differently. Child processes spawn a completely separate Node.js instance with its own memory. Communication happens via IPC, serializing data back and forth. They're heavyweight but fully isolated - if a child crashes, your main process is fine. Worker threads run JavaScript in parallel threads within the same process. They share memory through SharedArrayBuffer and can transfer data efficiently. They're lighter weight but less isolated. I use child processes when running shell commands, other executables, or when I need crash isolation. I use worker threads for CPU-heavy JavaScript work like parsing large JSON, image manipulation, or cryptography. The key difference: child processes are better for isolation and running external programs, worker threads are better for parallelizing JavaScript computation with efficient data sharing."],
    },
    {
        text: "How do you handle uncaught exceptions in Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["error-handling"]],
        answers: ["Handling uncaught exceptions is critical for application stability. I always set up a process-level handler using process.on('uncaughtException') to log the error and gracefully shut down the application. The key thing to understand is that once an uncaught exception occurs, your application is in an undefined state and you should not continue running. I also use process.on('unhandledRejection') for promises that reject without a catch handler. In production, I use a process manager like PM2 that automatically restarts the application after a crash. The best approach though is preventing uncaught exceptions in the first place by proper error handling with try-catch blocks, promise error handling, and wrapping async operations appropriately.",
            "There are two global handlers you need: process.on('uncaughtException') for synchronous throws that escape all try-catch blocks, and process.on('unhandledRejection') for promises that reject without a catch. In both handlers, I log the error with full stack traces, then exit the process. This is important - after an uncaught exception, the application state is unpredictable and continuing to run is risky. In production, I run Node behind a process manager like PM2 or systemd that restarts automatically on crash. But really, these handlers are a last resort. The goal is to never hit them. I use async/await with try-catch for async operations, validate inputs early, and make sure every promise chain has error handling. Express apps should have error-handling middleware to catch anything that slips through. The handlers should alert your monitoring and provide diagnostic info, but the app should be structured so unhandled exceptions are rare."],
    },
    {
        text: "What is backpressure and how do you handle it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.streams],
        answers: ["Backpressure happens when data is being produced faster than it can be consumed. It's a really common issue with streams. For example, if you're reading from a fast source and writing to a slow destination, the data can pile up in memory. Node.js streams have built-in backpressure handling. When you call write() on a writable stream, it returns false if the internal buffer is full, signaling you to pause. The proper way to handle this is to stop reading until the stream emits a 'drain' event. If you use the pipe() method, this is all handled automatically, which is why I prefer using pipe() when possible. The key is respecting these signals to prevent memory issues and ensure smooth data flow.",
            "Backpressure occurs when a producer generates data faster than a consumer can process it. In streams, this means data piles up in buffers, eventually exhausting memory. Node.js streams have a built-in mechanism: when you write to a writable stream and its internal buffer exceeds the high water mark, write() returns false. That's the signal to stop pushing data. You wait for the 'drain' event, which fires when the buffer has emptied enough to resume. If you're manually piping, you need to handle this yourself by pausing the readable stream on false and resuming on drain. The pipe() method handles this automatically, which is why it's preferred. The pattern is important beyond streams too - any producer-consumer scenario needs backpressure. Message queues handle it with acknowledgments. HTTP clients handle it with response streaming. Ignoring backpressure leads to memory exhaustion and crashed processes."],
    },
    {
        text: "What are buffers in Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.buffers],
        answers: ["Buffers are Node.js's way of handling binary data. Before we had typed arrays in JavaScript, buffers were the only way to work with raw binary data. They're fixed-size chunks of memory allocated outside the V8 heap. You use buffers when working with streams, file systems, or network protocols - basically anything that deals with binary data. For example, when reading an image file or handling TCP sockets, you're working with buffers. You can convert between buffers and strings using different encodings like UTF-8, Base64, or hex. One thing to watch out for is that buffers are mutable and have a fixed size, so you need to allocate the right amount of memory upfront.",
            "Buffers represent fixed-length sequences of bytes, essentially raw binary data in memory. JavaScript historically only had strings and objects, so Node needed a way to handle binary for things like files, network I/O, and cryptography. Buffers are allocated outside the V8 heap as raw memory. You create them with Buffer.alloc() for zeroed memory or Buffer.from() to create from existing data. They're useful whenever you're dealing with binary - reading files, TCP streams, image processing, cryptography. You can convert to and from strings with different encodings: buffer.toString('utf8') or Buffer.from('hello', 'utf8'). Buffers are like arrays but for bytes, with values from 0-255. They're mutable, fixed-size, and lower-level than normal JavaScript data. Modern Node also has Uint8Array which serves similar purposes, and Buffer is actually a subclass of it now. For most binary work in Node, you'll encounter buffers in stream events and file operations."],
    },
    {
        text: "What is the difference between require and import?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.modules],
        answers: ["require is the CommonJS module system that Node.js originally used, while import is the ES6 module syntax. The main differences are that require is synchronous and happens at runtime, while import is asynchronous and happens at parse time. This means with import you can do static analysis and tree-shaking. require gives you the entire module at once, whereas import lets you selectively import just what you need. In modern Node.js, you can use ES modules by setting type: 'module' in your package.json or using .mjs extensions. I tend to prefer ES modules for new projects because they're the standard and offer better tooling support, but require is still widely used and perfectly fine, especially in existing codebases.",
            "These are two different module systems. require is CommonJS, Node's original module format. It's synchronous - the module loads and executes right when require is called, at runtime. You can use require dynamically, like require(someVariable), because it's just a function call. import is ES Modules, the JavaScript standard. It's statically analyzed at parse time, before any code runs. This enables tree-shaking since bundlers can see exactly what's imported and eliminate dead code. ES Modules are also asynchronous, which matters for top-level await. To use ES Modules in Node, set \"type\": \"module\" in package.json or use .mjs extensions. The syntax differs too: require returns the whole exports object, while import can destructure named exports directly. For new projects, I prefer ES Modules since they're the standard and have better tooling support. But CommonJS works fine and lots of packages still use it. Node can interoperate between them with some caveats."],
    },
    {
        text: "How does the module resolution algorithm work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.modules],
        answers: ["When you require a module, Node.js follows a specific algorithm to find it. First, it checks if it's a core module like 'fs' or 'http' - those are loaded directly. If it starts with './' or '../', it treats it as a file or directory relative to the current file. For file paths, it tries adding .js, .json, and .node extensions. For directories, it looks for package.json and loads the file specified in the 'main' field, or defaults to index.js. If the module doesn't start with a path indicator, Node.js treats it as a package and searches up the directory tree in node_modules folders. This search continues all the way to the root. Understanding this is helpful for debugging module not found errors and organizing your project structure.",
            "When Node sees require('something'), it goes through a specific resolution process. First, is it a core module like 'fs' or 'path'? If yes, load it directly. Next, does it start with './' or '../'? Then it's a relative path - Node looks for that file, trying extensions .js, .json, .node if needed. If it's a directory, Node looks for package.json's main field or falls back to index.js. If the specifier has no path prefix, Node treats it as a package. It starts in the node_modules folder of the current directory and walks up the directory tree, checking node_modules at each level until it finds the package or hits the filesystem root. Once found, the same extension and directory rules apply. Modules are also cached after first load, so requiring the same module twice returns the same object. Knowing this helps debug 'module not found' errors - often it's a typo, missing dependency, or the package is in a node_modules that's not in the search path."],
    },
    {
        text: "What is the cluster module and how do you scale Node.js?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum.cluster, ValidTag.enum.scalability],
        answers: ["The cluster module lets you spawn multiple Node.js processes to take advantage of multi-core systems. Since Node.js is single-threaded, one process can only use one CPU core. With clustering, you create a master process that forks worker processes, typically one per CPU core. All workers can share the same server port, and the master distributes incoming connections among them. I usually use PM2 in production, which handles clustering automatically and adds features like automatic restarts and monitoring. For scaling beyond a single machine, you'd use horizontal scaling with load balancers and multiple servers. The key is understanding that Node.js scales through multiple processes, not threads, which is different from traditional server platforms.",
            "Since Node runs JavaScript on a single thread, one process only uses one CPU core. The cluster module lets you fork multiple worker processes that all share the same port. The master process accepts connections and distributes them to workers using round-robin by default on most platforms. This way, an 8-core machine can run 8 Node processes handling requests in parallel. Each worker is a full Node instance with its own event loop and memory. They're isolated - if one crashes, others keep running. In production, I typically use PM2 which wraps cluster with nice features: automatic restart on crash, zero-downtime reloads, and monitoring. For scaling beyond one machine, you put a load balancer in front of multiple servers. The horizontal scaling story is straightforward since Node apps are generally stateless. Just make sure shared state goes in external stores like Redis or your database, not in-memory."],
    },
    {
        text: "What are environment variables and how do you manage them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs],
        answers: ["Environment variables are key-value pairs that configure your application differently across environments like development, staging, and production. In Node.js, you access them through process.env. I typically use the dotenv package for local development, which loads variables from a .env file. For production, I set them through the hosting platform or container orchestration system. The key principle is never committing sensitive data like API keys or database passwords to version control. I usually have a .env.example file in the repo with placeholder values so other developers know what variables are needed. For complex configurations, I sometimes create a config module that validates and provides type-safe access to environment variables.",
            "Environment variables let you configure applications without changing code. Database URLs, API keys, feature flags - anything that varies between environments lives in env vars. In Node, you read them from process.env. For local development, I use dotenv to load from a .env file that's gitignored. For production, I set them through the platform - Heroku config vars, AWS Parameter Store, Kubernetes secrets, or docker-compose environment sections. The golden rule: never commit secrets to git. I keep a .env.example with dummy values so new developers know what to set up. I also validate environment variables on startup. If a required variable is missing, fail fast with a clear error rather than crashing later with a confusing one. For complex configs, I create a config module that reads env vars, provides defaults where appropriate, and exports typed configuration objects. This centralizes configuration and makes testing easier since you can override the config module."],
    },
    {
        text: "What is the event emitter pattern?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nodejs, ValidTag.enum["event-emitter"]],
        answers: ["The EventEmitter is a core pattern in Node.js for handling asynchronous events. It's basically a pub-sub system where objects can emit named events and listeners can subscribe to them. Most of Node.js core APIs use this pattern - streams, HTTP servers, and more all extend EventEmitter. You can create your own by extending the EventEmitter class. For example, you might emit a 'data' event when new data arrives and have multiple listeners react to it. I use this pattern when I need loose coupling between components or when one action needs to trigger multiple side effects. The key methods are on() or addListener() to subscribe, emit() to trigger events, and once() for one-time listeners. It's a powerful way to build event-driven architectures.",
            "EventEmitter is Node's implementation of the observer or pub-sub pattern. Objects can emit named events, and other code can listen for those events. It's fundamental to Node - streams emit 'data' and 'end' events, HTTP servers emit 'request' events, and so on. You use it by calling emitter.on('eventName', callback) to listen and emitter.emit('eventName', data) to trigger. There's also once() for one-time listeners and removeListener() to unsubscribe. I extend EventEmitter when building components that need to notify others about state changes without tight coupling. For example, a file watcher emitting 'change' events, or a WebSocket wrapper emitting 'message' events. The pattern decouples the emitter from its listeners - it doesn't need to know who's listening or what they'll do. One thing to watch: if you attach too many listeners, Node warns you about a potential memory leak. Set emitter.setMaxListeners() if you legitimately need many."],
    },
    // Express
    {
        text: "How do you handle errors in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum["error-handling"]],
        answers: ["Express has a specific pattern for error handling. You define error-handling middleware with four parameters - err, req, res, and next - and place it after all your other middleware and routes. When you call next() with an argument, Express skips to the error handlers. For async route handlers, I wrap them in a try-catch and pass errors to next(), or use a wrapper function that catches promise rejections automatically. In the error handler, I check the error type and send appropriate HTTP status codes and messages. I typically have different error classes for different scenarios like ValidationError or NotFoundError. In production, I'm careful not to expose stack traces or sensitive details. Having a centralized error handler makes your error responses consistent across the entire application.",
            "Express uses a special middleware signature for errors: four parameters instead of three, with err as the first. You place error handlers after all routes and other middleware. When any middleware or route calls next(error), Express skips everything and jumps to the error handler. For async code, wrap handlers in try-catch and call next(err) on failure. I often create a wrapper function that catches rejected promises automatically so I don't repeat try-catch everywhere. In the error handler itself, I check the error type - ValidationError gets 400, NotFoundError gets 404, everything else gets 500. I send a structured JSON response with an error code and message. In production, I never expose stack traces to clients but I log them server-side. I also set up custom error classes that carry their own status codes. This centralized approach ensures every error flows through one place, making responses consistent and debugging straightforward."],
    },
    {
        text: "How do you structure a large Express application?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["For large Express apps, I follow a layered architecture pattern. I separate concerns into routes, controllers, services, and models or repositories. Routes define the endpoints and use Express routers to modularize by feature or resource. Controllers handle the HTTP layer - parsing requests and sending responses. Services contain the business logic and are framework-agnostic, making them easier to test and reuse. I also create separate directories for middleware, utilities, and config. A typical structure might have folders like routes/, controllers/, services/, models/, middleware/, and utils/. I use dependency injection where possible to make testing easier. The key is keeping each layer focused on its responsibility and avoiding mixing business logic with HTTP concerns.",
            "I organize large Express apps by feature rather than by layer when possible. Each feature module contains its own routes, controllers, and services. For shared code, I have common folders for middleware, utilities, and database models. The structure typically looks like: src/features/users/, src/features/orders/, each with its own route file, controller, and service. Routes handle URL mapping and call controllers. Controllers handle HTTP concerns - parsing request data, calling services, formatting responses. Services contain pure business logic with no Express dependencies, making them testable in isolation. Models or repositories handle data access. I wire everything together in the main app file, mounting feature routers with prefixes like app.use('/api/users', usersRouter). This modular approach scales well - new features are isolated, teams can work independently, and you can easily extract a feature into a microservice later if needed."],
    },
    {
        text: "How do you implement rate limiting in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum["rate-limiting"]],
        answers: ["I typically use the express-rate-limit middleware for basic rate limiting. You configure it with a time window and maximum number of requests, and it tracks requests by IP address. For production apps, I often use Redis as the store instead of in-memory storage, especially when running multiple server instances. This ensures rate limits work correctly across all servers. You can apply rate limiting globally or to specific routes. I usually have stricter limits on authentication endpoints to prevent brute force attacks and more lenient limits on regular API endpoints. For more sophisticated needs, I might implement token bucket or sliding window algorithms. The key is finding the right balance between protecting your resources and not frustrating legitimate users.",
            "The express-rate-limit package is my go-to. You create a limiter with windowMs for the time window and max for the request limit. Apply it as middleware - globally or on specific routes. By default it tracks by IP, storing counts in memory. For production with multiple servers, that won't work since each server has its own count. I use rate-limit-redis to store counts in Redis, so limits apply correctly across all instances. I apply different limits to different routes - strict limits on login endpoints to prevent brute force, moderate limits on expensive operations, generous limits on read-only endpoints. When a client hits the limit, they get a 429 status with Retry-After header. I also include rate limit headers in responses so clients know their quota and remaining requests. The tricky part is handling users behind shared IPs like corporate proxies - sometimes I rate limit by user ID for authenticated routes instead."],
    },
    {
        text: "What are routers in Express and how do you modularize routes?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["Express routers are mini-applications that handle routing for a specific part of your app. Instead of defining all routes on the main app object, you create router instances for different features or resources. For example, I might have separate routers for users, posts, and comments. Each router is defined in its own file, and you mount them on the main app with a prefix like app.use('/api/users', userRouter). This keeps your code organized and makes it easier to manage as the application grows. Routers can also have their own middleware that only applies to their routes. I often organize my routes folder by resource, with each file exporting a router. This modular approach makes the codebase much more maintainable and testable.",
            "Routers let you break a large Express app into manageable pieces. You create a router with express.Router(), define routes on it just like the app object, then mount it on the main app with a path prefix. So in users.router.js I might define router.get('/', listUsers) and router.post('/', createUser). Then in the main app: app.use('/api/users', usersRouter). The routes become /api/users and /api/users with POST. Each router can have its own middleware that only applies to its routes - useful for feature-specific auth or validation. Routers can also be nested. I might have a main API router that mounts sub-routers for each resource. This organization mirrors your domain - users, orders, products each in their own file. It's easier to navigate, easier to test in isolation, and multiple developers can work on different routers without conflicts. When an app grows, good router organization is essential for maintainability."],
    },
    {
        text: "How do you handle file uploads in Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express],
        answers: ["For file uploads, I use middleware like multer, which handles multipart/form-data. You configure it with options like destination, file size limits, and file name handling. I typically validate the file type and size on both the client and server. For small applications, I might store files on the local filesystem, but for production, I usually upload directly to cloud storage like S3 or Google Cloud Storage. With multer, you can process the file in memory and stream it to cloud storage, which is more efficient than writing to disk first. I always implement proper error handling for cases like oversized files or invalid file types, and I'm careful about security - never trust the client-provided filename and always validate file content, not just the extension.",
            "Multer is the standard middleware for handling multipart/form-data uploads in Express. You configure it with storage settings - either disk storage with destination and filename options, or memory storage for processing files in memory. I set limits for file size to prevent huge uploads, and use fileFilter to reject unwanted file types. On the route handler, multer makes the file available on req.file or req.files for multiple uploads. For production, I typically don't store files on the server. Instead, I configure multer for memory storage, then stream the buffer to S3 or another cloud storage service. This avoids disk I/O and cleanup. Security-wise: never use the original filename directly - generate a unique name. Don't trust the MIME type the client sends - verify the actual file content. Set reasonable size limits. And always validate that the file type matches what you expect, checking magic bytes, not just extension."],
    },
    {
        text: "What is CORS middleware and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.express, ValidTag.enum.cors],
        answers: ["CORS middleware handles Cross-Origin Resource Sharing, which is the browser security feature that blocks requests from different origins. In Express, I use the cors package. For development, I might allow all origins, but in production, I configure it to only allow specific origins. You can set allowed methods, headers, and whether to allow credentials. I typically configure it with an object specifying origin, methods, and credentials. For APIs with multiple frontend clients, I maintain a whitelist of allowed origins. You can also configure CORS per-route rather than globally if different endpoints have different requirements. Understanding CORS is crucial because misconfiguration is a common source of bugs, and being too permissive can create security vulnerabilities.",
            "CORS is a browser security mechanism that blocks frontend JavaScript from making requests to different origins unless the server explicitly allows it. When the browser sends a cross-origin request, it includes an Origin header. The server must respond with Access-Control-Allow-Origin matching that origin, or the browser rejects the response. In Express, the cors package handles this. For simple cases, just app.use(cors()) allows all origins. For production, I configure it with specific allowed origins, methods, headers, and whether credentials are allowed. I often use a dynamic origin function that checks against a whitelist. For preflight requests on complex operations, CORS sends an OPTIONS request first - the middleware handles this automatically. Common issues: forgetting credentials: true when using cookies, not including custom headers in allowedHeaders, or setting Access-Control-Allow-Origin to * with credentials which browsers reject. I keep CORS config strict - only allow what's needed."],
    },

    // REST APIs
    {
        text: "How do you version APIs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["api-versioning"]],
        answers: ["There are a few common approaches to API versioning. The most popular is URL versioning, like /api/v1/users and /api/v2/users. It's explicit and easy to understand. Another approach is using headers, either a custom version header or the Accept header with content negotiation. Some teams use query parameters like /api/users?version=2. I personally prefer URL versioning because it's the most visible and makes it easy to route to different implementations. The key is to have a versioning strategy from the start and maintain backward compatibility as long as possible. When you do introduce a new version, clearly document the changes and give clients plenty of time to migrate. I also try to avoid versioning too frequently - only for breaking changes, not new features.",
            "There are three main approaches. URL versioning puts the version in the path: /api/v1/users, /api/v2/users. It's explicit, easy to understand, and simple to route. Header versioning uses a custom header like Api-Version: 2 or the Accept header for content negotiation. It keeps URLs clean but is less discoverable. Query parameter versioning like /api/users?version=2 is simple but can conflict with other parameters. I prefer URL versioning for most cases - it's visible in logs, easy to test with curl, and clients can see exactly which version they're using. The key principle is avoiding breaking changes whenever possible. Add new fields, new endpoints - that's fine. Removing fields, changing types, renaming things - those require a new version. When you do version, run old and new versions in parallel for a deprecation period, clearly communicate the timeline, and monitor usage to see when you can sunset the old version."],
    },
    {
        text: "How do you design error responses?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["error-handling"]],
        answers: ["Good error responses should be consistent and informative. I always use appropriate HTTP status codes - 400 for client errors, 500 for server errors, and so on. The response body typically includes an error code or type, a human-readable message, and sometimes additional details. For validation errors, I include which fields failed and why. I follow a consistent structure across all endpoints, something like { error: { code: 'VALIDATION_ERROR', message: 'Invalid input', details: [...] } }. In production, I'm careful not to expose sensitive information or stack traces. For client developers, I might include a correlation ID they can reference when contacting support. The key is making errors actionable - the client should understand what went wrong and how to fix it.",
            "I use a consistent structure for all errors: status code plus a JSON body with an error code, human-readable message, and optional details. Something like { error: 'VALIDATION_FAILED', message: 'Invalid input', details: [{ field: 'email', issue: 'Invalid format' }] }. The error code is machine-readable for programmatic handling. The message is for developers or users. Details provide context like which fields failed validation. I always use appropriate HTTP status codes - 400 for bad input, 401 for auth required, 403 for forbidden, 404 for not found, 422 for validation failures, 500 for server errors. In production, never expose stack traces or internal details that could help attackers. I include a request ID that's also in our logs, so support can trace issues. Good error design helps developers integrate faster - they can handle errors programmatically based on error codes rather than parsing messages."],
    },
    {
        text: "How do you handle pagination and what are the tradeoffs between offset and cursor-based?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum.pagination],
        answers: ["Offset-based pagination uses page numbers and limits, like ?page=2&limit=20. It's simple and lets users jump to any page, but it has issues with data consistency if items are added or deleted while paginating. Cursor-based pagination uses a pointer to a specific item, usually an ID or timestamp. It's more stable and efficient for large datasets because it uses indexed lookups, but you can't jump to arbitrary pages. I use offset for smaller datasets where users need random access, like admin dashboards. For feeds or large datasets, I prefer cursor-based pagination. I also always include metadata in the response like total count, next cursor, or has more pages, so clients can build proper UI. The key is matching the pagination strategy to your use case.",
            "Offset pagination is straightforward: LIMIT 20 OFFSET 40 gets you page 3 with 20 items per page. It lets users jump to any page, which is nice for admin interfaces. But it has problems. Performance degrades on large datasets because the database still scans skipped rows. And if items are added or deleted during pagination, you'll see duplicates or miss items. Cursor-based pagination uses an opaque token pointing to a specific record - usually based on an ID or timestamp. You request items after that cursor. It's more efficient because it uses indexed lookups regardless of how deep you are. It also handles real-time data well - no duplicates or gaps. The tradeoff is you can't jump to page 50; you can only go forward or backward. For feeds and infinite scroll, cursor-based is clearly better. For admin tables where users need page numbers, offset might be acceptable if the dataset is bounded. Either way, include metadata like hasNextPage in responses."],
    },
    {
        text: "What is rate limiting and how would you implement it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["rate-limiting"]],
        answers: ["Rate limiting restricts how many requests a client can make in a given time period. It protects your API from abuse and ensures fair usage. The simplest approach is a fixed window - count requests per IP in a time window, say 100 requests per hour. I typically implement this with Redis, storing a counter with an expiration. For more sophisticated needs, I use sliding window or token bucket algorithms, which provide smoother rate limiting. I include rate limit headers in responses so clients know their limits and remaining quota. Different endpoints might have different limits - stricter for expensive operations or auth endpoints. For authenticated APIs, I rate limit by user rather than IP. The implementation depends on your scale - simple in-memory storage works for single servers, but distributed systems need Redis or similar.",
            "Rate limiting caps how many requests a client can make in a time window. It prevents abuse, protects against DDoS, and ensures fair resource allocation. Fixed window counting is simplest - track requests per IP per time window, reject when over limit. I store counters in Redis with TTL matching the window. Sliding window log tracks timestamps of each request for smoother limits. Token bucket adds tokens at a fixed rate; each request consumes one. It allows bursts while maintaining average limits. I always include X-RateLimit-Limit, X-RateLimit-Remaining, and Retry-After headers so clients know their quota. For authenticated endpoints, I rate limit by user ID rather than IP - users behind corporate proxies shouldn't share a limit. Different endpoints get different limits: login gets stricter limits to prevent brute force, read-only endpoints can be more generous. When limits are hit, return 429 Too Many Requests with a clear message about when to retry."],
    },
    {
        text: "How do you document APIs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum["api-design"]],
        answers: ["I use OpenAPI (formerly Swagger) specification for API documentation. You can either write the spec by hand or generate it from code annotations. I prefer tools like swagger-jsdoc that let me document endpoints with comments, keeping docs close to the code. This generates interactive documentation where developers can try endpoints directly in the browser. I make sure to document all endpoints, parameters, request bodies, response formats, and error codes. For more complex APIs, I supplement with written guides covering authentication flows, common use cases, and best practices. The key is keeping documentation in sync with the code - I often automate this with CI checks that fail if the spec doesn't match the implementation. Good documentation significantly reduces support burden and helps developers integrate faster.",
            "OpenAPI, formerly Swagger, is my standard for REST API documentation. The spec is machine-readable YAML or JSON that describes every endpoint, parameter, request body, and response. From that spec, you can generate interactive docs with Swagger UI where developers can make real requests. I typically generate the spec from code annotations using tools like swagger-jsdoc, keeping docs co-located with handlers. This reduces drift between docs and implementation. I also include authentication info, error response schemas, and example payloads. Beyond the spec, I write getting-started guides that walk through common workflows. For internal APIs, I set up a CI check that validates the spec against actual routes. The payoff is huge - good docs mean fewer support questions, faster integration for clients, and a better developer experience. I treat documentation as a feature, not an afterthought."],
    },
    {
        text: "How do you handle filtering and sorting?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"]],
        answers: ["For filtering, I use query parameters that map to the fields being filtered, like ?status=active&category=tech. For complex filters, I might support operators like ?price[gte]=100&price[lte]=500. Sorting typically uses a sort parameter with the field name, like ?sort=createdAt, and a minus sign for descending order, like ?sort=-createdAt. I validate all filter and sort parameters against a whitelist to prevent injection attacks and ensure users can only filter on indexed fields for performance. On the backend, I parse these parameters and build database queries safely. I'm careful to handle edge cases like invalid field names or malformed values. The API should return clear errors if someone tries to filter or sort on unsupported fields. This approach is intuitive and scales well.",
            "I pass filtering and sorting through query parameters. For filtering, each filterable field gets its own parameter: ?status=active&type=premium. For range queries, I use a bracket syntax like ?price[gte]=100&price[lte]=500 or ?createdAt[after]=2024-01-01. For sorting, I use a sort parameter with field names: ?sort=createdAt for ascending, ?sort=-createdAt with minus for descending. Multiple sort fields: ?sort=-priority,createdAt. On the backend, I whitelist allowed filter and sort fields to prevent SQL injection and ensure only indexed columns are used. I parse the parameters into a structured format and build the query using parameterized queries. Invalid field names return 400 with a clear message listing valid options. For complex filtering needs, some APIs adopt a filter query language, but for most cases simple query params work well and are more discoverable. I document all available filters in the API spec."],
    },
    {
        text: "What are ETags and conditional requests?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum["rest-api"], ValidTag.enum.etag, ValidTag.enum.caching],
        answers: ["ETags are identifiers for specific versions of a resource, usually a hash of the content. The server includes an ETag header in the response. The client can then make a conditional request with the If-None-Match header containing that ETag. If the resource hasn't changed, the server returns 304 Not Modified without the body, saving bandwidth. Similarly, If-Match headers are used for safe updates - the server only processes the request if the resource hasn't changed since the client last saw it, preventing lost updates. This is great for caching and concurrency control. I implement ETags by hashing response bodies or using database version fields. Express has middleware that can handle this automatically, though for APIs I often implement it manually for better control over what's cached.",
            "ETags are version identifiers for resources, typically a hash of the content or a version number. The server sends ETag in the response header. For caching, clients send If-None-Match with the ETag on subsequent requests. If the resource hasn't changed, the server returns 304 Not Modified with no body - saving bandwidth. For concurrent updates, clients send If-Match with the ETag when updating. If someone else changed the resource, the ETag won't match, and the server returns 412 Precondition Failed. This prevents the lost update problem where two clients overwrite each other. I implement ETags either by hashing the response body, using a version column in the database, or using the updatedAt timestamp. The choice depends on how expensive hashing is versus how reliable timestamps are. For APIs where clients fetch and then update resources, ETags provide optimistic locking without holding database locks."],
    },

    // GraphQL
    {
        text: "What is GraphQL and how does it differ from REST?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["rest-api"]],
        answers: ["GraphQL is a query language for APIs where clients specify exactly what data they need. Unlike REST where you have multiple endpoints, GraphQL typically has a single endpoint and clients send queries describing their data requirements. The big advantage is it solves over-fetching and under-fetching - clients get exactly what they need in one request instead of making multiple calls or getting excessive data. REST is resource-based with fixed endpoints, while GraphQL is schema-based with flexible queries. GraphQL is great for complex, interconnected data and when you have diverse clients with different needs. REST is simpler and works well for straightforward CRUD operations. I choose GraphQL when the frontend needs a lot of flexibility, and REST when simplicity and caching are priorities.",
            "GraphQL is a query language and runtime for APIs that lets clients request exactly the data they need. Instead of multiple REST endpoints returning fixed data shapes, GraphQL has a single endpoint where clients send queries specifying the fields they want. The main benefit is solving over-fetching and under-fetching. With REST, you often get more data than you need or have to make multiple requests to get related data. GraphQL lets you fetch a user, their posts, and their friends in one request, returning only the fields you asked for. The tradeoffs: GraphQL adds complexity on the server and makes HTTP caching harder since every query is different. REST is simpler, has better tooling, and works great when your data needs are predictable. I use GraphQL when I have multiple clients with different data needs, complex interconnected data, or a need for rapid frontend iteration. REST is better for simple CRUD APIs and public APIs where caching matters."],
    },
    {
        text: "What are queries, mutations, and subscriptions?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["These are the three main operation types in GraphQL. Queries are for reading data - they're like GET requests in REST. You specify what fields you want and GraphQL returns just that data. Mutations are for modifying data - creating, updating, or deleting. They're similar to POST, PUT, and DELETE in REST. Mutations explicitly signal that something is changing, unlike queries. Subscriptions are for real-time updates. The client subscribes to events and the server pushes updates when data changes. This is typically implemented with WebSockets. In my schema, I define separate root types for each: Query, Mutation, and Subscription. I use queries for all read operations, mutations for writes, and subscriptions when clients need live updates, like in chat applications or live dashboards.",
            "GraphQL has three operation types for different use cases. Queries fetch data without side effects - think of them as read operations. You describe what you want and get back exactly that structure. Mutations modify data - create, update, delete operations. They explicitly indicate that something is changing on the server. The naming convention is usually verbs like createUser, updatePost, deleteComment. Subscriptions enable real-time updates. The client subscribes to an event type, and the server pushes data whenever that event occurs. They typically run over WebSockets. For example, a chat app might subscribe to newMessage and get notified whenever someone sends a message. Each operation type has its own root type in the schema: type Query, type Mutation, type Subscription. The separation makes intent clear - queries are safe to retry and cache, mutations are not, and subscriptions establish long-lived connections. Most apps use queries and mutations heavily; subscriptions are used selectively where real-time updates are genuinely needed."],
    },
    {
        text: "What is the GraphQL schema and type system?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["The GraphQL schema is a contract between the client and server that defines what data is available and how to request it. It's written in the GraphQL Schema Definition Language. You define object types with fields, like a User type with name, email, and posts fields. The type system is strongly typed with built-in scalars like String, Int, Boolean, and you can define custom scalars. Fields can be required with the ! operator or lists with brackets. The schema also defines the root Query, Mutation, and Subscription types that serve as entry points. This strong typing enables powerful developer tools like autocomplete and validation. I write the schema to model my domain, and it serves as living documentation that's always in sync with the implementation.",
            "The schema is the heart of GraphQL - it defines all available types, their fields, and how they relate. It's written in Schema Definition Language or SDL. You define types like type User { id: ID!, name: String!, email: String, posts: [Post!]! }. The exclamation mark means non-null - the field is required. Brackets denote lists. Built-in scalars include String, Int, Float, Boolean, and ID. You can define custom scalars for things like DateTime or JSON. Enums, interfaces, and union types provide more expressiveness. The root types Query, Mutation, and Subscription are entry points for operations. What makes the type system powerful is that it's introspectable - clients can query the schema itself to discover available types and fields. This enables IDE features like autocomplete, validates queries before execution, and makes the schema self-documenting. I design schemas to model the domain, not the database. The schema is the API contract that both frontend and backend develop against."],
    },
    {
        text: "How would you handle the n+1 problem in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["n-plus-one"], ValidTag.enum.performance],
        answers: ["The n+1 problem happens when you fetch a list and then make separate database queries for related data for each item. In GraphQL, this is really common because of how resolvers work. The solution is DataLoader, a batching and caching utility. Instead of making individual queries in each resolver, DataLoader batches them together and executes them in a single query. For example, if you're fetching users and their posts, DataLoader will collect all the user IDs that need posts and fetch them in one query. It also caches results within a single request. I create DataLoader instances per request and pass them through the context. This dramatically improves performance without changing your resolver structure. It's pretty much essential for any production GraphQL API.",
            "The n+1 problem is acute in GraphQL because resolvers execute independently. If you query 50 users with their posts, a naive implementation runs 1 query for users, then 50 queries for posts - 51 total instead of 2. DataLoader solves this by batching and caching. You give DataLoader a batch function that takes an array of keys and returns values. When resolvers call loader.load(id), DataLoader collects all the IDs requested in the current tick and calls the batch function once with all of them. So instead of 50 individual queries, you get one query with 50 IDs. DataLoader also caches within a request, so if the same user appears twice, it only fetches once. The key is creating fresh DataLoader instances per request - they shouldn't live across requests or you'll get stale data and memory leaks. I pass DataLoaders through the GraphQL context. This pattern is essential for performant GraphQL; without it, complex queries can generate hundreds of database queries."],
    },
    {
        text: "What are resolvers and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Resolvers are functions that return data for each field in your GraphQL schema. When a query comes in, GraphQL executes the resolver for each requested field. A resolver receives four arguments: parent (the result from the parent resolver), args (arguments passed to the field), context (shared data like the current user or database connection), and info (metadata about the query). For simple fields, resolvers can just return the property from the parent object. For complex fields, they might fetch from a database or call another service. Resolvers are executed in a specific order following the query structure. I organize my resolvers by type and keep them focused on data fetching, putting business logic in separate service layers. This separation makes testing easier and keeps resolvers clean.",
            "Resolvers are the functions that actually fetch the data for GraphQL fields. Each field in your schema can have a resolver, and GraphQL calls them to build the response. A resolver receives four arguments: parent is the result from the parent field's resolver (for nested types), args are any arguments passed to this field, context is shared request-wide data like authenticated user or DataLoaders, and info contains the parsed query and schema info. Resolvers can return data directly, a promise, or an array. For scalar fields on objects, you often don't need explicit resolvers - GraphQL uses default resolvers that just return object properties. Custom resolvers are needed when you need to transform data, fetch related data, or apply authorization. I keep resolvers thin - they handle data fetching but delegate business logic to services. This makes them easier to test and keeps concerns separated. Resolvers execute in parallel where possible, following the query tree structure."],
    },
    {
        text: "What is the DataLoader pattern?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.performance],
        answers: ["DataLoader is a pattern and library for batching and caching data fetches. It solves the n+1 query problem by collecting all the data requests made during a single tick of the event loop and batching them into one request. You give DataLoader a batch function that takes an array of keys and returns a promise of values in the same order. When resolvers call dataLoader.load(id), those calls are collected and the batch function is called once with all the IDs. DataLoader also provides per-request caching, so if the same ID is requested multiple times in one query, it only fetches once. I create new DataLoader instances per request and pass them through context. This is crucial for GraphQL performance and has become a standard pattern in the ecosystem.",
            "DataLoader batches and caches data-fetching operations. You initialize it with a batch function that receives an array of keys and returns a promise of corresponding values in the same order. When you call loader.load(key), DataLoader doesn't execute immediately. It waits until the next tick of the event loop, collects all load calls, then executes the batch function once with all the keys. This turns n individual database queries into one. DataLoader also deduplicates - if you load the same key twice, it only appears once in the batch and both calls receive the same result. Per-request caching means loading the same key later in the same request returns the cached result instantly. The pattern originated from Facebook for their GraphQL infrastructure. Beyond GraphQL, it's useful anywhere you have the n+1 pattern. The critical rule: create fresh DataLoader instances per request. Shared instances cause cache pollution between requests and can return stale data."],
    },
    {
        text: "How do you handle authentication and authorization in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.auth, ValidTag.enum.authorization],
        answers: ["Authentication typically happens before the GraphQL layer. I verify the token in middleware and attach the authenticated user to the context. Then all resolvers can access the current user from context. For authorization, I check permissions within resolvers before returning data. Some people use directive-based authorization like @auth or @hasRole, which is declarative and clean. Others prefer explicit checks in resolver code. I usually combine both - directives for simple role checks and manual checks for complex business logic. Field-level authorization is important in GraphQL since clients can request any fields. I make sure to check permissions on individual fields, not just top-level queries. The key is never trusting the client and always validating access at the resolver level.",
            "I handle authentication at the HTTP layer before GraphQL executes. Middleware verifies the JWT or session token and attaches the authenticated user to the request. That user goes into GraphQL context, available to all resolvers. Authorization happens within resolvers or using schema directives. For simple cases, I use custom directives like @authenticated or @hasRole(['admin']). The directive checks context.user before the resolver runs. For complex rules, I check authorization explicitly in resolver code. Field-level authorization is crucial in GraphQL - even if you can access a User, maybe you shouldn't see their email. I check permissions in field resolvers, not just root queries. One pattern I like: a permissions service that encapsulates authorization logic. Resolvers call permissionService.canViewUserEmail(context.user, targetUser) rather than inline checks. Never trust the client - in GraphQL, clients can query any field in your schema, so every sensitive field needs a guard."],
    },
    {
        text: "What are fragments and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Fragments are reusable pieces of GraphQL queries. Instead of repeating the same field selections in multiple places, you define a fragment and reference it. For example, you might have a UserBasicInfo fragment with name and email fields that you use across different queries. Fragments are client-side constructs - they make your queries more maintainable but don't affect the server. There are also inline fragments for querying fields on specific types in a union or interface. I use fragments to keep queries DRY, especially in frontend code where the same data structure is needed in multiple components. In tools like Apollo Client, fragments can be colocated with components, which creates a nice component-driven data fetching pattern. They're essential for organizing complex GraphQL applications.",
            "Fragments let you define reusable field selections. Instead of repeating the same fields across multiple queries, you define a fragment once and spread it with ...FragmentName. For example, fragment UserCard on User { id, name, avatar } can be reused in any query that fetches users. They're purely a client-side construct - the server receives the expanded query. Named fragments keep queries DRY and maintainable. There are also inline fragments, written as ... on TypeName { fields }. These are used for union and interface types where different concrete types have different fields. If you query a SearchResult that could be a User or Post, you'd use inline fragments to specify which fields for each type. In React apps with Apollo, I colocate fragments with components - each component defines what data it needs. The parent query composes these fragments. This creates a component-driven data fetching pattern where data requirements are explicit and encapsulated."],
    },
    {
        text: "What are directives in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["Directives are a way to modify the execution of queries and schemas. The built-in ones are @include and @skip for conditional field inclusion, and @deprecated for marking fields as deprecated. You can also create custom directives for things like formatting, authorization, or caching. On the client side, directives control query execution. On the server, schema directives let you add behavior to your types and fields. For example, you might have a @auth directive that checks permissions or a @cacheControl directive that sets cache hints. I use them sparingly - they're powerful but can make the schema harder to understand if overused. Custom directives are great for cross-cutting concerns that would otherwise require a lot of repetitive code in resolvers.",
            "Directives modify query execution or schema behavior using @directiveName syntax. GraphQL has three built-in directives. @include(if: Boolean) conditionally includes a field based on a variable. @skip(if: Boolean) conditionally excludes a field. @deprecated(reason: String) marks a field as deprecated in the schema. You can define custom schema directives for cross-cutting concerns. I've used @authenticated to require login, @hasRole(['admin']) for role-based access, @uppercase to transform string outputs, and @rateLimit for field-level rate limiting. Schema directives attach to types, fields, or arguments in your schema definition. At runtime, the directive's logic runs before or after the resolver. They're powerful for encapsulating behavior that applies to many fields without repeating code. The caveat: overusing directives can make the schema hard to understand. I use them for security and caching concerns that truly apply across many fields, not for business logic."],
    },
    {
        text: "How do you handle errors in GraphQL?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum["error-handling"]],
        answers: ["GraphQL has a unique approach to errors. Responses always return 200 OK with both data and errors fields. When a resolver throws an error, it's caught and added to the errors array, while other resolvers continue executing. This is different from REST where an error typically fails the whole request. I create custom error classes for different error types and format them in the GraphQL error formatter. You can add extensions to errors with additional context like error codes or validation details. For user-facing errors, I make sure the message is clear. For security, I avoid exposing stack traces or internal details in production. Some teams use union types for errors as part of the schema, treating errors as data, which gives more type safety on the client side.",
            "GraphQL errors work differently than REST. Every response returns HTTP 200 with both data and errors fields. If a resolver throws, that error goes into the errors array with its path, but other resolvers continue executing. You might get partial data with some fields null and corresponding errors. The errors array contains objects with message, path, and optionally extensions for additional info. I use extensions to add error codes like AUTHENTICATION_REQUIRED or VALIDATION_FAILED that clients can handle programmatically. In the error formatter hook, I sanitize errors before sending - strip stack traces in production, map internal errors to user-friendly messages. Never expose database errors or internal details. Some teams model errors in the schema itself using union types: type CreateUserResult = User | ValidationErrors. This makes errors type-safe and explicit in the schema, though it's more verbose. The approach depends on the team's preference and how critical type safety is for error handling."],
    },
    {
        text: "What is introspection and when should you disable it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql, ValidTag.enum.security],
        answers: ["Introspection is a feature that lets clients query the schema itself to discover available types, fields, and operations. This powers tools like GraphiQL and GraphQL Playground. Clients send special queries starting with __schema or __type to get schema information. It's incredibly useful for development and documentation, but it's a security consideration in production because it exposes your entire API structure to anyone. Many teams disable introspection in production to prevent attackers from easily mapping the API surface. However, this doesn't really provide security through obscurity since a determined attacker could still probe the API. I typically leave it enabled but rely on proper authentication, authorization, and rate limiting for actual security. If you do disable it, make sure your authenticated developers can still access it.",
            "Introspection lets clients query the GraphQL schema itself. Queries like __schema, __type, and __typename expose types, fields, arguments, and documentation. This powers developer tools like GraphiQL and enables IDE autocomplete. But it also means anyone can see your entire API structure. Should you disable it in production? Opinions vary. Some teams disable it to prevent attackers from easily discovering fields. But this is security through obscurity - a determined attacker can still probe endpoints. I focus on proper authorization as the real security layer. That said, I often disable introspection for public-facing APIs while keeping it enabled for internal APIs. If you do disable it, provide schema documentation through other means and ensure developers have access for tooling. You can also selectively allow introspection for authenticated users. Most GraphQL servers have a simple config option to toggle it."],
    },
    {
        text: "What are input types vs output types?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.graphql],
        answers: ["In GraphQL, output types are what you return from queries and mutations - things like object types, unions, and interfaces. Input types are what you pass as arguments to fields and mutations. They're defined with the input keyword and can only contain scalars, enums, and other input types - not object types. This separation exists because inputs and outputs serve different purposes and have different validation needs. For mutations, I create input types for complex arguments instead of having many scalar parameters. For example, CreateUserInput with name, email, and password fields. This makes mutations cleaner and easier to evolve. The naming convention is usually to suffix input types with Input. You can't use the same type for both input and output, which sometimes means duplicating similar structures.",
            "GraphQL has a strict separation between types you can return and types you can accept as arguments. Output types are what resolvers return: object types, interfaces, unions, scalars, enums. Input types are what you pass as arguments, defined with the input keyword. They can only contain scalars, enums, and other input types - not object types, interfaces, or unions. Why the separation? Inputs and outputs have different needs. Outputs can have complex resolvers and relationships. Inputs need to be simple, serializable data that clients can construct. For mutations with multiple parameters, I define input types: input CreateUserInput { name: String!, email: String! }. This is cleaner than multiple arguments and easier to extend. The naming convention is TypeNameInput. One downside: you often define similar structures for input and output. User and CreateUserInput might have overlapping fields. But the separation keeps the schema predictable and validation clear."],
    },
    // WebSockets
    {
        text: "What are WebSockets and how do they differ from HTTP?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["WebSockets provide full-duplex, bidirectional communication over a single TCP connection. Unlike HTTP where the client makes a request and the server responds, WebSockets keep a persistent connection open so both sides can send messages at any time. HTTP is request-response and stateless - each request is independent. WebSockets are stateful and persistent, which makes them perfect for real-time applications like chat, live notifications, or collaborative editing. The overhead is much lower than HTTP for frequent messages since you don't have to re-establish connections or send headers with every message. However, WebSockets are more complex to implement and scale than HTTP. I use them when I need real-time bidirectional communication, but stick with HTTP for traditional request-response patterns.",
            "WebSockets establish a persistent, bidirectional connection between client and server. Once connected, either side can send messages at any time without the overhead of new HTTP requests. HTTP is request-response: client asks, server answers, connection closes. WebSockets stay open - the server can push data whenever it wants, and the client can send without waiting for a response. The message overhead is tiny compared to HTTP headers on every request. This makes WebSockets ideal for real-time apps: chat, multiplayer games, live dashboards, collaborative editing. The tradeoffs: WebSockets are harder to scale because each connection is stateful. Load balancing needs sticky sessions or a pub-sub layer like Redis. HTTP benefits from caching, CDNs, and simpler infrastructure. I reach for WebSockets when I genuinely need push updates or bidirectional communication. For infrequent updates, polling or server-sent events might be simpler."],
    },
    {
        text: "What is the WebSocket handshake?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["The WebSocket handshake is how the connection gets established. It starts as a regular HTTP request with an Upgrade header asking to switch to the WebSocket protocol. The request includes a Sec-WebSocket-Key header with a random value. If the server supports WebSockets, it responds with 101 Switching Protocols status and a Sec-WebSocket-Accept header that's computed from the client's key. This handshake confirms both sides speak the WebSocket protocol. After the handshake completes, the connection switches from HTTP to WebSocket protocol and remains open for bidirectional messages. The handshake being over HTTP is nice because it works through existing infrastructure like load balancers and proxies that understand HTTP, though you need to ensure they also support the protocol upgrade.",
            "WebSocket connections start with an HTTP upgrade request. The client sends a GET request with headers: Connection: Upgrade, Upgrade: websocket, and a randomly generated Sec-WebSocket-Key. The server, if it supports WebSockets, responds with 101 Switching Protocols, echoing back a Sec-WebSocket-Accept header that's a hash of the client's key. This handshake confirms both sides agree to speak WebSocket. After the 101 response, the TCP connection stays open and switches to the WebSocket frame protocol. Messages are now exchanged as WebSocket frames, not HTTP. The HTTP handshake is intentional - it lets WebSocket connections traverse proxies and firewalls that understand HTTP. But infrastructure must support the upgrade. Some load balancers or proxies might not handle upgrades correctly, dropping the connection. When debugging WebSocket issues, the handshake is the first place to look - network tools show the upgrade request and response."],
    },
    {
        text: "What is Socket.io and how does it differ from native WebSockets?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Socket.io is a library that provides real-time communication with fallbacks and extra features. While it uses WebSockets when available, it can fall back to HTTP long-polling if WebSockets aren't supported. Socket.io adds features like automatic reconnection, rooms and namespaces for organizing connections, event-based messaging instead of raw data frames, and broadcasting to multiple clients easily. Native WebSockets are lower level - you send and receive raw messages and have to implement reconnection and other features yourself. Socket.io is easier to use and more reliable across different environments, but it's heavier and requires Socket.io on both client and server. I use Socket.io for complex real-time apps where I want those extra features, and native WebSockets for simpler use cases or when I need to integrate with non-Socket.io clients.",
            "Socket.io is a real-time library built on top of WebSockets with additional features and fallbacks. If WebSockets aren't available, it falls back to HTTP long-polling transparently. Beyond that, it adds automatic reconnection with exponential backoff, rooms for grouping connections, namespaces for multiplexing, event-based messaging with named events, and acknowledgments for confirming message delivery. Native WebSockets are lower-level - you get raw messages and implement everything else yourself. Socket.io is much more convenient but comes with tradeoffs: it requires Socket.io on both ends, adds overhead, and uses its own protocol over WebSockets. You can't connect a native WebSocket client to a Socket.io server. I choose Socket.io for complex apps needing rooms, broadcasting, and reliable reconnection. Native WebSockets work better for simple use cases, interoperability with other platforms, or when bundle size matters."],
    },
    {
        text: "How do you handle WebSocket authentication?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets, ValidTag.enum.auth],
        answers: ["WebSocket authentication is trickier than HTTP because you can't easily send headers after the initial handshake. I typically authenticate during the handshake by passing a token as a query parameter or in the initial HTTP headers. Once connected, I verify the token and store the user identity with the connection. Some people send an authentication message immediately after connecting instead. For session-based auth, cookies work since they're sent with the handshake. I always validate tokens on the server and handle expired tokens by closing the connection. With Socket.io, I use middleware to authenticate during the handshake. It's important to re-validate periodically for long-lived connections and handle token refresh gracefully. The key is ensuring only authenticated users can establish and maintain connections.",
            "WebSocket auth is tricky because you can't add headers after the handshake. There are a few approaches. First, pass the token in the query string during connection: ws://server?token=jwt. The server validates before completing the handshake. Second, if using cookies for auth, they're sent automatically with the upgrade request. Third, connect first, then send an auth message before allowing other operations - the server waits for valid auth before processing anything else. With Socket.io, I use middleware that runs during handshake, verifying the token from auth header or query. I always verify tokens server-side and associate the user identity with the socket. For long-lived connections, tokens expire. I handle this by having clients send a refresh message with a new token, or by closing connections and requiring reconnect with fresh tokens. Security-wise: validate on connect, associate identity with the socket, and check authorization on every sensitive message - don't assume a valid connection means all operations are allowed."],
    },
    {
        text: "What is the difference between WebSockets and Server-Sent Events?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Server-Sent Events (SSE) are a simpler alternative to WebSockets for server-to-client streaming. SSE is unidirectional - only the server can push data to the client. It uses regular HTTP, so it's easier to implement and works through proxies and firewalls better. SSE has automatic reconnection built-in and sends events as text. WebSockets are bidirectional and can send binary data, but are more complex. I use SSE for simple cases where I just need to push updates from server to client, like live news feeds or stock tickers. WebSockets are better when you need bidirectional communication, like chat or real-time collaboration. SSE is often overlooked but it's perfect for scenarios where you don't need full WebSocket complexity. The API is simpler and it's more reliable over unreliable networks.",
            "SSE is simpler than WebSockets when you only need server-to-client push. It's just HTTP with a text/event-stream content type - the server keeps the connection open and sends events. The browser's EventSource API handles parsing and reconnection automatically. WebSockets are bidirectional - both sides can send at any time. SSE is one-way: server pushes to client. If the client needs to send data, it uses regular HTTP requests. SSE has some advantages: it works over standard HTTP, so proxies, firewalls, and CDNs handle it fine. It auto-reconnects with the Last-Event-ID header for resumption. It only sends text, but that covers most use cases. WebSockets have lower overhead for frequent bidirectional messages and support binary data. I use SSE for dashboards, notifications, live feeds - anything where the client just listens. I reach for WebSockets when I need true bidirectional communication or binary data, like games or collaborative editing."],
    },
    {
        text: "How do you handle reconnection and connection state?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.websockets],
        answers: ["Managing connection state is crucial for reliable WebSocket applications. On the client side, I implement automatic reconnection with exponential backoff - if the connection drops, wait a bit before retrying, and increase the wait time with each failed attempt. I track connection state (connecting, connected, disconnected) and show this to users. On the server, I clean up resources when connections close. For reliability, I often implement message acknowledgments so you know if messages were received. Socket.io handles a lot of this automatically. I also implement heartbeats to detect dead connections. When clients reconnect, they need to restore their state - rejoin rooms, resync data, etc. For message ordering, I sometimes add sequence numbers. The challenge is making reconnection seamless from a user perspective while handling all the edge cases on the technical side.",
            "Robust WebSocket apps need careful connection management. On the client: track state as connecting, connected, or disconnected. On disconnect, reconnect automatically with exponential backoff - start at 1 second, double each attempt, cap at 30 seconds. Show connection status in the UI so users understand temporary glitches. On the server: detect dead connections with ping/pong heartbeats. Clients that don't respond within timeout are assumed dead - clean up their state and resources. When clients reconnect, they need to restore their state. This might mean re-authenticating, rejoining rooms, and syncing missed messages. I sometimes include a 'last seen' sequence number or timestamp on reconnect so the server can send missed events. Socket.io handles much of this automatically - reconnection, heartbeats, room rejoin. With native WebSockets, you implement it yourself. The goal: users shouldn't notice brief disconnects. The connection recovers automatically and state resynchronizes transparently."],
    },
    // NestJS
    {
        text: "What is NestJS and how does it differ from Express?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.express],
        answers: ["NestJS is a framework built on top of Express (or optionally Fastify) that adds structure and features inspired by Angular. While Express is minimalist and unopinionated, NestJS is opinionated and provides a complete architecture out of the box. It uses TypeScript by default and leverages decorators for routing, dependency injection, and metadata. NestJS enforces a modular structure with controllers, services, and modules, whereas Express leaves architecture up to you. It includes built-in support for things like validation, serialization, WebSockets, and microservices. I use Express for simple APIs or when I want maximum flexibility, and NestJS for larger applications where the structure and built-in features save time. The learning curve is steeper with NestJS, but it scales better for complex applications with large teams.",
            "NestJS is an opinionated Node.js framework that brings structure to backend development. It sits on top of Express or Fastify, adding TypeScript-first architecture, dependency injection, decorators, and modular organization. Express is minimal - you get routing and middleware, then build everything else yourself. NestJS provides the full picture: controllers for HTTP handling, services for business logic, modules for organization, pipes for validation, guards for auth, interceptors for transforming responses. It's inspired by Angular, so if you know Angular, NestJS feels familiar. The DI container makes testing straightforward - inject mocks easily. I choose Express for small APIs where I want full control and minimal overhead. NestJS shines for larger applications where consistent architecture, built-in features, and team scalability matter. The tradeoff is more boilerplate and a steeper learning curve, but the structure pays off as the codebase grows."],
    },
    {
        text: "What are modules, controllers, and providers in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["These are the core building blocks of NestJS. Modules organize the application into cohesive units - each module groups related controllers and providers. You have a root module and can have feature modules for different parts of your app. Controllers handle incoming requests and return responses - they define your API endpoints using decorators like @Get() and @Post(). Providers are anything that can be injected as a dependency, typically services that contain business logic. You mark them with @Injectable(). The module decorator declares which controllers and providers belong to that module and which ones are exported for other modules to use. This structure enforces separation of concerns - controllers handle HTTP, services handle business logic, and modules organize everything. It makes the application very testable and maintainable.",
            "These are the three core concepts organizing NestJS apps. Modules are containers that group related code. Each module declares its controllers, providers, imports from other modules, and what it exports. The app starts with a root module that imports feature modules. Controllers handle HTTP. They use decorators like @Controller('users'), @Get(), @Post() to define routes. Controllers receive requests, use services to do work, and return responses. They're thin - just HTTP handling, no business logic. Providers are injectable classes, typically services. Decorated with @Injectable(), they contain the actual logic: database queries, external API calls, computations. The module lists providers, and NestJS's DI container injects them where needed. The pattern: controllers call services, services do the work. Modules package related controllers and services. This structure scales well - each feature module is self-contained, and dependency injection makes everything testable."],
    },
    {
        text: "How does dependency injection work in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum["dependency-injection"]],
        answers: ["NestJS has a powerful dependency injection system. You mark classes with @Injectable() and register them as providers in a module. Then you can inject them into constructors of other classes, and NestJS automatically instantiates and provides them. The DI container manages the lifecycle of these instances. By default, providers are singletons scoped to the application, but you can also have request-scoped or transient providers. This makes testing easy because you can inject mock dependencies. It also promotes loose coupling - classes depend on interfaces rather than concrete implementations. I use custom providers when I need more control, like factory providers or using existing instances. The DI system is one of NestJS's best features, making the code more modular, testable, and maintainable than manually managing dependencies.",
            "NestJS has a built-in IoC container that handles dependency injection. You mark classes with @Injectable() and list them as providers in a module. When NestJS sees a constructor parameter with a type, it looks up the provider for that type and injects an instance. By default, providers are singletons - one instance shared across the app. You can also use request scope for per-request instances or transient scope for new instances each time. For testing, you swap real providers with mocks using the testing module's overrideProvider(). The DI system supports advanced patterns: useValue for constants, useClass for alternative implementations, useFactory for dynamic creation with async setup. I use interfaces with injection tokens when I need to inject different implementations based on configuration. The key benefit: components don't create their own dependencies, they receive them. This inverts control, makes testing trivial, and keeps components focused on their own logic."],
    },
    {
        text: "What are guards and how do you implement authorization?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.guards, ValidTag.enum.authorization],
        answers: ["Guards are NestJS classes that determine whether a request should be handled by the route handler. They implement the CanActivate interface and return true or false. I use guards for authentication and authorization. For example, an AuthGuard checks if the user is logged in, and a RolesGuard checks if they have the required role. Guards execute before interceptors and pipes, making them perfect for access control. You can apply them globally, to controllers, or to individual routes using the @UseGuards() decorator. I often combine guards with custom decorators to extract user information from the request. For role-based access, I create a @Roles() decorator and a guard that reads it using reflection. Guards are cleaner than middleware for authorization because they have access to the execution context and fit into NestJS's architecture.",
            "Guards implement the CanActivate interface - a canActivate() method that returns true to allow the request or false to deny it. They're perfect for auth. An auth guard might verify a JWT and attach the user to the request. A roles guard might check if the user has required permissions. Apply guards with @UseGuards(): globally, on controllers, or on specific routes. Guards run before pipes and interceptors, so they're the first line of defense. For role-based auth, I combine a @Roles('admin') decorator with a guard that reads the metadata. The guard uses Reflector to get the roles from the decorator, then checks if the user has them. Guards have access to ExecutionContext, so they can examine the request, get handler metadata, and make nuanced decisions. They're much cleaner than middleware for authorization because they integrate with NestJS's decorator-based approach and have access to full request context."],
    },
    {
        text: "What are interceptors and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.interceptors],
        answers: ["Interceptors let you transform the result or add extra logic before or after method execution. They implement the NestInterceptor interface and use RxJS operators since NestJS uses observables. I use interceptors for things like response transformation, logging, caching, and adding extra headers. For example, a logging interceptor can measure request duration, or a transform interceptor can wrap responses in a consistent format. You can bind interceptors globally, to controllers, or to specific routes. They're powerful for cross-cutting concerns that affect multiple routes. The ability to transform responses using RxJS operators like map is really useful. I've used interceptors to automatically serialize responses, add pagination metadata, and implement timeout logic. They're similar to middleware but operate at a higher level with access to the execution context and return values.",
            "Interceptors wrap around route handlers, giving you hooks before and after execution. They implement NestInterceptor with an intercept() method that receives the execution context and a call handler. You call next.handle() to invoke the route handler, which returns an Observable. Then you can use RxJS operators to transform the response, add timeout, cache results, or log timing. Common uses: wrapping all responses in { data: ... } format with map(), logging request duration, implementing cache with memoization, adding timeout with the timeout() operator. Apply with @UseInterceptors() globally, per-controller, or per-route. Interceptors run after guards and before pipes for the request phase, then after the handler for the response phase. They're more powerful than middleware because they can transform responses, not just requests. I use them for cross-cutting concerns like consistent response formatting, logging, and performance monitoring."],
    },
    {
        text: "What are pipes and how do you handle validation?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.pipes, ValidTag.enum.validation],
        answers: ["Pipes transform or validate input data before it reaches the route handler. NestJS provides built-in pipes like ValidationPipe and ParseIntPipe. The ValidationPipe works with class-validator decorators on DTOs - you define your data transfer objects with validation decorators like @IsString() or @IsEmail(), and the pipe automatically validates incoming data. If validation fails, it throws an error with details. I use the global ValidationPipe in most applications because it ensures all inputs are validated. You can also create custom pipes for specific transformations. Pipes can be applied globally, to controllers, to routes, or to specific parameters. For example, ParseIntPipe transforms a string parameter to a number. This declarative approach to validation is much cleaner than manual validation in route handlers.",
            "Pipes have two main uses: validation and transformation. They run before the route handler, processing arguments. Built-in pipes include ParseIntPipe for converting strings to numbers, ParseUUIDPipe for validating UUID format, and ValidationPipe for full DTO validation. ValidationPipe is the most powerful - it works with class-validator decorators. You define DTOs with decorators like @IsEmail(), @IsNotEmpty(), @MinLength(6). When a request comes in, ValidationPipe validates the body against the DTO and throws BadRequestException with details if it fails. I enable it globally with app.useGlobalPipes(new ValidationPipe({ whitelist: true })) - whitelist strips unknown properties. You can apply pipes at different levels: globally, per-controller, per-route, or per-parameter. Custom pipes implement PipeTransform with a transform() method. Use them for data conversions your app needs. The declarative validation with DTOs is cleaner than manual checks and keeps validation rules next to the types."],
    },
    {
        text: "What are exception filters?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum["error-handling"]],
        answers: ["Exception filters handle errors thrown anywhere in the application. NestJS has a built-in global exception filter that handles standard HTTP exceptions, but you can create custom filters for specific error handling. They implement the ExceptionFilter interface and catch exceptions, allowing you to transform them into appropriate HTTP responses. I create custom exception filters for domain-specific errors or to format error responses consistently. For example, a database exception filter might catch database errors and return user-friendly messages. You can catch all exceptions or specific types. Filters can be global, controller-scoped, or method-scoped. They're the last line of defense in the request lifecycle, ensuring even unexpected errors are handled gracefully. I typically have a global filter for general errors and specific filters for things like validation errors or database errors.",
            "Exception filters catch thrown errors and transform them into HTTP responses. NestJS has a default filter that handles HttpException and its subclasses, returning appropriate status codes and messages. Custom filters implement ExceptionFilter with a catch() method that receives the exception and argument host. You can target specific exception types or catch everything. Use @UseFilters() to apply per-route or per-controller, or app.useGlobalFilters() for global. I create custom filters for consistent error formatting, logging, and handling domain-specific exceptions. For example, a ValidationException filter might structure validation errors in a specific format. A generic filter catches everything else and logs to monitoring while returning a safe error to clients. Filters are the last layer - if an error gets thrown and nothing catches it, the filter kicks in. They ensure clients always get structured error responses rather than raw stack traces."],
    },
    {
        text: "How do you handle configuration in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["NestJS has a dedicated @nestjs/config module built on dotenv. I import ConfigModule globally in the root module, usually with ConfigModule.forRoot(). It loads environment variables from .env files and makes them available through ConfigService, which you can inject anywhere. I create configuration namespaces for different parts of the app like database, auth, etc., using configuration factories. These return typed configuration objects, providing type safety. ConfigService has a get method to retrieve values, and you can provide default values. I validate configuration on startup using Joi or class-validator to fail fast if required variables are missing. For different environments, I use different .env files like .env.development and .env.production. This approach is much better than accessing process.env directly because it's testable, typed, and centralized.",
            "The @nestjs/config module handles environment configuration cleanly. Import ConfigModule.forRoot() in your root module - it loads .env files using dotenv. Then inject ConfigService anywhere you need config values. I organize configuration into namespaces using configuration factories. A database config factory returns { host, port, name } from env vars, registered with ConfigModule.forFeature(). This gives you typed, namespaced access: configService.get('database.host'). Validation is crucial - use validationSchema option with Joi to verify required vars on startup. If DATABASE_URL is missing, the app fails fast rather than crashing later. For environments, envFilePath can load .env.development or .env.production based on NODE_ENV. The ConfigService is injectable, so it's easy to mock in tests. This approach beats raw process.env access: configuration is validated, typed, injectable, and testable."],
    },
    {
        text: "What is the difference between @Injectable() and @Controller()?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["These are different types of decorators that serve different purposes. @Injectable() marks a class as a provider that can be managed by NestJS's dependency injection container. You use it on services, repositories, or any class you want to inject elsewhere. Controllers use @Controller(), which marks the class as handling HTTP requests and binds it to a route. Controllers define your API endpoints using route decorators like @Get() and @Post(). Under the hood, controllers are also injectable, which is why you can inject services into them. But conceptually, controllers handle the HTTP layer while injectable services handle business logic. You declare controllers in the module's controllers array and providers in the providers array. The separation keeps concerns clear - controllers focus on request/response handling, and services focus on application logic.",
            "@Controller() and @Injectable() mark different roles in NestJS. @Controller('users') creates an HTTP controller bound to /users. It handles requests using @Get(), @Post(), etc. Controllers are the entry point for HTTP traffic. @Injectable() marks a class as a provider that can be injected. Services, repositories, helpers - any class you want in the DI container gets @Injectable(). Controllers can receive injected dependencies, so they're implicitly injectable. But their purpose differs: controllers handle HTTP, services contain logic. In the module, controllers go in the controllers array, providers in providers. A controller has UserService injected in its constructor. The service has @Injectable() and contains the actual database queries or business rules. This separation keeps controllers thin - they parse requests, call services, return responses. Services are reusable and framework-agnostic. The distinction enforces clean architecture."],
    },
    {
        text: "How do you implement WebSockets in NestJS?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.websockets],
        answers: ["NestJS has great WebSocket support through the @nestjs/websockets and @nestjs/platform-socket.io packages. You create a gateway class decorated with @WebSocketGateway(), which is similar to a controller but for WebSocket events. Inside the gateway, you use @SubscribeMessage() decorators to handle specific events, just like route handlers. You can inject services into gateways and use all the NestJS features like guards and pipes. For Socket.io features, you use decorators like @ConnectedSocket() and @MessageBody() to access the socket and message data. Gateways also have lifecycle hooks like afterInit, handleConnection, and handleDisconnect. I typically create separate gateways for different namespaces or features. The pattern is very similar to HTTP controllers, making it intuitive if you already know NestJS.",
            "NestJS integrates WebSockets through gateways - classes decorated with @WebSocketGateway(). They're like controllers but for socket events. Inside, @SubscribeMessage('eventName') handlers respond to client messages, similar to @Get() routes. You receive message data with @MessageBody() and the socket with @ConnectedSocket(). The gateway can inject services just like controllers. Lifecycle hooks include handleConnection() and handleDisconnect() for managing client connections. Guards work too - apply them to protect certain message handlers. NestJS uses Socket.io by default via @nestjs/platform-socket.io. You can configure the port, namespace, and CORS in @WebSocketGateway({ port: 3001, namespace: 'chat' }). To emit from elsewhere, inject the gateway and call this.server.emit(). The pattern mirrors HTTP controllers closely - same dependency injection, similar decorators, familiar structure. If you know NestJS REST, you'll pick up WebSocket gateways quickly."],
    },
    {
        text: "How do you structure a large NestJS application?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs],
        answers: ["I organize NestJS apps by feature modules rather than by layer. Each feature gets its own directory with its module, controllers, services, DTOs, and entities. For example, a users feature would have users.module.ts, users.controller.ts, users.service.ts, etc. Shared code goes in a common or shared module. I keep core infrastructure like database connections and configuration in a core module. Domain logic lives in services within feature modules, keeping controllers thin. For large apps, I might have sub-modules within features. I use barrel exports (index.ts files) to simplify imports. Database entities or schemas go with their feature. This structure scales well because features are isolated and teams can work independently on different modules. I also separate DTOs from entities to keep API contracts separate from database models.",
            "I organize by feature, not by type. Rather than having all controllers in one folder and all services in another, I group by domain. A users/ folder contains users.module.ts, users.controller.ts, users.service.ts, dto/, and entities/. Each feature is a self-contained module. The AppModule imports feature modules. Shared utilities go in a common/ or shared/ module that other modules import. Infrastructure like database configuration lives in a core/ module imported once at the root. For complex features, nest sub-modules: orders/ might contain orders.module plus shipping/ and payments/ sub-modules. I use index.ts barrel exports to clean up imports. DTOs are separate from entities - the API contract shouldn't be tied to database schema. This structure scales: features are isolated, teams own modules independently, and extracting a microservice later is straightforward. The key is keeping modules focused and using dependency injection to wire them together."],
    },
    {
        text: "What are custom decorators and how do you create them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.nestjs, ValidTag.enum.decorators],
        answers: ["Custom decorators let you extract common patterns and make your code more declarative. You create them using the createParamDecorator function from @nestjs/common. For example, I often create a @CurrentUser() decorator that extracts the authenticated user from the request, so controllers can just use @CurrentUser() user: User instead of digging into the request object. You can also create metadata decorators using SetMetadata for things like @Roles(['admin']) that guards can read using Reflector. Decorators can be composed - you can combine multiple decorators into one using applyDecorators. I use custom decorators to reduce boilerplate, improve readability, and encapsulate common request data extraction. They make your route handlers cleaner and more focused on business logic rather than request manipulation.",
            "Custom decorators encapsulate repetitive patterns into clean, reusable annotations. For parameter decorators, use createParamDecorator(). Classic example: @CurrentUser() that extracts the authenticated user from the request context. Instead of @Req() req then req.user everywhere, just @CurrentUser() user: User. For metadata decorators that guards or interceptors read, use SetMetadata(). I create @Roles('admin', 'manager') that sets metadata, then a guard reads it with Reflector and checks the user's roles. You can compose multiple decorators with applyDecorators(): combine @UseGuards(AuthGuard) with @SetMetadata('roles', ['admin']) into a single @Auth('admin') decorator. This reduces boilerplate dramatically. Controllers become expressive: @Get() @Auth('admin') @CurrentUser() user. The decorator approach is core to NestJS's philosophy - decorators keep handlers focused on logic rather than extracting data or checking conditions repeatedly."],
    },

    // Serverless
    {
        text: "What is serverless architecture and what are its tradeoffs?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless means you write functions that run in managed compute environments without managing servers yourself. You're billed only for actual execution time, not idle time. The main benefits are automatic scaling, no server maintenance, and cost efficiency for variable workloads. The tradeoffs are cold starts causing latency, execution time limits, vendor lock-in, and statelessness requiring external storage for any state. Debugging and local development are harder. For applications with unpredictable traffic or event-driven workloads, serverless is great. For consistent high-traffic applications, traditional servers might be more cost-effective. I use serverless for APIs with sporadic traffic, background jobs, and webhooks. It's not great for long-running processes or latency-sensitive applications. The key is matching the architecture to your traffic patterns and requirements.",
            "Serverless lets you deploy functions without managing infrastructure. The cloud provider handles servers, scaling, and availability. You just write code, and it runs on demand. AWS Lambda, Google Cloud Functions, Azure Functions are the main platforms. Benefits: automatic scaling from zero to thousands of concurrent executions, pay only for actual compute time, no server maintenance. Tradeoffs: cold starts add latency when functions haven't run recently, execution time limits (15 minutes on Lambda), statelessness means no local storage between invocations, debugging is harder since you can't SSH in. Serverless shines for event-driven workloads, APIs with variable traffic, scheduled jobs, and webhook handlers. It's less ideal for constant high-throughput applications where traditional servers might be cheaper, or for latency-sensitive work where cold starts matter. The mental model shift: design around ephemeral, stateless execution units that scale automatically."],
    },
    {
        text: "What is cold start and how do you mitigate it?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless, ValidTag.enum.lambda, ValidTag.enum.performance],
        answers: ["Cold start is the latency when a serverless function is invoked for the first time or after being idle. The platform needs to spin up a new container, load your code, and initialize it. This can add hundreds of milliseconds or more to the first request. To mitigate it, I keep functions small and minimize dependencies. Using compiled languages like Go instead of interpreted ones like Python helps. Provisioned concurrency keeps functions warm, though it costs more. I reuse connections by initializing them outside the handler function. For critical paths, scheduled pings can keep functions warm. Some platforms now have better cold start performance. I also design around it - using serverless for non-latency-critical paths and keeping time-sensitive operations on always-warm infrastructure. Understanding cold starts is crucial for realistic serverless performance expectations.",
            "Cold start happens when a function hasn't run recently and the platform needs to spin up a new execution environment. This involves loading your code, initializing the runtime, and running any initialization code. Cold starts can add anywhere from 100ms to several seconds depending on the runtime, package size, and initialization logic. Mitigations: keep deployment packages small by trimming dependencies. Use faster runtimes - Go and Rust cold start faster than Node, which is faster than Java or .NET. Initialize connections outside the handler so they're reused across invocations in the same container. Provisioned concurrency on AWS Lambda keeps containers warm, eliminating cold starts but adding cost. Some teams use scheduled pings to prevent functions from going cold. For latency-critical paths, consider keeping those on always-on infrastructure while using serverless for background jobs. Cold starts matter most for user-facing synchronous calls - async processing can tolerate the latency."],
    },
    {
        text: "How do you handle state in serverless functions?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless functions are stateless by design - each invocation might run in a different container. Any state needs to be stored externally. For session data, I use databases like DynamoDB or Redis. For file storage, I use S3 or similar object storage. You can use container reuse for temporary caching by storing data outside the handler function, but you can't rely on it being there. For state that needs to persist across invocations, external storage is the only option. I also use managed services like queues or event buses to coordinate between functions. The stateless nature is actually a benefit for scaling - any instance can handle any request. The key is designing your application to be inherently stateless and using appropriate external storage for different types of data.",
            "Serverless functions are ephemeral - you can't store state locally between invocations. All state goes external. For persistent data: DynamoDB, Aurora Serverless, or Firestore. For fast temporary state: Redis via ElastiCache or Memcached. For file storage: S3. For session state: JWTs or tokens that carry state within them, avoiding session stores entirely. The design principle: assume every invocation starts fresh. Don't rely on global variables persisting - they might between warm invocations, but that's not guaranteed. Connection reuse is one exception: initialize database connections outside the handler function. If the container stays warm, that connection persists. But always handle reconnection because containers get recycled. This stateless design is actually a feature - it's why serverless scales so easily. Without local state to synchronize, any instance can handle any request."],
    },
    {
        text: "What is the difference between AWS Lambda and serverless containers?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless, ValidTag.enum.lambda, ValidTag.enum.aws],
        answers: ["AWS Lambda is a function-as-a-service with specific language runtimes, size limits, and execution time caps. You upload your code or a deployment package, and AWS manages everything. Serverless containers like AWS Fargate or Cloud Run let you deploy containerized applications that scale to zero. They give you more flexibility - any language, larger packages, longer execution times, and you control the entire runtime environment. Lambda is simpler and faster to deploy for standard use cases. Containers are better when you need custom dependencies, larger workloads, or want to use the same container locally and in production. Lambda cold starts are typically faster. I use Lambda for simple functions and event handlers, and serverless containers for existing applications or when I need more control over the environment.",
            "Lambda is function-as-a-service: you provide code for a supported runtime, AWS handles everything else. It has constraints: 15-minute timeout, 50MB zipped package, limited memory and CPU. Serverless containers like Fargate or Cloud Run let you package any application as a container and scale to zero. You control the full environment - any language, any dependencies, any base image. Container cold starts are typically slower than Lambda. Lambda is simpler for event-driven functions: S3 triggers, API Gateway, SQS. Containers work better for existing apps you want to containerize, longer-running processes, or when you need more resources. Lambda also integrates deeper with AWS services. I use Lambda for glue code, webhooks, and event processing. Serverless containers for APIs that need more control, larger memory, or when the team already has a containerized workflow."],
    },
    {
        text: "How do you handle database connections in serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Database connections are challenging in serverless because traditional connection pools don't work well. Each function instance creates its own connections, and at scale, you can exhaust database connection limits. I use a few strategies. First, create the connection outside the handler function to reuse it across invocations in the same container. Use connection poolers like RDS Proxy or PgBouncer that manage connections efficiently. Consider using HTTP-based databases or serverless-friendly databases like DynamoDB, Aurora Serverless, or FaunaDB that handle this better. Keep functions warm if database connection latency is critical. Set appropriate connection timeouts and cleanup. For APIs with unpredictable traffic, a connection pooler is almost mandatory to prevent overwhelming your database. The key is understanding that serverless scales quickly and your database needs to handle that.",
            "Traditional connection pooling breaks in serverless. Each function instance might create its own connection, and with hundreds of concurrent instances, you exhaust database connection limits fast. Strategies: initialize connections outside the handler so they're reused across invocations in the same warm container. Use a connection proxy like RDS Proxy, PgBouncer, or ProxySQL - they pool connections from many function instances into fewer database connections. Consider serverless-native databases like DynamoDB, FaunaDB, or PlanetScale that use HTTP and don't have connection limits. Aurora Serverless v2 handles scaling well. Set connection timeouts aggressively - don't let stale connections hang around. For PostgreSQL, set statement_timeout and idle_in_transaction_session_timeout. The mental model: assume your function will scale to hundreds of instances instantly. Your database must handle that. Without a proxy or connection-friendly database, you'll hit limits during traffic spikes."],
    },
    {
        text: "What are edge functions and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Edge functions run on CDN edge nodes close to users rather than in a central region. Services like Cloudflare Workers, Vercel Edge Functions, and Lambda@Edge deploy your code globally. The main benefit is extremely low latency because code runs near the user. They're great for things like authentication checks, redirects, header manipulation, A/B testing, and serving personalized content. The tradeoffs are limited execution time (usually milliseconds), smaller code size limits, and restricted APIs. You typically can't do heavy computation or access regional databases efficiently. I use edge functions for simple, latency-critical operations that benefit from being close to users. For example, checking auth tokens, rewriting URLs, or serving cached content with personalization. They complement regional functions rather than replacing them.",
            "Edge functions execute on CDN nodes worldwide, close to users. Cloudflare Workers, Lambda@Edge, Vercel Edge Functions, and Deno Deploy are the main options. Latency is minimal - code runs in the same data center as the user, not a central region. Use cases: authentication checks before hitting origin, URL rewrites and redirects, A/B testing, geolocation-based routing, header manipulation, and personalized responses. Constraints: very short execution limits (milliseconds to low seconds), limited CPU, smaller package sizes, and restricted APIs. No file system, limited outbound network. You can't do heavy computation or maintain long-lived connections. Think of them as lightweight middleware that runs globally. I use edge for auth validation, bot detection, and light personalization. Heavy lifting stays in regional functions. Edge complements your architecture by handling latency-sensitive preprocessing before requests reach your core backend."],
    },
    {
        text: "How do you handle long-running tasks in serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Serverless functions have execution time limits, so long-running tasks need different approaches. I break tasks into smaller chunks that can be chained together using queues or step functions. For example, AWS Step Functions orchestrate multiple Lambda functions into a workflow. I use queues to process items in batches asynchronously. For truly long tasks, I might trigger a serverless container or a traditional background job. Another approach is recursive function calls where a function processes part of the work and invokes itself with the remaining work. For data processing, I use stream processing to handle data incrementally. The key is designing tasks to be dividable and using orchestration services to coordinate the pieces. This actually makes tasks more resilient since each piece can retry independently.",
            "Lambda has a 15-minute timeout, so long tasks need restructuring. The main pattern is chunking and orchestration. Break work into smaller pieces that complete within limits. Use Step Functions to orchestrate multi-step workflows - each step is a Lambda, and Step Functions manages sequencing, parallelization, retries, and state. Use SQS or SNS to chain functions: one function publishes to a queue, another picks up and continues. For data processing, process in batches rather than all at once. Another pattern: the function processes a chunk, then invokes itself asynchronously with a continuation token for the remaining work. For truly long tasks that can't be chunked, use Fargate or ECS - serverless containers without time limits. The chunked approach is actually better for reliability: each piece can retry independently, progress is tracked, and partial failures don't lose all work."],
    },
    {
        text: "How do you handle local development for serverless?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["Local development for serverless can be challenging since you're developing for a cloud environment. I use tools like serverless-offline or AWS SAM CLI that emulate the serverless environment locally. They spin up local API gateways and invoke functions locally, giving you a development experience similar to traditional apps. For AWS services, LocalStack emulates many AWS services locally. I also write functions to be testable independently of the serverless runtime by extracting business logic from handler functions. Unit tests don't need the full serverless environment. For integration testing, I sometimes use a dev stage in the cloud rather than local emulation. The key is separating business logic from the serverless handler code, which makes it easier to test and develop locally while still deploying to serverless infrastructure.",
            "Local dev requires emulating the cloud environment or separating testable logic from the runtime wrapper. Tools like serverless-offline, AWS SAM CLI, or the Serverless Framework's local invoke simulate Lambda and API Gateway locally. LocalStack emulates many AWS services - S3, DynamoDB, SQS - for integration testing without cloud costs. But emulation has limits; some behaviors differ from real AWS. I architect for testability: extract business logic into pure functions that don't depend on Lambda context. The handler is a thin wrapper that parses the event and calls business logic. Those pure functions are unit-testable without any serverless tooling. For integration tests, I often use a dev stage deployed to the actual cloud - it's the most reliable way to catch environment-specific issues. The pattern: fast local unit tests for logic, cloud-deployed dev environment for integration testing."],
    },
    {
        text: "What are the cost implications of serverless vs traditional servers?",
        level: Level.enum.mid,
        category: Category.enum.backend,
        tags: [ValidTag.enum.serverless],
        answers: ["The cost model is completely different. Serverless charges per request and execution time, while traditional servers charge for uptime regardless of usage. For low-traffic applications or those with sporadic usage, serverless is usually cheaper because you only pay for what you use. There's a generous free tier too. For high-traffic applications with consistent load, traditional servers or containers often become more cost-effective because serverless per-request costs add up. The break-even point depends on your traffic patterns. I also factor in operational costs - serverless reduces DevOps overhead, which has value. Hidden costs in serverless include data transfer, API Gateway fees, and keeping functions warm. I analyze expected traffic patterns and calculate costs for both approaches. For startups or variable workloads, serverless often wins. For established apps with predictable high traffic, traditional infrastructure might be cheaper.",
            "Cost models differ fundamentally. Traditional servers: fixed monthly cost regardless of usage - you pay whether it's handling 1 request or 1 million. Serverless: pay per invocation and execution time - zero cost when idle. For sporadic or variable traffic, serverless wins because you don't pay for idle capacity. AWS Lambda's free tier covers a million requests monthly. For steady high traffic, traditional servers become cheaper - serverless per-request costs accumulate. The crossover point varies, but roughly at consistent high utilization, reserved EC2 or containers beat Lambda on raw compute cost. But factor in operational costs: serverless eliminates server management, patching, scaling configuration. That's engineering time saved. Hidden serverless costs: API Gateway charges per request, data transfer fees, provisioned concurrency for low latency. I model expected traffic patterns and calculate both approaches. For startups and variable loads, serverless usually wins. For mature apps with predictable high traffic, containers may be more economical."],
    },
];
