import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.devops,
    typeof Level.enum.mid
>[] = [
    // AWS General
    {
        text: "What are the core AWS services and when would you use each?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["The core AWS services I work with regularly include EC2 for virtual machines when you need full control over the infrastructure, S3 for object storage like static assets or backups, RDS for managed relational databases, Lambda for serverless functions that run on-demand, and VPC for networking. Then you have IAM for access management, CloudWatch for monitoring, and Route 53 for DNS. I'd use EC2 when you need persistent compute with specific configurations, Lambda when you have event-driven workloads or want to avoid server management, S3 for any file storage needs, and RDS when you want a managed database without worrying about patches and backups. The key is matching the service to your workload characteristics and operational requirements.",
            "The foundational services are compute, storage, database, and networking. For compute, EC2 gives you full virtual machines, Lambda runs code without managing servers. For storage, S3 handles object storage like files and backups. For databases, RDS provides managed relational databases like PostgreSQL or MySQL. VPC lets you set up isolated networking. IAM controls who can access what. CloudWatch handles monitoring and logging. Route 53 is for DNS. Each service has its use case - I pick EC2 when I need specific configurations or long-running processes, Lambda for event-driven workloads, S3 for any file storage, RDS when I don't want to manage database infrastructure myself. It's about choosing the right tool for your specific requirements."],
    },
    {
        text: "What is the AWS shared responsibility model?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.security],
        answers: ["The shared responsibility model defines what AWS manages versus what you're responsible for as a customer. AWS handles security 'of' the cloud - so the physical infrastructure, hardware, networking, and the foundational services themselves. You're responsible for security 'in' the cloud - things like your data, IAM configurations, operating system patches on EC2 instances, application security, and network configurations. For example, AWS ensures the physical security of their data centers, but you need to ensure your S3 buckets aren't publicly accessible or your security groups are properly configured. The exact split varies by service type - with managed services like RDS, AWS handles more, whereas with EC2, you handle more. Understanding this model is critical for proper security posture.",
            "It's a division of security responsibilities between you and AWS. AWS secures the underlying infrastructure - physical data centers, networking hardware, the hypervisor, and managed service platforms. You secure everything you put on top - your data, access controls, application code, and configuration. The split point varies by service. With EC2, you're responsible for patching the OS and everything above it. With Lambda, AWS handles much more since you're just providing code. A common mistake is assuming AWS secures everything, but if you misconfigure an S3 bucket or leave a security group wide open, that's on you. Understanding this boundary is essential for compliance and security."],
    },
    {
        text: "What are AWS regions and availability zones?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["AWS regions are separate geographic areas around the world, like us-east-1 in Virginia or eu-west-1 in Ireland. Each region is completely independent and isolated from the others. Within each region, you have multiple availability zones, which are essentially separate data centers with independent power, cooling, and networking. AZs within a region are connected with low-latency links but are far enough apart to be isolated from disasters. The key is that you can deploy your application across multiple AZs for high availability - if one AZ goes down, your app keeps running in another. You choose regions based on factors like latency to your users, data residency requirements, or service availability, since not all services are available in all regions.",
            "Regions are independent geographic locations where AWS operates - like Northern Virginia, London, or Singapore. Each region has multiple availability zones, which are physically separate data centers within that region. AZs have their own power, networking, and cooling, so a failure in one shouldn't affect others. They're connected with high-speed, low-latency links. For high availability, you deploy across at least two AZs - if one has issues, traffic shifts to the other. Region selection matters for latency, compliance, and cost. If your users are in Europe, use a European region. Some industries require data to stay in specific countries. Not all AWS services are available everywhere, so check availability for what you need."],
    },
    {
        text: "What is VPC and how do you configure networking?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.vpc],
        answers: ["A VPC is your own isolated virtual network within AWS where you launch your resources. You define the IP address range using CIDR blocks, then create subnets within that range across different availability zones. Subnets can be public or private - public subnets have a route to an internet gateway, while private subnets don't. You'd typically put web servers in public subnets and databases in private ones. Then you set up route tables to control traffic flow, security groups to act as firewalls at the instance level, and network ACLs for subnet-level security. For connecting to the outside world, you might use an internet gateway for public access, a NAT gateway to let private instances reach the internet, or VPN/Direct Connect for secure connections to your on-premise infrastructure. The key is designing a network that's secure by default.",
            "VPC is your private network in AWS - think of it as your own data center in the cloud. You define an IP range with CIDR notation, then carve it into subnets across availability zones. Public subnets route to an internet gateway and host things like load balancers. Private subnets have no direct internet access and host your databases and application servers. A NAT gateway lets private instances make outbound internet requests without being directly reachable. Security groups act as instance-level firewalls, controlling what traffic can reach each resource. Route tables determine where traffic goes. The standard pattern is web tier in public subnets, app and database tiers in private subnets. This gives you defense in depth and makes your architecture more secure by default."],
    },
    {
        text: "What are security groups and NACLs?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.vpc, ValidTag.enum.security],
        answers: ["Security groups and NACLs are both firewall mechanisms but they work differently. Security groups operate at the instance level and are stateful - meaning if you allow inbound traffic, the response is automatically allowed back out. They only have allow rules, and you specify what traffic is permitted. NACLs work at the subnet level and are stateless, so you need explicit rules for both inbound and outbound traffic. They support both allow and deny rules and are evaluated in order. In practice, I rely heavily on security groups for most access control because they're easier to manage and being stateful is usually what you want. NACLs are more of a backup layer or for specific deny rules. A common pattern is to have a security group for web servers allowing port 443 from anywhere, and another for databases allowing port 5432 only from the web server security group.",
            "Both control network traffic, but at different levels. Security groups are stateful firewalls attached to instances - if you allow inbound traffic on a port, the response is automatically allowed out. You only define allow rules. NACLs are stateless firewalls at the subnet level - you must explicitly allow both inbound and outbound, and they support deny rules too. For most use cases, security groups are sufficient and easier to manage. I use them to allow HTTPS to web servers, and to let web servers connect to the database on a specific port. NACLs are useful as an extra layer or when you need to explicitly block specific IP ranges. The stateful nature of security groups saves a lot of configuration headaches."],
    },
    {
        text: "What is AWS CloudFormation and infrastructure as code?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["CloudFormation is AWS's infrastructure as code service where you define your infrastructure in templates using JSON or YAML. Instead of manually clicking through the console to create resources, you declare what you want in a template and CloudFormation provisions everything for you. The benefits are huge - you get version control for your infrastructure, repeatability across environments, and you can tear down and recreate entire environments consistently. The templates define resources, their properties, and dependencies between them. CloudFormation handles the ordering and manages the state. You can also use features like stack sets to deploy across multiple accounts and regions. The main value is treating infrastructure like code - it's reviewed, tested, and versioned just like application code, which reduces errors and makes your infrastructure reproducible.",
            "CloudFormation lets you define AWS infrastructure in code rather than clicking through the console. You write templates in YAML or JSON describing what resources you want - EC2 instances, databases, networking, IAM roles - and CloudFormation creates them for you in the right order based on dependencies. The big win is reproducibility. Need a new environment? Run the same template. Made a mistake? Rollback or delete the stack. Everything is version controlled so you can track changes and review them like code. It also prevents configuration drift since the template is the source of truth. For teams, this means no more 'I don't remember how I set that up' - the infrastructure definition is documented in the template itself."],
    },

    // EC2
    {
        text: "What is EC2 and how do instance types work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["EC2 is AWS's virtual server service where you can launch instances with different compute, memory, and storage configurations. Instance types follow a naming pattern like t3.medium or m5.large, where the letter indicates the family, the number is the generation, and the size determines the resources. The families are optimized for different workloads - T instances are burstable and good for workloads with variable CPU usage, M instances are general purpose balanced compute and memory, C instances are compute-optimized for CPU-intensive tasks, R instances are memory-optimized for in-memory databases, and so on. You choose instance types based on your application's resource requirements. For example, I'd use a T3 instance for a small web server with occasional traffic, but a C5 instance for heavy data processing or an R5 for caching layers.",
            "EC2 provides virtual servers you can configure however you need. Instance types are named with a family letter, generation number, and size - like m6i.large. The family tells you what it's optimized for: T for burstable general purpose, M for balanced workloads, C for CPU-heavy tasks, R for memory-intensive applications, and so on. Newer generations offer better price-performance. Size goes from nano up through metal, scaling CPU, memory, and network. I pick the type based on the workload - T instances are great for dev environments or apps with variable load, C instances for computation-heavy jobs, R instances for in-memory caching or databases. You can always resize if your initial choice doesn't fit."],
    },
    {
        text: "What are EC2 pricing models (on-demand, reserved, spot)?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["On-demand instances are pay-as-you-go with no commitment - you pay by the hour or second and can start and stop anytime. Reserved instances require a commitment of one or three years but give you significant discounts, up to 75% off on-demand pricing. Spot instances let you bid on unused EC2 capacity at up to 90% off, but AWS can reclaim them with a two-minute warning when they need the capacity back. I use on-demand for unpredictable workloads or short-term needs, reserved instances for baseline capacity that I know will run continuously like production databases, and spot for fault-tolerant or flexible workloads like batch processing or CI/CD jobs. A good strategy is to cover your baseline with reserved instances, handle normal traffic with on-demand, and use spot for burst capacity or non-critical workloads.",
            "There are three main pricing options with different tradeoffs. On-demand is the simplest - pay per hour or second with no commitment. It's the most expensive but gives you full flexibility. Reserved instances commit you for one or three years but cut costs significantly, up to 75% savings. Use these for steady-state workloads you know will run continuously. Spot instances are the cheapest, up to 90% off, but AWS can terminate them with two minutes notice when they need the capacity. They're perfect for batch jobs, CI/CD runners, or anything that can handle interruption. Most organizations use a mix - reserved for baseline, on-demand for variable load, spot for non-critical or fault-tolerant work."],
    },
    {
        text: "What is an AMI?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["An AMI is an Amazon Machine Image, basically a template for launching EC2 instances. It includes the operating system, application server, applications, and any configuration you've baked in. You can use AWS-provided AMIs like Amazon Linux or Ubuntu, find community AMIs, or create your own custom AMIs. Creating custom AMIs is really useful for standardizing your environment - you can configure an instance exactly how you want it, install all your software and dependencies, then create an AMI from it. Now you can launch identical instances quickly without repeating the setup. I typically create golden AMIs for different purposes like web servers or application servers, and update them periodically with security patches. AMIs are also region-specific, so you need to copy them if you want to use them in different regions.",
            "An AMI is a template for launching EC2 instances - it captures the OS, installed software, and configuration. AWS provides base AMIs like Amazon Linux or Ubuntu, and you can create custom ones from running instances. Custom AMIs are valuable for standardization - configure an instance with everything you need, snapshot it as an AMI, and now every new instance starts exactly the same. This speeds up scaling and ensures consistency. I build golden AMIs with security patches and required software baked in, then update them regularly. One thing to remember is that AMIs are regional, so if you need to launch instances in multiple regions, you'll need to copy the AMI to each region first."],
    },
    {
        text: "What is auto-scaling and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2, ValidTag.enum.scalability],
        answers: ["Auto-scaling automatically adjusts the number of EC2 instances based on demand. You create an auto-scaling group that defines the minimum, maximum, and desired number of instances, along with a launch template that specifies what kind of instances to create. Then you set up scaling policies that trigger based on CloudWatch metrics like CPU utilization or request count. For example, you might scale out when average CPU exceeds 70% and scale in when it drops below 30%. You can also use target tracking policies that maintain a specific metric at a target value, or scheduled scaling for predictable load patterns. The key is to configure health checks so unhealthy instances get replaced automatically, and to test your scaling policies to make sure they respond appropriately. Auto-scaling is essential for handling variable traffic while optimizing costs.",
            "Auto-scaling lets your infrastructure grow and shrink based on demand. You define an auto-scaling group with minimum, maximum, and desired instance counts. A launch template specifies what type of instances to create. Scaling policies react to CloudWatch metrics - scale out when CPU hits 70%, scale in when it drops to 30%. Target tracking is simpler - just say 'keep average CPU at 50%' and it figures out the right number of instances. You can also schedule scaling for known patterns like business hours. Health checks automatically replace failed instances. The beauty is you don't overpay for capacity you don't need, and you don't crash under load because more instances spin up automatically."],
    },
    {
        text: "How do you handle EC2 instance storage?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["EC2 instances can have two types of storage: EBS volumes and instance store. EBS volumes are network-attached storage that persists independently of the instance lifecycle - if you stop an instance, the EBS volume retains its data. They come in different types like gp3 for general purpose SSD, io2 for high-performance workloads needing consistent IOPS, or st1 for throughput-optimized HDDs. Instance store is ephemeral storage physically attached to the host - it's very fast but the data is lost if the instance stops or terminates. I use EBS for most workloads because the persistence is critical, and I'll choose gp3 for most use cases since it offers good performance at reasonable cost. For databases or high-IOPS applications, I'd use io2. Instance store is good for temporary data like caches or buffers where you can afford to lose the data.",
            "There are two storage options: EBS and instance store. EBS volumes are network-attached and persist independently of the instance - stop the instance, data stays intact. You pick volume types based on needs: gp3 is the default general-purpose SSD, io2 provides guaranteed IOPS for databases, st1 is HDD for throughput-heavy workloads. Instance store is ephemeral storage directly attached to the physical host - it's very fast but data is lost if the instance stops or terminates. I use EBS for anything that matters because persistence is essential. Instance store is fine for temp files or caches. For databases and production data, always use EBS and configure snapshots for backups."],
    },

    // S3
    {
        text: "What is S3 and what are its use cases?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 is AWS's object storage service, designed for storing and retrieving any amount of data from anywhere. Unlike block storage, it stores data as objects in buckets, where each object has data, metadata, and a unique key. Common use cases include storing static website content, backups and archives, data lakes for analytics, hosting images and videos for applications, and storing application logs. It's incredibly durable with 11 nines of durability and highly available. I use it for things like serving static assets for web apps, storing user uploads, archiving database backups, and as a data source for data processing pipelines. The pay-per-use model makes it cost-effective, and features like lifecycle policies let you automatically transition older data to cheaper storage classes. It's one of the most versatile AWS services.",
            "S3 is object storage for any kind of file - images, videos, backups, logs, you name it. Data is stored as objects in buckets with unique keys. It's designed for durability, offering 11 nines, which essentially means your data won't be lost. Common uses include hosting static website assets, storing user uploads, backup archives, and building data lakes. I use it constantly - it's one of AWS's most fundamental services. The pricing scales with usage, and lifecycle policies automatically move older data to cheaper storage tiers. S3 also integrates with virtually every other AWS service, making it the default choice for file storage in cloud architectures."],
    },
    {
        text: "What are S3 storage classes?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 offers different storage classes optimized for different access patterns and cost requirements. Standard is for frequently accessed data with low latency. Standard-IA is for infrequently accessed data that you still need quick access to when required - it's cheaper but charges for retrieval. One Zone-IA is even cheaper but stores data in a single AZ so it's less resilient. Glacier classes are for archival - Glacier Flexible Retrieval for archives with retrieval times from minutes to hours, and Glacier Deep Archive for long-term retention with retrieval times of 12 hours. There's also Intelligent-Tiering that automatically moves objects between access tiers based on usage patterns. I use Standard for active application data, Standard-IA for things like older backups I might need occasionally, and Glacier for compliance archives. The key is matching the storage class to your access patterns to optimize costs.",
            "Storage classes let you optimize cost based on how often you access data. Standard is for frequently accessed data with instant retrieval. Standard-IA costs less for storage but charges for retrieval, perfect for data you access occasionally. Glacier tiers are for archival - you trade retrieval speed for dramatically lower storage costs. Deep Archive is the cheapest but takes 12 hours to retrieve. Intelligent-Tiering automatically moves objects between tiers based on access patterns, which is great when you're not sure how data will be used. I typically use Standard for active data, set up lifecycle rules to move older data to IA after 30 days, and archive to Glacier after 90 days. This approach balances cost with accessibility."],
    },
    {
        text: "What are S3 bucket policies vs IAM policies?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3, ValidTag.enum.iam],
        answers: ["Both control access to S3, but from different angles. IAM policies are attached to IAM identities like users, groups, or roles, and define what those identities can do across AWS resources. Bucket policies are attached directly to S3 buckets and define who can access that specific bucket and what they can do. The key difference is bucket policies can grant access to principals outside your AWS account, like making a bucket publicly readable or granting access to another AWS account. I use IAM policies to manage what my users and services can do, and bucket policies when I need to grant cross-account access or set bucket-wide permissions. For example, I'd use a bucket policy to allow CloudFront to read from a bucket serving a static website, or to grant another team's AWS account access to specific objects.",
            "IAM policies are identity-centric - they define what a user or role can do. Bucket policies are resource-centric - they define who can access that specific bucket. The key distinction is that bucket policies can grant access to principals outside your account, which IAM policies cannot. If you need cross-account access or public access, you need a bucket policy. For most internal access control, I use IAM policies on roles. For special cases like allowing CloudFront to access a bucket or sharing with another team's AWS account, I add a bucket policy. Both are evaluated together - access is granted only if allowed by both applicable policies and denied by neither."],
    },
    {
        text: "What is S3 versioning?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 versioning keeps multiple versions of an object in the same bucket. When enabled, every time you upload an object with the same key, S3 creates a new version instead of overwriting the existing one. This protects against accidental deletions and overwrites. If you delete an object, S3 just adds a delete marker but the previous versions remain. You can restore any previous version when needed. I enable versioning on buckets storing critical data like backups or important application files. It's also useful during deployments - if you upload a bad version of a static asset, you can easily roll back to a previous version. The tradeoff is you pay for storage of all versions, so I typically combine it with lifecycle policies to delete old versions after a certain period.",
            "Versioning preserves every version of every object in a bucket. Instead of overwriting files, S3 keeps all previous versions accessible. Delete something accidentally? You can restore it because the old versions still exist. This is great protection against human error and even ransomware - you can always recover previous state. When you delete a versioned object, S3 adds a delete marker rather than actually removing data. To fully delete, you must remove specific versions. The downside is storage costs accumulate since you're keeping all versions. I pair versioning with lifecycle rules to automatically delete old versions after a retention period, balancing protection with cost."],
    },
    {
        text: "What is S3 lifecycle management?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Lifecycle management lets you automatically transition objects between storage classes or delete them based on rules you define. You create lifecycle policies with rules that say things like 'move objects to Standard-IA after 30 days' or 'transition to Glacier after 90 days and delete after one year.' You can apply rules to all objects or filter by prefix or tags. This is really valuable for managing costs - instead of manually moving old data to cheaper storage, it happens automatically. For example, I set up policies for log buckets to move logs to IA after 30 days and to Glacier after 90 days, then delete them after a year. For versioned buckets, you can also have rules to delete old versions after a certain period. It's a set-it-and-forget-it way to optimize storage costs over time.",
            "Lifecycle rules automate storage management. You define rules that transition objects between storage classes or delete them based on age. For example: move to Standard-IA after 30 days, to Glacier after 90 days, delete after a year. Rules can target all objects or filter by prefix or tags. This is essential for cost optimization because you don't want to pay Standard prices for data nobody accesses. I set up lifecycle policies on every bucket - logs get archived and eventually deleted, old backups move to Glacier, temporary files get cleaned up. For versioned buckets, rules can also delete old versions. Once configured, it just works without manual intervention."],
    },
    {
        text: "How do you configure S3 for static website hosting?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["To host a static website on S3, you enable static website hosting on the bucket and specify your index document like index.html and optionally an error document. Then you upload your HTML, CSS, JavaScript, and other static assets to the bucket. The critical step is setting the right permissions - you need a bucket policy that allows public read access to the objects, since by default S3 buckets are private. S3 gives you a website endpoint URL that you can use to access your site. For production, I usually put CloudFront in front of it to get HTTPS support, better performance with edge caching, and the ability to use a custom domain with Route 53. This setup is great for single-page applications, documentation sites, or any static content. It's highly available, scalable, and very cost-effective compared to running web servers.",
            "Enable static website hosting in the bucket settings, specifying your index and error documents. Upload your static files - HTML, CSS, JavaScript, images. The key step is the bucket policy granting public read access since S3 is private by default. S3 provides a website endpoint URL for access. For production, I always add CloudFront in front for HTTPS, edge caching, and custom domains. This combination is ideal for SPAs, documentation sites, and static content - it's infinitely scalable, highly available, and costs almost nothing compared to running servers. You're leveraging AWS infrastructure without managing any servers yourself."],
    },
    {
        text: "What are presigned URLs and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Presigned URLs give temporary access to private S3 objects without requiring AWS credentials. You generate a URL using your credentials that includes an expiration time, and anyone with that URL can perform the specified action like downloading or uploading a file until it expires. This is really useful for giving users temporary access to files. For example, if users upload profile pictures to your app, you can store them in a private S3 bucket, then generate presigned URLs when users need to view them. Or for file downloads, you generate a presigned URL that expires in a few minutes. This way your bucket stays private and secure, but users can still interact with objects through your application. I typically set short expiration times, like 5-15 minutes, to minimize security risks if URLs get leaked.",
            "Presigned URLs let you grant temporary access to private S3 objects. Your server generates a URL with embedded credentials and an expiration time. Anyone with that URL can access the object until it expires - no AWS credentials needed on their end. This is perfect for file downloads and uploads in applications. Keep the bucket private, generate presigned URLs on demand. For downloads, I might give users a URL valid for 5 minutes. For uploads, the URL authorizes putting an object to a specific key. The bucket stays secure while users interact with specific objects. I keep expiration times short to limit exposure if a URL gets shared."],
    },
    {
        text: "How do you handle cross-region replication?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Cross-region replication automatically copies objects from a source bucket to a destination bucket in a different region. You need versioning enabled on both buckets, then you create a replication rule that specifies what to replicate - all objects or filtered by prefix or tags. You also set up an IAM role that gives S3 permission to replicate on your behalf. Objects uploaded after enabling replication are automatically copied; existing objects need a separate batch operation. I use cross-region replication for disaster recovery, to reduce latency by having data closer to users in different regions, or for compliance requirements around data locality. For example, you might replicate critical application data to another region so if the primary region has issues, you can failover. The data is replicated asynchronously, usually within minutes.",
            "Cross-region replication copies objects to a bucket in another region automatically. Both buckets need versioning enabled. You create replication rules specifying what to copy - everything, or filtered by prefix or tags. An IAM role authorizes S3 to perform the replication. New objects replicate automatically; existing objects need a batch job. I use this for disaster recovery - if your primary region has issues, your data exists elsewhere. It's also useful for serving content closer to users in different geographies. Replication is asynchronous, typically completing within minutes. Keep in mind you're paying for storage and data transfer, so be thoughtful about what you replicate."],
    },
    {
        text: "What is the difference between S3 and EBS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 is object storage accessed via HTTP APIs, while EBS is block storage that attaches to EC2 instances like a hard drive. S3 is designed for storing files you access occasionally over the network, with virtually unlimited storage and 11 nines of durability. EBS is designed for high-performance workloads that need low-latency block-level access, like databases or operating system volumes, but an EBS volume can only attach to one instance at a time in the same availability zone. I use S3 for backups, static assets, data lakes, and file storage - anywhere I need durable, scalable object storage. I use EBS for database storage, application file systems, and anything needing consistent IOPS or low latency. They're complementary - you might run a database on EBS and backup to S3.",
            "They serve different purposes. S3 is object storage accessed via HTTP - great for files, backups, static content. It scales infinitely and offers extreme durability. EBS is block storage attached to EC2 - it's like a virtual hard drive with low latency, used for OS volumes and databases. S3 is accessed over the network from anywhere; EBS is tied to a specific instance in a specific AZ. I use EBS for anything needing fast disk access like databases, and S3 for everything else - file uploads, backups, logs, static assets. They work together well: database runs on EBS, backups go to S3."],
    },

    // Lambda
    {
        text: "What is AWS Lambda?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.serverless],
        answers: ["Lambda is AWS's serverless compute service where you run code without managing servers. You upload your function code, specify the runtime like Node.js or Python, and Lambda handles all the infrastructure - provisioning, scaling, patching, and high availability. You only pay for the compute time when your code is actually running. Lambda functions are event-driven, triggered by things like HTTP requests via API Gateway, file uploads to S3, messages in SQS, or scheduled events. Each function has a handler that processes events and can run for up to 15 minutes. It's great for microservices, API backends, data processing, automation tasks, and anything with variable or unpredictable load. The main advantages are no server management, automatic scaling, and cost efficiency for workloads that don't run continuously.",
            "Lambda runs your code without you managing any servers. Upload a function, configure a trigger, and AWS handles everything else - scaling, availability, patching. You pay only for execution time, down to the millisecond. Functions respond to events: HTTP requests through API Gateway, S3 uploads, SQS messages, scheduled events. They can run up to 15 minutes per invocation. I use Lambda for APIs, event processing, scheduled tasks, and anything that doesn't need to run constantly. The scaling is automatic and nearly instant. If you go from zero to thousands of concurrent requests, Lambda handles it. It's ideal for variable workloads where you don't want to pay for idle servers."],
    },
    {
        text: "What are Lambda triggers?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda triggers are the event sources that invoke your Lambda functions. Common triggers include API Gateway for HTTP requests, S3 for object events like uploads or deletions, DynamoDB Streams for database changes, SQS for processing messages from queues, SNS for pub-sub notifications, EventBridge for scheduled or custom events, and CloudWatch Events for AWS service events. Each trigger passes event data to your function in a specific format. For example, an S3 trigger includes details about the bucket and object key, while an API Gateway trigger includes the HTTP request details. You configure triggers when you set up your function, and Lambda handles polling or listening for events automatically. A single function can have multiple triggers, or you can have different functions for different event types.",
            "Triggers are what invoke your Lambda functions. They're event sources that tell Lambda when to run your code. API Gateway triggers on HTTP requests. S3 triggers on object uploads or deletions. SQS triggers when messages arrive in a queue. EventBridge triggers on schedules or custom events. DynamoDB Streams trigger on database changes. Each trigger type has its own event format that gets passed to your function. I configure triggers based on what should start the function - user HTTP requests, file uploads, messages in a queue, or a cron schedule. You can attach multiple triggers to one function, making it respond to different event types."],
    },
    {
        text: "How do you handle Lambda cold starts?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.performance],
        answers: ["Cold starts happen when Lambda needs to spin up a new execution environment for your function, which adds latency to the first request. To minimize impact, I keep functions small and minimize dependencies - fewer packages mean faster initialization. Moving initialization code outside the handler function means it only runs once per container, not per invocation. Choosing faster languages helps too - compiled languages like Go or newer Node.js runtimes start quicker than Java. For critical paths, you can use provisioned concurrency which keeps execution environments warm and ready, though it costs more. Increasing memory allocation also gives more CPU which can speed up initialization. In practice, I optimize code and accept some cold starts for infrequent functions, and use provisioned concurrency only for latency-sensitive user-facing APIs.",
            "Cold starts are the latency hit when Lambda initializes a new execution environment. Strategies to minimize them: keep deployment packages small with minimal dependencies, put initialization code outside the handler so it runs once per container, use faster runtimes like Go or Node.js over Java, and allocate more memory which also increases CPU. For latency-critical paths, provisioned concurrency keeps environments warm and ready, eliminating cold starts at additional cost. I accept cold starts for background tasks since users don't notice, but use provisioned concurrency for user-facing APIs where latency matters. The key is understanding your workload and applying the right strategy."],
    },
    {
        text: "What are Lambda layers?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda layers are packages of libraries, custom runtimes, or other dependencies that you can share across multiple functions. Instead of bundling the same dependencies with every function, you create a layer once and attach it to multiple functions. This reduces deployment package size and makes it easier to share common code. For example, I create layers for commonly used libraries or SDKs, custom utility functions that multiple functions need, or monitoring tools. Layers are versioned and extracted to the /opt directory in the execution environment. You can attach up to five layers per function. This is especially useful for large dependencies that would otherwise make deployment packages unwieldy, and it makes updates easier - change the layer once instead of updating every function.",
            "Layers let you package dependencies separately from your function code. Create a layer with your common libraries, SDKs, or utilities, then attach it to multiple functions. This shrinks deployment packages and centralizes shared code. Update the layer once and all functions using it get the new version. Layers are versioned, extracted to /opt at runtime, and you can stack up to five per function. I use layers for things like monitoring SDKs, common utilities, or large dependencies that would bloat every function's package. It simplifies management when the same code is needed across many functions."],
    },
    {
        text: "How do you configure Lambda memory and timeout?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["You configure Lambda memory from 128 MB to 10 GB, and timeout from 1 second to 15 minutes. The key insight is that memory allocation also determines CPU power - more memory gives you proportionally more CPU and network bandwidth. So sometimes increasing memory can actually reduce costs by making your function run faster and finish in less time. I start with the default 128 MB and test, then increase if the function is timing out or running slow. For CPU-intensive tasks, more memory helps significantly. For timeout, I set it based on expected execution time plus buffer - if a function normally takes 2 seconds, I might set a 10 second timeout. You don't want it too high because you pay for execution time, but too low and legitimate requests might fail. Monitoring helps tune these values over time.",
            "Memory ranges from 128 MB to 10 GB, timeout from 1 second to 15 minutes. The important thing to know is memory also controls CPU allocation - double the memory, double the CPU. Sometimes paying for more memory reduces total cost because the function finishes faster. I start low and increase based on actual performance. For timeouts, I set them above typical execution time with some buffer. Too short and valid requests fail; too long and you waste money on stuck functions. I use CloudWatch metrics to find the right balance - look at actual duration and memory usage, then tune accordingly."],
    },
    {
        text: "What is API Gateway and how does it integrate with Lambda?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["API Gateway is AWS's fully managed service for creating REST and WebSocket APIs. It acts as the front door for your Lambda functions, handling HTTP requests and routing them to the appropriate function. You define resources and methods like GET /users or POST /orders, and map each to a Lambda function. API Gateway handles authentication, rate limiting, request validation, and response transformation. It also provides features like API keys, usage plans, and custom domain names. The integration is straightforward - API Gateway invokes your Lambda function with the HTTP request details, your function processes it and returns a response, and API Gateway sends that back to the client. Together they make a powerful serverless API solution with no servers to manage, automatic scaling, and you only pay for actual requests.",
            "API Gateway is the HTTP front door for Lambda functions. You define routes like GET /users or POST /orders and map them to Lambda functions. When requests come in, API Gateway invokes your function with the request data, your code processes it, and the response goes back to the client. It handles the HTTP layer concerns - authentication, rate limiting, CORS, request validation - so your Lambda just focuses on business logic. You get features like custom domains, API keys, and usage plans. Together, Lambda and API Gateway give you a fully serverless API that scales automatically with no servers to manage. You pay per request and compute time only."],
    },
    {
        text: "What are Lambda environment variables and secrets management?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda environment variables let you pass configuration to your function without hardcoding values. You set key-value pairs in the Lambda configuration and access them in your code through process.env or equivalent. Lambda automatically encrypts them at rest using KMS. For sensitive data like API keys or database passwords, I use AWS Secrets Manager or Parameter Store instead of environment variables. Your function retrieves secrets at runtime using the AWS SDK. This keeps secrets out of your code and configurations, and you can rotate them without redeploying functions. You can also cache secrets within your function to reduce API calls. Some teams use environment variables for non-sensitive config like feature flags or region settings, and Secrets Manager for credentials. The key is never committing secrets to code and using appropriate encryption for sensitive data.",
            "Environment variables pass configuration to functions without hardcoding. Set them in Lambda config, access via process.env. They're encrypted at rest by default. For sensitive data like database passwords or API keys, I use Secrets Manager instead - it supports rotation and audit logging. Your function fetches secrets at runtime using the SDK, caching them for efficiency. This keeps credentials out of code and environment variable configurations that might be visible in the console. I use env vars for non-sensitive config like feature flags or endpoints, Secrets Manager for actual credentials. Never put secrets in code or version control."],
    },
    {
        text: "How do you monitor Lambda functions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.monitoring],
        answers: ["Lambda automatically integrates with CloudWatch for monitoring. Execution logs go to CloudWatch Logs, and metrics like invocations, duration, errors, and throttles go to CloudWatch Metrics. I set up CloudWatch alarms on error rates and duration to catch issues. For deeper insights, I use X-Ray for distributed tracing to see how requests flow through my functions and identify bottlenecks. I also add custom logging with structured JSON so I can search and analyze logs easily. For production, I instrument functions with custom metrics using CloudWatch embedded metrics or third-party tools like Datadog. The key metrics I watch are invocation count, error rate, duration, throttles, and concurrent executions. I also monitor costs since Lambda charges per invocation and duration. Good monitoring helps you understand function behavior and troubleshoot issues quickly.",
            "Lambda sends logs to CloudWatch Logs and metrics to CloudWatch Metrics automatically. Key metrics are invocations, duration, errors, and throttles. I set up alarms for error rates and unusual duration spikes. For tracing requests through multiple services, X-Ray shows the full picture. I use structured JSON logging so logs are searchable and parseable. For production, I track custom business metrics alongside the built-in ones. I also watch concurrent execution limits to avoid throttling. The combination of logs, metrics, and traces gives you full visibility into function behavior. Third-party tools like Datadog integrate well if you want unified monitoring."],
    },

    // IAM
    {
        text: "What is IAM and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam, ValidTag.enum.security],
        answers: ["IAM is AWS's Identity and Access Management service that controls who can access what in your AWS account. It works on a deny-by-default model - everything is blocked unless explicitly allowed. You create identities like users for individuals, groups to organize users, and roles for applications or services. Then you attach policies that define permissions using JSON documents. When a request comes in, IAM evaluates all applicable policies to determine if it's allowed. Policies specify actions like s3:GetObject, resources using ARNs, and optionally conditions like IP address or time of day. IAM is fundamental to AWS security - you use it to give developers access to specific resources, allow EC2 instances to access S3, or let Lambda functions write to DynamoDB. The key is following least privilege - only grant the minimum permissions needed.",
            "IAM controls who can do what in your AWS account. It's deny-by-default - nothing is allowed unless you explicitly permit it. You create users for people, groups to organize users, and roles for services and applications. Policies are JSON documents defining what actions are allowed on what resources. IAM evaluates all applicable policies on every request. For security, I give users console access, services get roles with specific permissions, and I always follow least privilege - only grant what's actually needed. IAM is foundational to AWS security. If you get IAM wrong, everything else is vulnerable."],
    },
    {
        text: "What are IAM users, groups, and roles?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM users represent individual people or services with permanent credentials like passwords or access keys. Groups are collections of users that you can attach policies to, making it easier to manage permissions - like a developers group or admins group. Roles are identities that can be assumed temporarily by users, applications, or AWS services. The key difference is roles don't have permanent credentials - they use temporary security credentials when assumed. I create users for people who need AWS console or CLI access, use groups to organize users by job function, and use roles for applications running on EC2 or Lambda. Roles are more secure than access keys because the credentials rotate automatically. For example, an EC2 instance assumes a role to get temporary credentials to access S3, rather than storing access keys on the instance.",
            "Users are for individuals - they have permanent credentials like passwords for console or access keys for CLI. Groups organize users so you can apply policies to many users at once - developers group, admins group. Roles are for services or for temporary access - they provide temporary credentials rather than permanent ones. I use users for human access, groups to organize permissions by job function, and roles for everything else. EC2 instances assume roles, Lambda functions assume roles. Roles are preferred over access keys because credentials rotate automatically and can't be leaked like static keys can."],
    },
    {
        text: "What are IAM policies and how do you write them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM policies are JSON documents that define permissions. They have statements with an effect of Allow or Deny, actions that specify what API calls are permitted, and resources that define what the actions apply to using ARNs. You can also add conditions for more granular control. For example, a policy might allow s3:GetObject on a specific bucket, or allow ec2:StartInstance but only during business hours. I write policies by starting with AWS managed policies as templates, then customize for specific needs. The key is being specific - instead of allowing all S3 actions, specify exactly what's needed like GetObject and PutObject. I use wildcards carefully and test policies with the IAM policy simulator before deploying. Good policy design follows least privilege and makes intent clear through naming and structure.",
            "Policies are JSON documents with statements defining permissions. Each statement has an Effect (Allow or Deny), Actions (what API calls), and Resources (what AWS resources via ARNs). Optional conditions add constraints like IP restrictions or MFA requirements. I start from AWS managed policies as templates, then narrow them down for specific needs. Be explicit about actions rather than using wildcards. If a Lambda only reads from one S3 bucket, the policy allows s3:GetObject on that specific bucket ARN only. I test policies with the IAM policy simulator before deploying. The goal is least privilege - just enough access to do the job, nothing more."],
    },
    {
        text: "What is the principle of least privilege in IAM?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam, ValidTag.enum.security],
        answers: ["Least privilege means giving users and services only the minimum permissions they need to do their job, nothing more. Instead of granting broad permissions like full S3 access, you'd give access to only the specific buckets and actions required. This limits the damage if credentials are compromised. In practice, I start by granting no permissions, then add what's needed as requirements become clear. I avoid using wildcard permissions in production and regularly audit to remove unused permissions. For example, if a Lambda function only reads from one DynamoDB table, its role should allow only GetItem and Query on that specific table, not all DynamoDB actions on all tables. IAM Access Analyzer helps identify overly permissive policies. The challenge is balancing security with developer productivity, but least privilege is essential for a strong security posture.",
            "Grant only what's necessary, nothing more. If a service only needs to read from one S3 bucket, don't give it access to all buckets. If it only needs GetObject, don't allow PutObject or DeleteObject. This limits blast radius - if credentials are compromised, the attacker can only do what that role could do. I start with zero permissions and add specifically what's needed. I audit regularly to remove unused access. AWS provides tools like Access Analyzer to find overly permissive policies. It's more work upfront than broad permissions, but it's fundamental to security. A single misconfigured role with admin access can compromise your entire account."],
    },
    {
        text: "What are IAM conditions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM conditions add context-based logic to policies, letting you allow or deny actions based on factors beyond just who and what. You use condition operators like StringEquals, IpAddress, or DateGreaterThan with condition keys like aws:SourceIp, aws:CurrentTime, or resource tags. For example, you can allow EC2 actions only from your office IP range, require MFA for sensitive operations, or allow deleting objects only if they have a specific tag. I use conditions to enforce security requirements like ensuring S3 uploads are encrypted, restricting actions to specific times or locations, or requiring encryption in transit. Conditions make policies more dynamic and secure. They're powerful but need to be tested carefully to avoid accidentally blocking legitimate access.",
            "Conditions add context-aware rules to policies. Beyond who and what action, you can specify when and how. Common uses include restricting access to specific IP ranges, requiring MFA for sensitive operations, enforcing encryption on S3 uploads, or limiting actions to business hours. You use operators like IpAddress, StringEquals, or Bool with condition keys like aws:SourceIp, aws:MultiFactorAuthPresent, or s3:x-amz-server-side-encryption. Conditions give you fine-grained control that goes beyond simple allow or deny. I use them heavily for security requirements - if the condition isn't met, the action is denied even if the user has the permission."],
    },
    {
        text: "What is the difference between identity-based and resource-based policies?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["Identity-based policies attach to IAM identities like users, groups, or roles and define what those identities can do. Resource-based policies attach to resources like S3 buckets or Lambda functions and define who can access those resources. The key difference is perspective - identity-based says 'this user can access these resources', while resource-based says 'these users can access this resource'. Resource-based policies are useful for cross-account access since you can grant access to principals from other accounts. For example, an S3 bucket policy is resource-based and can allow another account's users to access it. IAM policies on a user are identity-based. In many cases you need both - the identity needs permission to perform the action, and the resource needs to allow access to that identity.",
            "It's about perspective. Identity-based policies attach to users or roles and say what they can do. Resource-based policies attach to resources like S3 buckets and say who can access them. The big difference is that resource-based policies can grant cross-account access - you can allow principals from other AWS accounts. For example, an S3 bucket policy can allow another team's account to read from it, something identity policies can't do. Within an account, you often need both to align - the role needs permission for the action, and the resource needs to allow that role. For cross-account access, resource-based policies are essential."],
    },
    {
        text: "How do you audit IAM permissions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["I use several tools to audit IAM permissions regularly. IAM Access Analyzer identifies resources shared with external entities and helps find overly permissive policies. Credential reports show all users and their credential status, helping identify unused accounts. CloudTrail logs all API calls so you can see what actions are actually being used. IAM's Last Accessed information shows when policies or permissions were last used, making it easy to identify and remove unused permissions. I also use the IAM policy simulator to test whether specific actions would be allowed before deploying policies. For automation, I set up automated scans to check for things like users with console access but no MFA, unused access keys older than 90 days, or overly broad permissions. Regular audits help maintain security and ensure you're following least privilege.",
            "AWS provides several tools for auditing. Access Analyzer finds overly permissive policies and external access. Credential reports show all users and their access key status. Last Accessed data shows when permissions were actually used, helping identify unused access to remove. CloudTrail logs every API call for forensics. Policy Simulator tests what actions would be allowed. I run regular audits checking for MFA on all users, access keys older than 90 days, unused permissions, and overly broad policies. Automation is key - manual audits don't scale. I set up alerts for suspicious patterns like new admin users or external resource sharing."],
    },

    // RDS
    {
        text: "What is RDS and how does it differ from self-managed databases?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["RDS is AWS's managed relational database service supporting engines like MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. The main difference from self-managed databases on EC2 is that AWS handles the operational heavy lifting - automated backups, patching, failover, and scaling. With RDS, you don't manage the underlying infrastructure or database software, you just configure what you need and AWS keeps it running. You lose some flexibility like SSH access to the host or certain advanced configurations, but you gain reliability and reduced operational overhead. I choose RDS when I want to focus on the application rather than database administration, which is most of the time. I'd only self-manage on EC2 if I need specific database configurations that RDS doesn't support or have very specialized requirements.",
            "RDS gives you managed relational databases - MySQL, PostgreSQL, Oracle, SQL Server, MariaDB. AWS handles the undifferentiated heavy lifting: backups, patches, failover, scaling. You just configure your database and use it. Compared to running your own database on EC2, you trade some low-level control for dramatically reduced operational burden. No SSH access, no OS management, but also no 3am pages about failed disks. I use RDS for almost all relational database needs. The only time I'd self-manage is for exotic configurations RDS doesn't support, which is rare. The reliability and reduced ops overhead are worth the tradeoff."],
    },
    {
        text: "What are RDS Multi-AZ deployments?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum["high-availability"]],
        answers: ["Multi-AZ deployments provide high availability by automatically replicating your database to a standby instance in a different availability zone. RDS synchronously replicates data to the standby, so if the primary instance fails, RDS automatically fails over to the standby with minimal downtime, usually under a minute. The standby isn't used for read traffic - it's purely for failover. This protects against AZ failures, instance failures, and even during maintenance operations like patching. I enable Multi-AZ for production databases where downtime is unacceptable. The tradeoff is additional cost for the standby instance and slightly higher write latency due to synchronous replication. For development environments where some downtime is acceptable, I skip Multi-AZ to save costs. The failover is automatic and doesn't require changing connection strings.",
            "Multi-AZ replicates your database synchronously to a standby in another availability zone. If the primary fails, RDS automatically fails over to the standby in under a minute. The standby handles no traffic until failover - it's purely for redundancy. This protects against hardware failures, AZ outages, and even allows maintenance without downtime. I enable it for all production databases. The cost is roughly double since you're running two instances, plus slightly higher latency for writes due to synchronous replication. For dev or staging where brief downtime is acceptable, I skip it to save money. Failover is transparent to applications - same endpoint, no code changes needed."],
    },
    {
        text: "What are RDS read replicas?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum.scalability],
        answers: ["Read replicas are copies of your database that you can use to scale read traffic. RDS asynchronously replicates data from the source database to one or more read replicas, which means there can be some replication lag. You direct read queries to the replicas to offload the primary database, which is useful for read-heavy workloads. You can create up to five read replicas per source instance, and they can be in different regions for disaster recovery or reducing latency for global users. Unlike Multi-AZ standbys, read replicas actively serve traffic. You can also promote a read replica to become a standalone database. I use read replicas when I have analytics queries or reporting that would impact production performance, or when I need to serve read traffic from multiple geographic regions. The main limitation is replication lag - reads might be slightly behind writes.",
            "Read replicas scale out read capacity by replicating data asynchronously to secondary instances. Unlike Multi-AZ standbys, replicas actively serve read traffic. Send reporting queries or heavy reads to replicas to offload the primary. You can have up to five replicas, even in different regions for global performance or disaster recovery. The tradeoff is replication lag - replicas might be slightly behind, so you can't use them for data that must be perfectly current. Replicas can also be promoted to standalone databases for migration or DR scenarios. I use them whenever read load is high or I need to isolate analytics from production transactions."],
    },
    {
        text: "How does RDS backup and recovery work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["RDS provides automated backups and manual snapshots. Automated backups happen daily during a backup window you specify, and RDS also captures transaction logs throughout the day. This lets you restore to any point in time within your retention period, which can be 1 to 35 days. The backup process uses the standby instance in Multi-AZ deployments to avoid I/O impact. Manual snapshots are user-initiated and you keep them until you explicitly delete them - useful before major changes or for long-term retention. To restore, you create a new RDS instance from a snapshot or to a specific point in time. You can't restore over an existing instance. I typically set a 7-day retention for automated backups, take manual snapshots before major migrations, and copy critical snapshots to another region for disaster recovery. Backups are stored in S3 but managed by RDS.",
            "RDS automates backups with daily snapshots plus continuous transaction log backup. This enables point-in-time recovery to any second within your retention period, up to 35 days. Manual snapshots persist until you delete them - use these before migrations. To restore, you create a new instance; you can't restore over existing. For Multi-AZ, backups use the standby to avoid impacting production. I set retention based on compliance needs, typically 7-14 days. Copy critical snapshots to another region for disaster recovery. Always test restores occasionally - untested backups aren't reliable backups."],
    },
    {
        text: "How do you scale RDS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum.scalability],
        answers: ["You can scale RDS vertically or horizontally. Vertical scaling means changing the instance type to get more CPU, memory, or I/O capacity - this requires downtime unless you're using Multi-AZ. You can also scale storage capacity and IOPS independently. Horizontal scaling for reads uses read replicas to distribute read traffic across multiple databases. For writes, you're limited to the capacity of the primary instance, though Aurora has some additional options. I also scale by optimizing queries, adding indexes, and using caching to reduce database load. For predictable load changes, I schedule scaling during maintenance windows. For variable workloads, Aurora Serverless can automatically scale capacity. The key is monitoring metrics like CPU, connections, and IOPS to know when to scale, and load testing to validate that scaling improves performance as expected.",
            "Vertical scaling changes instance type for more CPU and memory - causes brief downtime unless Multi-AZ. You can also increase storage and IOPS independently. Horizontal read scaling uses read replicas to spread read load. Write scaling is harder - you're limited to primary instance capacity, though Aurora offers more options. Beyond infrastructure, optimize queries, add indexes, and add caching layers like ElastiCache. Monitor CPU, memory, connections, and IOPS to know when scaling is needed. Often query optimization gives bigger gains than throwing hardware at it. Aurora Serverless auto-scales for variable workloads without manual intervention."],
    },
    {
        text: "What is Amazon Aurora and how does it differ from RDS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["Aurora is AWS's cloud-native database engine that's MySQL and PostgreSQL compatible, but it's built from the ground up for the cloud. The main differences are performance and architecture - Aurora uses a distributed storage layer that replicates data six ways across three AZs automatically, making it much more resilient. It's faster than standard MySQL or PostgreSQL on RDS, with up to 5x throughput for MySQL and 3x for PostgreSQL. Aurora supports up to 15 read replicas with lower lag, and features like Aurora Serverless that automatically scale capacity. The storage automatically grows up to 128 TB. The tradeoff is cost - Aurora is more expensive than standard RDS. I choose Aurora for production workloads that need high performance, availability, and scale, and standard RDS for less demanding workloads where cost is a bigger concern.",
            "Aurora is AWS's cloud-native database with MySQL and PostgreSQL compatibility. The big difference from standard RDS is the storage layer - it replicates six ways across three AZs automatically, offering much higher durability and faster failover. Performance is significantly better, up to 5x MySQL throughput. You get up to 15 read replicas with very low replication lag, and storage auto-grows to 128TB. Aurora Serverless scales compute automatically for variable workloads. It costs more than standard RDS but provides enterprise-grade performance and reliability. I use Aurora for production workloads needing high availability and performance, standard RDS when cost matters more than those features."],
    },

    // Cognito
    {
        text: "What is AWS Cognito?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.auth],
        answers: ["Cognito is AWS's managed authentication and user management service. It handles user sign-up, sign-in, and access control for web and mobile apps without you having to build auth infrastructure from scratch. The main components are user pools for authentication and user directories, and identity pools for granting AWS credentials to users. Cognito supports email/password authentication, social login through providers like Google and Facebook, and SAML federation for enterprise identity providers. It includes features like MFA, password policies, account recovery, and email verification. I use Cognito when building apps that need user authentication because it's secure out of the box, scales automatically, and integrates well with other AWS services. It handles the security complexities so you can focus on your application logic.",
            "Cognito handles user authentication so you don't build it yourself. User pools manage sign-up, sign-in, and user profiles. Identity pools grant AWS credentials to authenticated users. It supports username/password, social logins like Google and Facebook, and enterprise SAML providers. Features include MFA, password policies, email verification, and account recovery. I use Cognito for most apps needing auth because it's secure by default and scales automatically. Building authentication from scratch is error-prone and time-consuming. Cognito handles the hard security work - password hashing, token management, rate limiting - while integrating smoothly with other AWS services."],
    },
    {
        text: "What are user pools vs identity pools?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito],
        answers: ["User pools are user directories that handle authentication - sign-up, sign-in, password management, and user profiles. They give you JWT tokens when users authenticate successfully. Identity pools handle authorization by providing temporary AWS credentials so authenticated users can access AWS resources like S3 or DynamoDB. You typically use both together - the user pool authenticates the user and issues tokens, then the identity pool exchanges those tokens for AWS credentials with specific IAM permissions. For example, a mobile app might use a user pool to let users log in, then use an identity pool to give them temporary credentials to upload photos to S3. User pools are about who the user is, identity pools are about what AWS resources they can access.",
            "User pools authenticate users and manage their accounts - sign-up, sign-in, profiles, password recovery. They issue JWT tokens proving the user's identity. Identity pools are different - they exchange those tokens for temporary AWS credentials, allowing users to directly access AWS services like S3 or DynamoDB. Think of user pools as 'who is this user?' and identity pools as 'what AWS resources can they access?' A typical flow: user logs in via user pool, gets JWT tokens, app exchanges tokens at identity pool for AWS credentials, user can now upload to S3 directly. You often use both together but they serve distinct purposes."],
    },
    {
        text: "How does Cognito authentication flow work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.auth],
        answers: ["The standard flow starts when a user signs up or signs in through your app. For sign-up, the user provides credentials and Cognito creates the account, optionally sending verification codes via email or SMS. For sign-in, Cognito validates the credentials and returns JWT tokens - an ID token with user attributes, an access token for authorizing API calls, and a refresh token for getting new tokens. Your app includes the ID or access token in requests to your backend, which validates the token signature to confirm it's legitimate. If you're using identity pools, you exchange the user pool token for temporary AWS credentials. The tokens expire after a set time, and you use the refresh token to get new ones without making the user log in again. The flow handles challenges like MFA or password changes as part of the authentication process.",
            "User submits credentials, Cognito validates them and returns three JWT tokens: ID token with user info, access token for API authorization, and refresh token for getting new tokens without re-login. Your app attaches the access or ID token to API requests. Your backend validates the token signature using Cognito's public keys. If using identity pools, exchange the token for temporary AWS credentials. Tokens expire after configurable periods - use the refresh token to get fresh ones silently. The flow handles MFA challenges, password changes, and verification codes as needed. It's a standard OAuth/OIDC flow with AWS integration."],
    },
    {
        text: "How do you implement social login with Cognito?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.oauth],
        answers: ["To implement social login, you configure identity providers in your Cognito user pool by registering your app with providers like Google, Facebook, or Amazon and getting OAuth credentials. You add these credentials to Cognito and specify which attributes to map from the provider to your user pool. When users choose social login, they're redirected to the provider's login page, authenticate there, and get redirected back to your app with tokens. Cognito creates or updates the user in the pool and issues its own tokens. The user can then log in with either their social account or a password if you've enabled both. I typically enable multiple social providers to give users options, and use attribute mapping to ensure consistent user data regardless of how they sign in. Cognito handles the OAuth flow complexity automatically.",
            "Register your app with social providers like Google or Facebook and get OAuth credentials. Configure these in Cognito, mapping provider attributes to your user pool attributes. Users click 'Sign in with Google,' get redirected to Google's login, authenticate there, then redirect back. Cognito handles the token exchange, creates or links the user in your pool, and issues its own JWT tokens. You can support multiple providers simultaneously. Cognito abstracts the OAuth complexity - different providers have different flows, but Cognito normalizes them. I typically enable several options so users can choose their preferred method while getting consistent tokens and user data."],
    },
    // Other AWS
    {
        text: "What is CloudFront and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cloudfront, ValidTag.enum.cdn],
        answers: ["CloudFront is AWS's content delivery network that caches and serves content from edge locations around the world, reducing latency for users. You create a distribution that points to an origin like an S3 bucket, ALB, or custom server. CloudFront caches content at edge locations based on TTL settings and cache behaviors. You can configure multiple origins, path patterns to route requests, custom headers, HTTPS certificates, and access restrictions. Features include custom error pages, geographic restrictions, and Lambda@Edge for running code at the edge. I use CloudFront for serving static websites from S3, accelerating API responses, or distributing media files globally. The configuration includes setting cache policies to balance freshness and performance, invalidating cached content when needed, and using signed URLs for private content. It integrates well with other AWS services and significantly improves performance for global users.",
            "CloudFront is AWS's CDN - it caches content at edge locations worldwide for faster delivery. Create a distribution pointing to your origin (S3, ALB, or custom server). Cache behaviors define how different paths are cached - static assets might cache for days, API responses might not cache at all. Configure HTTPS with ACM certificates, custom error pages, and geographic restrictions. Lambda@Edge runs code at edge locations for request manipulation. I use it for static sites from S3, accelerating APIs, and serving media globally. The performance improvement for distant users is dramatic. Invalidations clear cached content when you deploy updates."],
    },
    {
        text: "What is Route 53 and how does DNS work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.route53],
        answers: ["Route 53 is AWS's DNS service that translates domain names to IP addresses. When you register a domain or configure DNS, you create a hosted zone with record sets like A records for IPv4 addresses, CNAME records for aliasing, MX records for email, and Alias records which are AWS-specific and can point to CloudFront, ALB, or S3 without additional charges. Route 53 supports routing policies like simple routing, weighted for A/B testing, latency-based to route users to the fastest region, failover for disaster recovery, and geolocation for serving different content by location. Health checks monitor endpoints and trigger failover if needed. I use Route 53 for domain management, traffic routing across regions, and blue-green deployments by switching DNS records. The global network ensures low-latency DNS resolution.",
            "Route 53 is AWS's DNS service - it resolves domain names to IP addresses. Create hosted zones with DNS records: A records point to IPs, CNAME aliases to other domains, Alias records point to AWS resources without extra cost. The powerful routing policies let you direct traffic based on latency, geography, or weights for A/B testing. Failover routing with health checks enables automatic disaster recovery. I use it for domain management, multi-region routing, and blue-green deployments. DNS changes propagate quickly through Route 53's global network. Health checks continuously monitor endpoints and automatically route away from failures."],
    },
    {
        text: "What is SQS and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.sqs, ValidTag.enum["message-queues"]],
        answers: ["SQS is AWS's message queuing service that decouples components by allowing asynchronous communication. Producers send messages to a queue, and consumers poll and process them independently. There are two types: standard queues offer maximum throughput with at-least-once delivery and best-effort ordering, while FIFO queues guarantee exactly-once processing and preserve message order. Messages can have a visibility timeout so they're hidden while being processed, and a retention period up to 14 days. I use SQS to decouple microservices, buffer workloads during traffic spikes, distribute tasks to workers, or implement retry logic with dead-letter queues for failed messages. For example, you might queue image processing jobs triggered by uploads, or buffer orders before processing them. SQS is fully managed, scales automatically, and is great for making systems more resilient.",
            "SQS is a managed message queue for decoupling services. Producers send messages, consumers poll and process them independently. Standard queues maximize throughput with at-least-once delivery. FIFO queues guarantee order and exactly-once processing at lower throughput. Visibility timeout hides messages during processing; if processing fails, they reappear. Dead-letter queues catch failed messages for investigation. I use SQS to buffer traffic spikes, distribute work across workers, and decouple systems so failures don't cascade. For example, queue image processing jobs so upload responses are instant while processing happens async. It's fundamental for building resilient distributed systems."],
    },
    {
        text: "What is SNS and how does it differ from SQS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.sns, ValidTag.enum.sqs],
        answers: ["SNS is a pub-sub messaging service where publishers send messages to topics, and multiple subscribers receive them simultaneously. The key difference is SNS pushes messages to subscribers while SQS requires consumers to poll. SNS is for broadcasting messages to multiple consumers - like sending a notification to email, SMS, Lambda functions, and SQS queues all at once. SQS is for point-to-point communication where you want to decouple and queue work. A common pattern is to combine them - use SNS to fan out messages to multiple SQS queues, each consumed by different services. For example, an order event might publish to SNS, which triggers a Lambda for email notifications, pushes to SQS for inventory updates, and sends to another queue for analytics. SNS is for real-time notifications, SQS is for reliable asynchronous processing.",
            "SNS is pub-sub - publish once, deliver to many subscribers. SQS is a queue - one message, one consumer. SNS pushes to subscribers instantly; SQS requires polling. SNS subscribers can be Lambda, SQS, HTTP endpoints, email, or SMS. A common pattern combines them: publish to SNS topic which fans out to multiple SQS queues for different services. Order placed? SNS notifies email service, inventory system, and analytics - each via their own SQS queue. SNS is for broadcasting and fan-out, SQS is for reliable work distribution and decoupling. Use them together for powerful event-driven architectures."],
    },
    {
        text: "What is EventBridge?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum["event-driven"]],
        answers: ["EventBridge is AWS's serverless event bus that routes events between AWS services, your applications, and SaaS providers. You can create rules that pattern-match events and route them to targets like Lambda, Step Functions, SQS, or SNS. It supports scheduled events like cron jobs, AWS service events like EC2 state changes, custom events from your applications, and events from SaaS partners. The event patterns let you filter based on event content to only trigger relevant targets. I use EventBridge for building event-driven architectures - for example, triggering workflows when S3 objects are created, running scheduled maintenance jobs, or integrating with external systems. It's more flexible than CloudWatch Events with better filtering, schema registry, and SaaS integrations. EventBridge makes it easy to build loosely coupled systems that react to events.",
            "EventBridge is a serverless event bus for routing events between services. Create rules that pattern-match events and send them to targets - Lambda, SQS, Step Functions. It handles AWS service events like EC2 state changes, custom application events, scheduled events like cron, and SaaS partner events. The filtering is powerful - match on any event field to route precisely. I use it for event-driven workflows, scheduled jobs, and reacting to AWS service changes. It's evolved from CloudWatch Events with better filtering and schema registry. EventBridge makes building loosely coupled, reactive systems straightforward."],
    },
    {
        text: "What is DynamoDB and when would you choose it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.dynamodb],
        answers: ["DynamoDB is AWS's fully managed NoSQL database that provides single-digit millisecond latency at any scale. It's a key-value and document database where you define a partition key and optionally a sort key, and it distributes data automatically. You can use on-demand pricing for unpredictable workloads or provisioned capacity for consistent traffic. Features include auto-scaling, global tables for multi-region replication, DynamoDB Streams for change data capture, and transactions for ACID operations. I choose DynamoDB for high-throughput applications with simple access patterns, like user sessions, gaming leaderboards, mobile backends, or IoT data. It excels when you need predictable performance at scale and can model your data to use partition keys effectively. For complex queries or joins, RDS might be better. The key is designing your schema around your access patterns rather than normalizing data.",
            "DynamoDB is a managed NoSQL database with consistent single-digit millisecond latency at any scale. It's key-value based - you define partition and sort keys, and it handles distribution. On-demand pricing scales automatically; provisioned gives predictable costs. Features include global tables for multi-region, Streams for change data capture, and ACID transactions. I use it for high-throughput, simple access patterns - user sessions, game state, IoT data. The trick is modeling data for your access patterns rather than normalizing. If you need complex queries or joins, relational databases are better. DynamoDB excels at predictable, fast reads and writes at massive scale."],
    },
    {
        text: "What is ElastiCache?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.elasticache, ValidTag.enum.caching],
        answers: ["ElastiCache is AWS's managed in-memory caching service supporting Redis and Memcached. It provides microsecond latency for read-heavy workloads by caching frequently accessed data in memory. Redis supports advanced data structures, persistence, replication, and clustering, while Memcached is simpler and good for basic caching. Common use cases include database query caching, session storage, real-time analytics, and leaderboards. I use ElastiCache to reduce database load by caching query results, improve application performance by storing frequently accessed data, or as a session store for distributed applications. For example, caching product catalogs in an e-commerce site or user profiles in a social app. ElastiCache handles provisioning, patching, monitoring, and failure recovery. The key is implementing proper cache invalidation strategies and monitoring hit rates to ensure effectiveness.",
            "ElastiCache provides managed Redis or Memcached for in-memory caching. Microsecond latency for read-heavy workloads. Redis offers rich data structures, persistence, and replication. Memcached is simpler but multi-threaded for pure caching. Common uses: cache database queries to reduce load, store sessions for distributed apps, real-time leaderboards, and rate limiting. I use it whenever database read latency matters or when the same data is requested repeatedly. The challenge is cache invalidation - deciding when cached data is stale. Monitor hit rates to ensure caching is effective. ElastiCache handles cluster management, patching, and failover."],
    },
    {
        text: "What are CloudWatch metrics, logs, and alarms?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cloudwatch, ValidTag.enum.monitoring],
        answers: ["CloudWatch is AWS's monitoring and observability service with three main components. Metrics are numerical data points collected over time like CPU utilization or request count - AWS services publish metrics automatically, and you can publish custom metrics. Logs capture text data from applications, Lambda functions, or AWS services, organized into log groups and streams. Alarms watch metrics and trigger actions like SNS notifications or auto-scaling when thresholds are breached. I set up CloudWatch to monitor application health, track custom business metrics, aggregate logs for analysis, and alert on-call engineers when issues occur. For example, I'll create alarms for high error rates, configure log insights queries to analyze patterns, and build dashboards to visualize system health. CloudWatch integrates with all AWS services and is essential for maintaining operational visibility.",
            "CloudWatch provides metrics, logs, and alarms for AWS monitoring. Metrics are numerical data over time - AWS services emit them automatically, and you can publish custom ones. Logs collect text output from applications and services. Alarms trigger actions when metrics cross thresholds - SNS notifications, auto-scaling, Lambda invocations. I use metrics for dashboards and trends, logs for debugging, and alarms for alerting. Log Insights lets you query logs with SQL-like syntax. For production systems, I set up alarms on error rates, latency percentiles, and resource utilization. CloudWatch is the foundation of AWS observability, though many teams add tools like Datadog for better visualization."],
    },
    {
        text: "What is AWS Secrets Manager vs Parameter Store?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.security],
        answers: ["Both store configuration and secrets, but with different features and pricing. Secrets Manager is designed specifically for secrets like database credentials and API keys, with automatic rotation, fine-grained access control, and cross-region replication. It costs per secret stored and API call. Parameter Store is part of Systems Manager and can store any configuration data or secrets. The free tier supports up to 10,000 parameters with standard throughput, and advanced parameters cost per parameter. Parameter Store doesn't have built-in rotation but is cheaper. I use Secrets Manager for credentials that need rotation like database passwords or API keys, where the automatic rotation justifies the cost. I use Parameter Store for general configuration like feature flags, environment settings, or secrets that don't need rotation. Both encrypt data and integrate with IAM for access control.",
            "Secrets Manager is built for credentials with features like automatic rotation, cross-region replication, and audit logging. It costs per secret and API call. Parameter Store is part of Systems Manager - it can store config and secrets but is simpler and cheaper, with a generous free tier. Parameter Store lacks built-in rotation. I use Secrets Manager for database passwords and API keys that need rotation, and Parameter Store for general config like feature flags or environment settings. Both encrypt data and integrate with IAM. The choice often comes down to whether automatic rotation justifies Secrets Manager's cost."],
    },

    // GCP
    {
        text: "What are the core GCP services and how do they map to AWS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.aws],
        answers: ["The core GCP services parallel AWS quite closely. Compute Engine is like EC2 for virtual machines, Cloud Run is container-based serverless similar to Fargate, Cloud Functions matches Lambda for serverless functions. Cloud Storage is equivalent to S3 for object storage, Cloud SQL is like RDS for managed databases, and Cloud Spanner is a globally distributed database unique to GCP. For networking, VPC works similarly to AWS VPC. BigQuery is GCP's data warehouse comparable to Redshift but serverless. Pub/Sub is like SNS/SQS combined for messaging. Firebase is mobile/web app platform with Firestore as a NoSQL database similar to DynamoDB. The main philosophical difference is GCP tends toward more managed, serverless options and has stronger data analytics services, while AWS offers more configuration options and the broadest service catalog. Both achieve similar outcomes with different approaches.",
            "The mapping is fairly direct. Compute Engine equals EC2 for VMs. Cloud Run and Cloud Functions match App Runner and Lambda for serverless. Cloud Storage is like S3, Cloud SQL like RDS, Cloud Spanner is unique - globally distributed relational database. BigQuery is an incredibly powerful serverless data warehouse. Pub/Sub combines SNS and SQS functionality. Firebase/Firestore for mobile backends like Cognito/DynamoDB. GCP tends to be more opinionated with fewer knobs to turn, and has excellent data and ML services. AWS has broader service selection. Skills transfer well between them - same concepts, different implementations."],
    },
    {
        text: "What is Cloud Run?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-run"]],
        answers: ["Cloud Run is GCP's serverless container platform where you deploy containerized applications without managing infrastructure. You give it a container image and Cloud Run handles everything - deploying, scaling to zero when idle, scaling up based on requests, and load balancing. It's built on Knative so you can port to Kubernetes if needed. You're billed per request and compute time with automatic scaling from zero to thousands of instances. Cloud Run is great for APIs, web apps, microservices, or any stateless workload you can containerize. Compared to Cloud Functions, Cloud Run gives you more control since you bring your own container, supports any language or runtime, and can handle larger deployments up to 60 minutes per request. I use it when I want the simplicity of serverless but need more flexibility than functions provide.",
            "Cloud Run runs containers serverlessly - provide an image, it handles scaling, load balancing, and HTTPS. Scales to zero when idle, scales up automatically under load. Pay per request and compute time. Built on Knative, so portable to Kubernetes. Great for APIs, web apps, and microservices. Compared to Cloud Functions, you get full container control - any language, runtime, or dependencies. Requests can run up to 60 minutes. I use it for anything containerized that needs serverless simplicity. It's like AWS App Runner but more mature. Extremely developer-friendly for production workloads."],
    },
    {
        text: "What is Cloud Functions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-functions"]],
        answers: ["Cloud Functions is GCP's serverless functions platform, similar to AWS Lambda. You write individual functions in supported languages like Node.js, Python, or Go, and they run in response to events like HTTP requests, Cloud Storage changes, Pub/Sub messages, or Firebase events. The runtime, scaling, and infrastructure are fully managed. Functions can run for up to 9 minutes and automatically scale based on load. I use Cloud Functions for event-driven tasks like processing uploaded files, triggering workflows, sending notifications, or building webhooks. The advantages are zero infrastructure management, automatic scaling, and pay-per-use pricing. The main limitation compared to Cloud Run is less control over the environment and dependencies. Cloud Functions is perfect for small, focused tasks that respond to events.",
            "Cloud Functions is GCP's Lambda equivalent - write a function, deploy it, it runs on events. Supports Node.js, Python, Go, and other languages. Triggers include HTTP, Pub/Sub, Cloud Storage, Firebase, and scheduled events. Fully managed with automatic scaling. Up to 9 minutes per invocation. Pay per invocation and compute time. I use them for event-driven tasks - process uploaded files, respond to webhooks, trigger workflows. Quick to deploy and iterate. Less control than Cloud Run since you don't manage the container, but simpler for straightforward functions."],
    },
    {
        text: "What is Cloud Storage?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-storage"]],
        answers: ["Cloud Storage is GCP's object storage service, equivalent to S3. It stores data as objects in buckets with global unique names and provides different storage classes: Standard for frequently accessed data, Nearline for monthly access, Coldline for quarterly, and Archive for annual access. Features include versioning, lifecycle management, access controls via IAM and ACLs, signed URLs for temporary access, and strong consistency. Cloud Storage integrates well with other GCP services and can trigger Cloud Functions on object events. I use it for backups, static website hosting, data lakes, media storage, and archival. The pricing model charges for storage, operations, and network egress. Compared to S3, Cloud Storage has simpler storage classes and better performance for certain workloads, but both services are quite similar in capabilities.",
            "Cloud Storage is GCP's S3 equivalent - object storage with global bucket namespaces. Storage classes optimize cost: Standard for frequent access, Nearline/Coldline/Archive for decreasing access frequency with lower storage costs but retrieval fees. Features include versioning, lifecycle rules, IAM and signed URLs for access control. Strong consistency unlike S3's eventual consistency model. Integrates with Cloud Functions for event-driven processing. I use it for static assets, backups, data lakes, and archives. Very similar to S3 in practice - if you know one, you know the other."],
    },
    {
        text: "What is Cloud SQL vs Cloud Spanner?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp],
        answers: ["Cloud SQL is GCP's managed relational database service for MySQL, PostgreSQL, and SQL Server, similar to AWS RDS. It's regional, handles automatic backups, replication, and patching. Cloud Spanner is a globally distributed, horizontally scalable relational database that offers strong consistency and SQL support across regions - unique to GCP. Cloud SQL is for traditional relational workloads within a region with predictable performance at lower cost. Cloud Spanner is for applications needing global distribution, automatic sharding, and the ability to scale horizontally while maintaining ACID transactions. For example, I'd use Cloud SQL for a regional web app's database, but Cloud Spanner for a global application needing consistent reads and writes across continents. Spanner is more expensive but provides capabilities traditional databases can't match at scale.",
            "Cloud SQL is managed MySQL, PostgreSQL, or SQL Server - like RDS, regional with automatic backups and HA replicas. Cloud Spanner is something unique: a globally distributed relational database with strong consistency and SQL support that scales horizontally. Traditional databases make you choose between scale and consistency; Spanner gives both. Cloud SQL for regional apps at reasonable cost. Spanner for global apps needing consistent reads and writes across continents with automatic sharding. Spanner is significantly more expensive but provides capabilities no traditional database can match."],
    },
    {
        text: "What is BigQuery?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp],
        answers: ["BigQuery is GCP's fully managed, serverless data warehouse designed for analytics on massive datasets using SQL. You load data from Cloud Storage, streaming sources, or other databases, then run SQL queries that can analyze terabytes in seconds. It separates storage and compute so you can store large amounts of data cheaply and only pay for queries when you run them. BigQuery supports nested and repeated fields, streaming inserts, partitioning and clustering for performance, and integration with BI tools. I use BigQuery for data analytics, business intelligence, machine learning with BigQuery ML, and log analysis. It's great when you need to query large datasets without managing infrastructure or optimizing indexes. The serverless nature means it scales automatically and you don't worry about capacity planning, just pay for storage and queries.",
            "BigQuery is a serverless data warehouse that queries terabytes in seconds using standard SQL. Load data from Cloud Storage, streams, or other sources. Storage and compute are separated - store data cheaply, pay for queries when you run them. Partition and cluster tables for performance. BigQuery ML runs machine learning models directly with SQL. I use it for analytics, BI reporting, and log analysis. No infrastructure to manage, no indexes to tune, just run queries. It automatically parallelizes across thousands of nodes. For analytics at scale, it's hard to beat. GCP's data story is one of their strongest areas."],
    },
    {
        text: "What is Firebase and how does it integrate with GCP?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.firestore],
        answers: ["Firebase is Google's mobile and web app development platform with tools for authentication, real-time databases, file storage, hosting, and analytics. Firestore is Firebase's NoSQL document database offering real-time synchronization and offline support. Firebase integrates tightly with GCP - Firebase projects are GCP projects, you can use GCP services from Firebase apps, and Firebase Authentication works with Cloud Identity. Firestore data can be analyzed in BigQuery, Cloud Functions can respond to Firebase events, and you can use Cloud Storage from Firebase apps. I use Firebase for rapid mobile app development because it provides backend services out of the box. The real-time database and authentication are particularly strong. For web apps, Firebase Hosting with Cloud Functions makes deployment easy. The GCP integration means you can start with Firebase and expand to full GCP services as needed.",
            "Firebase is Google's mobile and web app platform - authentication, databases, hosting, analytics in one package. Firestore is the NoSQL database with real-time sync and offline support. Firebase projects are GCP projects, so integration is seamless. Use Cloud Functions with Firebase triggers, analyze Firestore data in BigQuery, Cloud Storage for files. I use Firebase for rapid prototyping and mobile backends because so much is built-in. Start with Firebase for quick development, expand to full GCP services as you grow. Authentication is particularly well-designed and saves weeks of work. Great for MVPs and mobile-first applications."],
    },
    {
        text: "What is Cloud IAM?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.security],
        answers: ["Cloud IAM controls access to GCP resources through policies that bind members to roles at different hierarchy levels. Members can be users, service accounts, groups, or domains. Roles define permissions - there are primitive roles like Owner, Editor, and Viewer, and predefined roles for specific services with granular permissions. You can also create custom roles. Policies are set at the project, folder, or organization level and inherited down the hierarchy. Service accounts are for applications to authenticate to GCP services. The principle is similar to AWS IAM but the structure differs - GCP uses resource hierarchy and role binding while AWS uses policies attached to identities or resources. I use Cloud IAM to grant developers access to specific projects, give applications service account permissions, and implement least privilege. The organization hierarchy makes multi-project management cleaner than AWS.",
            "Cloud IAM manages access by binding members to roles. Members are users, service accounts, or groups. Roles bundle permissions - use predefined roles for most cases, custom roles for specific needs. Policies are set at organization, folder, or project level and inherit downward. Service accounts give applications identity to access GCP services. Compared to AWS, GCP's hierarchy model is cleaner for multi-project organizations. I bind developers to project-level roles, applications to service accounts with minimal permissions. Same principles as AWS IAM - least privilege, audit access, use groups over individual grants."],
    },
    {
        text: "What is Pub/Sub?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["pub-sub"]],
        answers: ["Pub/Sub is GCP's messaging service for asynchronous communication between services. Publishers send messages to topics, and subscribers receive messages from subscriptions to those topics. It's similar to SNS/SQS combined but with more flexibility. Pub/Sub offers at-least-once delivery, automatic scaling, global distribution, and message ordering within a key. You can have push subscriptions that deliver to HTTP endpoints or pull subscriptions where consumers poll. Features include message filtering, dead-letter topics, and exactly-once delivery. I use Pub/Sub for event-driven architectures, streaming data ingestion, distributing tasks to workers, or decoupling microservices. For example, you might publish user events that trigger multiple downstream processes like analytics, notifications, and data warehouse updates. Pub/Sub handles massive scale and ensures reliable message delivery without managing infrastructure.",
            "Pub/Sub handles asynchronous messaging in GCP. Publishers send to topics, subscribers receive from subscriptions. Push delivers to HTTP endpoints automatically; pull requires consumers to fetch. It combines SNS and SQS concepts with more flexibility. Features include message ordering, filtering, dead-letter topics, and exactly-once delivery options. Scales automatically, globally distributed. I use it for event-driven systems, data streaming, and service decoupling. Publish an event once, multiple subscribers handle it differently - analytics, notifications, data processing. Reliable at massive scale without managing any infrastructure."],
    },

    // Docker (mid-level only - basics in junior-advanced, advanced in mid-advanced)
    {
        text: "What are Docker volumes and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.volumes],
        answers: ["Docker volumes are the preferred way to persist data generated by containers. Unlike container filesystems which are ephemeral, volumes persist independently of container lifecycles. There are named volumes managed by Docker, bind mounts that map to specific host paths, and tmpfs mounts for temporary in-memory storage. Named volumes are created with 'docker volume create' or automatically when specified in a run command, stored in Docker's directory structure, and can be shared between containers. Bind mounts directly mount a host directory into the container, useful for development. I use named volumes for database data, application logs, or any data that needs to survive container restarts. Bind mounts are great for local development where you want code changes to immediately reflect in the container. Volumes are managed independently and can be backed up, migrated, or shared across containers safely.",
            "Volumes persist data beyond container lifecycles. Container filesystems are ephemeral - when the container stops, data is gone. Volumes are the solution. Named volumes are managed by Docker, stored in its directory structure, perfect for database data. Bind mounts link a host directory directly into the container - great for development since code changes appear instantly. tmpfs mounts are in-memory for temporary data. I use named volumes for anything that needs persistence, bind mounts for development workflows. Volumes can be shared between containers and backed up independently. Essential for running stateful applications in containers."],
    },
    {
        text: "What is Docker networking?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.networks],
        answers: ["Docker networking allows containers to communicate with each other and the outside world. There are several network types: bridge is the default where containers on the same bridge network can communicate, host removes network isolation and uses the host's network directly, none disables networking, and overlay enables communication across multiple Docker hosts. When containers are on the same user-defined bridge network, they can communicate using container names as hostnames thanks to built-in DNS. Port mapping with -p exposes container ports to the host. I create custom bridge networks for applications so containers can communicate securely by name, use host networking when I need maximum network performance, and overlay networks for Docker Swarm or multi-host setups. Understanding networking is crucial for microservices where containers need to communicate without hard-coded IPs.",
            "Docker networking controls how containers communicate. Bridge networks connect containers on the same host - they can reach each other by container name thanks to built-in DNS. Host networking removes isolation and uses the host's network directly for max performance. Overlay networks span multiple Docker hosts for swarm deployments. I create custom bridge networks for applications so my web container can connect to 'database' by name without hardcoding IPs. Port mapping with -p exposes container ports to the outside world. Understanding networking is essential for microservices architectures where many containers need to communicate."],
    },
    {
        text: "What are Docker health checks?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker],
        answers: ["Health checks let you define a command that Docker runs periodically to check if a container is healthy. You specify them in the Dockerfile with HEALTHCHECK or in docker-compose.yml. The check command returns 0 for healthy or 1 for unhealthy. Docker tracks the health status and orchestrators like Kubernetes can use this to restart unhealthy containers or remove them from load balancers. For example, a web server health check might curl its health endpoint. I define health checks to ensure containers are not just running but actually functional - the process might be up but unable to serve requests. This is crucial in production where you want automatic recovery from failures. Health checks run at intervals you specify and have configurable timeout and retries. They're a simple way to add resilience by catching issues before users do.",
            "Health checks verify a container is actually working, not just running. Define a command in your Dockerfile with HEALTHCHECK or in docker-compose. The command returns 0 for healthy, 1 for unhealthy. Docker periodically runs this check and tracks the status. Orchestrators use health status to restart failing containers or remove them from load balancers. A web server might check its /health endpoint. I always define health checks because a running process isn't necessarily a working service. Configure interval, timeout, and retries appropriately. Health checks catch issues before users notice and enable automatic recovery."],
    },

    // Kubernetes (mid-level only - basics in junior-advanced, advanced in mid-advanced)
    {
        text: "What is the Kubernetes architecture (control plane, nodes)?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Kubernetes has a control plane and worker nodes. The control plane manages the cluster and includes the API server which is the front-end receiving commands, etcd for storing cluster state, the scheduler which assigns pods to nodes, and controller managers that maintain desired state. Worker nodes run your workloads and include the kubelet agent that communicates with the control plane, kube-proxy for networking, and a container runtime like Docker or containerd. When you deploy an application, you send a manifest to the API server, it stores it in etcd, the scheduler chooses nodes to run pods, and kubelets on those nodes start the containers. Controllers continuously watch state and make corrections if reality drifts from desired state. The control plane can be highly available with multiple instances. Understanding this architecture helps troubleshoot issues and design robust deployments.",
            "The control plane runs the cluster - API server receives all commands, etcd stores state, scheduler places pods on nodes, controller managers maintain desired state. Worker nodes run your containers - kubelet communicates with control plane, kube-proxy handles networking, container runtime runs the containers. When you apply a manifest, it goes to API server, gets stored in etcd, scheduler picks nodes, kubelets start containers. Controllers continuously reconcile actual state with desired state. If something drifts, they fix it. Understanding this helps troubleshooting - API server issues mean no commands work, scheduler issues mean pods stay pending, kubelet issues affect specific nodes."],
    },
    {
        text: "What is a ReplicaSet?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["A ReplicaSet ensures a specified number of pod replicas are running at all times. If pods die, the ReplicaSet creates replacements. If there are too many, it deletes extras. You define the desired replica count and a pod template, and the ReplicaSet maintains that state. However, you typically don't create ReplicaSets directly - deployments create and manage them for you. The advantage of deployments over ReplicaSets is deployments handle updates elegantly by creating new ReplicaSets and transitioning traffic. ReplicaSets just maintain a static set of pods. Under the hood, when you update a deployment, it creates a new ReplicaSet for the new version while keeping the old one around for rollback. I interact with deployments which manage ReplicaSets automatically, rarely working with ReplicaSets directly.",
            "A ReplicaSet maintains a stable set of pod replicas. Specify desired count and pod template, it ensures that many pods are running. Pod crashes? ReplicaSet creates a replacement. Too many pods? It removes extras. In practice, you rarely create ReplicaSets directly. Deployments manage them for you, creating new ReplicaSets during updates to enable rolling updates and rollbacks. When you update a deployment, it creates a new ReplicaSet, scales it up while scaling the old one down. The old ReplicaSet sticks around for rollback. I work with Deployments, which abstract ReplicaSet management."],
    },
    {
        text: "What are ConfigMaps and Secrets?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.configmaps, ValidTag.enum.secrets],
        answers: ["ConfigMaps and Secrets decouple configuration from container images. ConfigMaps store non-sensitive configuration like feature flags, URLs, or config files. Secrets store sensitive data like passwords, tokens, or keys, and are base64 encoded. You create them separately and inject them into pods as environment variables or mounted files. This lets you use the same image across environments with different configurations. The key difference is Secrets are handled more carefully - encrypted at rest in etcd if configured, access-controlled, and not logged. However, Secrets aren't super secure by default, so for production I often use external secret managers like Vault or cloud-provider solutions. I use ConfigMaps for app config and Secrets for credentials, mounting them as volumes when I need full config files or env vars for simple values.",
            "ConfigMaps hold non-sensitive configuration - feature flags, URLs, config files. Secrets hold sensitive data - passwords, tokens, keys. Both decouple configuration from images so you can use the same image across environments. Inject them as environment variables or mounted files. Secrets are base64 encoded and handled more carefully - encrypted at rest if configured, access-controlled. But Secrets aren't super secure by default, so many teams use Vault or cloud secret managers for production credentials. I use ConfigMaps for app configuration, Secrets for credentials, and external managers for critical production secrets."],
    },
    {
        text: "What are volumes in Kubernetes and how do they differ from Docker volumes?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.docker],
        answers: ["Kubernetes volumes provide storage to pods, with many more types than Docker volumes. Docker volumes are simple persistent storage. Kubernetes volumes support emptyDir for temporary storage that lives with the pod, hostPath for mounting host directories, persistentVolumeClaims for durable storage that outlives pods, configMaps and secrets for configuration, and cloud-provider volumes like AWS EBS or GCP Persistent Disks. The volume lifecycle is tied to the pod by default - when the pod dies, emptyDir volumes are deleted. For persistence, you use PersistentVolumes and PersistentVolumeClaims which decouple storage from pod lifecycle. PVs are cluster resources provisioned by admins or dynamically, and PVCs are requests for storage by users. This abstraction lets you write pod specs without knowing the underlying storage. I use emptyDir for temp data, PVCs for databases, and ConfigMaps for configuration files.",
            "Kubernetes has more volume types than Docker. emptyDir is temporary storage that dies with the pod. hostPath mounts host directories. PersistentVolumeClaims request durable storage that survives pod restarts. ConfigMaps and Secrets can be mounted as volumes. Cloud volumes like EBS or GCP Persistent Disks integrate natively. Docker volumes are simpler persistent storage. In Kubernetes, PersistentVolumes and PersistentVolumeClaims abstract storage - request what you need, let the cluster provision it. I use emptyDir for temp files, PVCs for databases, ConfigMaps for configuration. The abstraction lets you write specs without caring about underlying storage technology."],
    },
    {
        text: "What are labels and selectors?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Labels are key-value pairs attached to Kubernetes objects like pods for identification and organization. Selectors query objects by their labels. For example, you might label pods with app=nginx and environment=prod. Services use selectors to find which pods to route traffic to, and deployments use them to manage pods. This loose coupling is powerful - you can change which pods a service targets by changing labels without modifying the service itself. Labels can be added, modified, or removed at any time. You can query with kubectl using label selectors like 'kubectl get pods -l app=nginx'. I use labels extensively for organization, selecting resources, and routing traffic. Common labels include app name, version, environment, and tier. They're fundamental to how Kubernetes objects relate to each other and how you organize and query resources.",
            "Labels are key-value pairs on Kubernetes objects for identification. Selectors query by labels. Services find pods via label selectors. Deployments manage pods by label matching. This loose coupling is powerful - change labels to change which pods a service targets without modifying the service. Query with 'kubectl get pods -l app=nginx'. I label everything with app name, environment, version, and tier. Labels are fundamental to how Kubernetes objects relate. Services route to pods matching labels. Deployments own pods matching labels. It's how the whole system is organized and connected."],
    },

    // CI/CD (mid-level only - basics in junior-advanced, advanced in mid-advanced/senior)
    {
        text: "What is a CI/CD pipeline and what stages does it typically have?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum.pipeline],
        answers: ["A CI/CD pipeline is an automated workflow that takes code from version control through to production. Think of it as an assembly line for software. Each stage performs specific checks or transformations, and the code only moves forward if it passes that stage. A typical pipeline starts with a source stage that triggers when code is pushed. Then there's a build stage where the code is compiled or bundled, dependencies are installed, and artifacts are created. Next comes the test stage, which usually runs unit tests first, then integration tests, and sometimes end-to-end tests. After testing, there's often a security scanning stage that checks for vulnerabilities in dependencies and the code itself. Then you might have a staging deployment stage where the application is deployed to a test environment for further validation. Finally, there's a production deployment stage. Some pipelines include additional stages like performance testing, database migrations, or approval gates. The key principle is that each stage acts as a quality gate. If tests fail or security scans find critical issues, the pipeline stops and the team is notified. This gives you confidence that anything reaching production has been thoroughly validated.",
            "A pipeline automates getting code from commit to production. Typical stages: source triggers on push, build compiles and creates artifacts, test runs unit and integration tests, security scans for vulnerabilities, deploy to staging for validation, then production. Each stage is a quality gate - fail and the pipeline stops. Additional stages might include performance testing, database migrations, or manual approvals. The key value is automation and consistency. Every change goes through the same validated process. Nothing reaches production without passing all gates. Failures alert the team immediately. This gives confidence that production deployments are safe and well-tested."],
    },
    {
        text: "How do you implement automated testing in CI/CD?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum.testing],
        answers: ["Implementing automated testing in CI/CD is all about building a reliable safety net that catches issues before they reach production. I structure tests in a pyramid shape. At the base, you have unit tests which are fast, numerous, and test individual functions or components in isolation. These run first because they give the quickest feedback. Above that are integration tests that verify different parts of the system work together correctly, like testing that your API endpoints correctly interact with the database. At the top are end-to-end tests that simulate real user behavior, like clicking through a checkout flow. In the pipeline configuration, I set up the testing stage to run after the build succeeds. Unit tests run first, and if they fail, the pipeline stops immediately. Integration tests run next in parallel where possible to save time. E2E tests typically run against a deployed staging environment. I also include test coverage reporting to track whether we're maintaining good coverage over time. For flaky tests, I might configure retries, but the real solution is fixing the flakiness. The key is making tests fast enough that developers get feedback quickly and reliable enough that failures are always meaningful.",
            "Structure tests in a pyramid - many fast unit tests at the base, fewer integration tests in the middle, few slow E2E tests at the top. Run unit tests first for fast feedback - if they fail, stop immediately. Integration tests verify components work together. E2E tests simulate real user flows on deployed staging. Parallelize where possible to keep pipelines fast. Track coverage to ensure it stays healthy over time. Fix flaky tests rather than ignoring them - unreliable tests erode trust. The goal is fast, reliable feedback. Developers should know within minutes if their change broke something. Every failure should be meaningful."],
    },
    {
        text: "What are GitHub Actions and how do you configure them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum["github-actions"]],
        answers: ["GitHub Actions is GitHub's built-in CI/CD platform that lets you automate workflows directly in your repository. It's tightly integrated with GitHub, so you get seamless triggering based on GitHub events like pushes, pull requests, releases, or even issue comments. Configuration is done through YAML files in the .github/workflows directory. Each workflow file defines when it runs, what jobs it contains, and what steps each job executes. A basic workflow might trigger on push to main, spin up an Ubuntu runner, checkout the code, install dependencies, run tests, and deploy. Jobs run in parallel by default but can have dependencies on each other. Each job runs on a fresh virtual machine called a runner. GitHub provides hosted runners for Linux, macOS, and Windows, or you can use self-hosted runners for specialized needs. One of the powerful features is the marketplace of pre-built actions. Instead of writing scripts from scratch, you can use community actions for common tasks like setting up Node, caching dependencies, or deploying to AWS. You reference these actions with the uses keyword. Secrets are managed through the repository settings and accessed as environment variables. Overall, it's a great choice when you're already using GitHub since there's minimal setup and the integration is smooth.",
            "GitHub Actions is GitHub's native CI/CD. Workflows are YAML files in .github/workflows/. Define triggers (push, PR, schedule), jobs, and steps. Jobs run on runners - GitHub-hosted or self-hosted VMs. Steps are shell commands or reusable actions from the marketplace. The marketplace has pre-built actions for common tasks - setup languages, cache dependencies, deploy to clouds. Jobs run in parallel by default but can depend on each other. Secrets are stored in repo settings and accessed as environment variables. Great choice if you're on GitHub - tight integration, simple setup, generous free tier. The ecosystem of community actions accelerates development significantly."],
    },
    {
        text: "What is the difference between GitHub Actions, Jenkins, CircleCI, and GitLab CI?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum["github-actions"], ValidTag.enum.jenkins, ValidTag.enum["gitlab-ci"]],
        answers: ["Each of these CI/CD tools has its strengths and fits different contexts. GitHub Actions is tightly integrated with GitHub, which makes it ideal if that's where your code lives. Configuration is simple YAML files, and there's a huge marketplace of pre-built actions. It's free for public repos and has generous free tier for private. The downside is you're locked into the GitHub ecosystem. Jenkins is the veteran. It's open-source, extremely flexible, and can do almost anything through its massive plugin ecosystem. However, it requires self-hosting and maintenance, and the configuration can get complex. It's often the choice for enterprises with existing Jenkins infrastructure or specialized requirements. CircleCI is a cloud-native option known for good performance and caching. It works with multiple git providers and has powerful parallelism features. The configuration is clean YAML, and it scales well for larger teams. GitLab CI is integrated into GitLab, similar to how GitHub Actions is integrated with GitHub. If you use GitLab for source control, it's the natural choice. It has a complete DevOps platform including container registry, security scanning, and more. The choice often comes down to where your code lives and what ecosystem you're already using. For new projects on GitHub, GitHub Actions is usually the simplest path. For complex enterprise needs, Jenkins offers the most flexibility.",
            "Each tool has its context. GitHub Actions integrates tightly with GitHub, simple YAML config, huge marketplace. Best when you're already on GitHub. Jenkins is the veteran - open source, infinitely flexible via plugins, but requires self-hosting and maintenance. Enterprises often use it for complex requirements. CircleCI is cloud-native with excellent performance and parallelism, works with multiple providers. GitLab CI integrates with GitLab like Actions does with GitHub, includes full DevOps platform. The choice usually follows your source control. On GitHub? Use Actions. On GitLab? Use GitLab CI. Need maximum flexibility or have legacy requirements? Jenkins. Need cloud-native with good performance? CircleCI."],
    },
];
