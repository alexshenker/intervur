import { Category, Level, ValidTag } from "../../../db";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.devops,
    typeof Level.enum.mid
>[] = [
    // AWS General
    {
        text: "What are the core AWS services and when would you use each?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["The core AWS services I work with regularly include EC2 for virtual machines when you need full control over the infrastructure, S3 for object storage like static assets or backups, RDS for managed relational databases, Lambda for serverless functions that run on-demand, and VPC for networking. Then you have IAM for access management, CloudWatch for monitoring, and Route 53 for DNS. I'd use EC2 when you need persistent compute with specific configurations, Lambda when you have event-driven workloads or want to avoid server management, S3 for any file storage needs, and RDS when you want a managed database without worrying about patches and backups. The key is matching the service to your workload characteristics and operational requirements."],
    },
    {
        text: "What is the AWS shared responsibility model?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.security],
        answers: ["The shared responsibility model defines what AWS manages versus what you're responsible for as a customer. AWS handles security 'of' the cloud - so the physical infrastructure, hardware, networking, and the foundational services themselves. You're responsible for security 'in' the cloud - things like your data, IAM configurations, operating system patches on EC2 instances, application security, and network configurations. For example, AWS ensures the physical security of their data centers, but you need to ensure your S3 buckets aren't publicly accessible or your security groups are properly configured. The exact split varies by service type - with managed services like RDS, AWS handles more, whereas with EC2, you handle more. Understanding this model is critical for proper security posture."],
    },
    {
        text: "What are AWS regions and availability zones?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["AWS regions are separate geographic areas around the world, like us-east-1 in Virginia or eu-west-1 in Ireland. Each region is completely independent and isolated from the others. Within each region, you have multiple availability zones, which are essentially separate data centers with independent power, cooling, and networking. AZs within a region are connected with low-latency links but are far enough apart to be isolated from disasters. The key is that you can deploy your application across multiple AZs for high availability - if one AZ goes down, your app keeps running in another. You choose regions based on factors like latency to your users, data residency requirements, or service availability, since not all services are available in all regions."],
    },
    {
        text: "What is VPC and how do you configure networking?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.vpc],
        answers: ["A VPC is your own isolated virtual network within AWS where you launch your resources. You define the IP address range using CIDR blocks, then create subnets within that range across different availability zones. Subnets can be public or private - public subnets have a route to an internet gateway, while private subnets don't. You'd typically put web servers in public subnets and databases in private ones. Then you set up route tables to control traffic flow, security groups to act as firewalls at the instance level, and network ACLs for subnet-level security. For connecting to the outside world, you might use an internet gateway for public access, a NAT gateway to let private instances reach the internet, or VPN/Direct Connect for secure connections to your on-premise infrastructure. The key is designing a network that's secure by default."],
    },
    {
        text: "What are security groups and NACLs?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.vpc, ValidTag.enum.security],
        answers: ["Security groups and NACLs are both firewall mechanisms but they work differently. Security groups operate at the instance level and are stateful - meaning if you allow inbound traffic, the response is automatically allowed back out. They only have allow rules, and you specify what traffic is permitted. NACLs work at the subnet level and are stateless, so you need explicit rules for both inbound and outbound traffic. They support both allow and deny rules and are evaluated in order. In practice, I rely heavily on security groups for most access control because they're easier to manage and being stateful is usually what you want. NACLs are more of a backup layer or for specific deny rules. A common pattern is to have a security group for web servers allowing port 443 from anywhere, and another for databases allowing port 5432 only from the web server security group."],
    },
    {
        text: "What is AWS CloudFormation and infrastructure as code?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws],
        answers: ["CloudFormation is AWS's infrastructure as code service where you define your infrastructure in templates using JSON or YAML. Instead of manually clicking through the console to create resources, you declare what you want in a template and CloudFormation provisions everything for you. The benefits are huge - you get version control for your infrastructure, repeatability across environments, and you can tear down and recreate entire environments consistently. The templates define resources, their properties, and dependencies between them. CloudFormation handles the ordering and manages the state. You can also use features like stack sets to deploy across multiple accounts and regions. The main value is treating infrastructure like code - it's reviewed, tested, and versioned just like application code, which reduces errors and makes your infrastructure reproducible."],
    },

    // EC2
    {
        text: "What is EC2 and how do instance types work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["EC2 is AWS's virtual server service where you can launch instances with different compute, memory, and storage configurations. Instance types follow a naming pattern like t3.medium or m5.large, where the letter indicates the family, the number is the generation, and the size determines the resources. The families are optimized for different workloads - T instances are burstable and good for workloads with variable CPU usage, M instances are general purpose balanced compute and memory, C instances are compute-optimized for CPU-intensive tasks, R instances are memory-optimized for in-memory databases, and so on. You choose instance types based on your application's resource requirements. For example, I'd use a T3 instance for a small web server with occasional traffic, but a C5 instance for heavy data processing or an R5 for caching layers."],
    },
    {
        text: "What are EC2 pricing models (on-demand, reserved, spot)?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["On-demand instances are pay-as-you-go with no commitment - you pay by the hour or second and can start and stop anytime. Reserved instances require a commitment of one or three years but give you significant discounts, up to 75% off on-demand pricing. Spot instances let you bid on unused EC2 capacity at up to 90% off, but AWS can reclaim them with a two-minute warning when they need the capacity back. I use on-demand for unpredictable workloads or short-term needs, reserved instances for baseline capacity that I know will run continuously like production databases, and spot for fault-tolerant or flexible workloads like batch processing or CI/CD jobs. A good strategy is to cover your baseline with reserved instances, handle normal traffic with on-demand, and use spot for burst capacity or non-critical workloads."],
    },
    {
        text: "What is an AMI?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["An AMI is an Amazon Machine Image, basically a template for launching EC2 instances. It includes the operating system, application server, applications, and any configuration you've baked in. You can use AWS-provided AMIs like Amazon Linux or Ubuntu, find community AMIs, or create your own custom AMIs. Creating custom AMIs is really useful for standardizing your environment - you can configure an instance exactly how you want it, install all your software and dependencies, then create an AMI from it. Now you can launch identical instances quickly without repeating the setup. I typically create golden AMIs for different purposes like web servers or application servers, and update them periodically with security patches. AMIs are also region-specific, so you need to copy them if you want to use them in different regions."],
    },
    {
        text: "What is auto-scaling and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2, ValidTag.enum.scalability],
        answers: ["Auto-scaling automatically adjusts the number of EC2 instances based on demand. You create an auto-scaling group that defines the minimum, maximum, and desired number of instances, along with a launch template that specifies what kind of instances to create. Then you set up scaling policies that trigger based on CloudWatch metrics like CPU utilization or request count. For example, you might scale out when average CPU exceeds 70% and scale in when it drops below 30%. You can also use target tracking policies that maintain a specific metric at a target value, or scheduled scaling for predictable load patterns. The key is to configure health checks so unhealthy instances get replaced automatically, and to test your scaling policies to make sure they respond appropriately. Auto-scaling is essential for handling variable traffic while optimizing costs."],
    },
    {
        text: "What are launch templates?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["Launch templates are the modern way to specify instance configuration for EC2 instances and auto-scaling groups. They define all the parameters needed to launch an instance like the AMI, instance type, key pair, security groups, user data, and storage configuration. They're versioned, so you can have multiple versions and roll back if needed. The advantage over the older launch configurations is that templates are more flexible, support newer instance features, and can be used both for launching individual instances and with auto-scaling groups. I create launch templates with sensible defaults for my environment, and can override specific parameters when launching. They're also easier to manage in infrastructure as code compared to the deprecated launch configurations."],
    },
    {
        text: "How do you handle EC2 instance storage?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.ec2],
        answers: ["EC2 instances can have two types of storage: EBS volumes and instance store. EBS volumes are network-attached storage that persists independently of the instance lifecycle - if you stop an instance, the EBS volume retains its data. They come in different types like gp3 for general purpose SSD, io2 for high-performance workloads needing consistent IOPS, or st1 for throughput-optimized HDDs. Instance store is ephemeral storage physically attached to the host - it's very fast but the data is lost if the instance stops or terminates. I use EBS for most workloads because the persistence is critical, and I'll choose gp3 for most use cases since it offers good performance at reasonable cost. For databases or high-IOPS applications, I'd use io2. Instance store is good for temporary data like caches or buffers where you can afford to lose the data."],
    },

    // S3
    {
        text: "What is S3 and what are its use cases?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 is AWS's object storage service, designed for storing and retrieving any amount of data from anywhere. Unlike block storage, it stores data as objects in buckets, where each object has data, metadata, and a unique key. Common use cases include storing static website content, backups and archives, data lakes for analytics, hosting images and videos for applications, and storing application logs. It's incredibly durable with 11 nines of durability and highly available. I use it for things like serving static assets for web apps, storing user uploads, archiving database backups, and as a data source for data processing pipelines. The pay-per-use model makes it cost-effective, and features like lifecycle policies let you automatically transition older data to cheaper storage classes. It's one of the most versatile AWS services."],
    },
    {
        text: "What are S3 storage classes?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 offers different storage classes optimized for different access patterns and cost requirements. Standard is for frequently accessed data with low latency. Standard-IA is for infrequently accessed data that you still need quick access to when required - it's cheaper but charges for retrieval. One Zone-IA is even cheaper but stores data in a single AZ so it's less resilient. Glacier classes are for archival - Glacier Flexible Retrieval for archives with retrieval times from minutes to hours, and Glacier Deep Archive for long-term retention with retrieval times of 12 hours. There's also Intelligent-Tiering that automatically moves objects between access tiers based on usage patterns. I use Standard for active application data, Standard-IA for things like older backups I might need occasionally, and Glacier for compliance archives. The key is matching the storage class to your access patterns to optimize costs."],
    },
    {
        text: "What are S3 bucket policies vs IAM policies?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3, ValidTag.enum.iam],
        answers: ["Both control access to S3, but from different angles. IAM policies are attached to IAM identities like users, groups, or roles, and define what those identities can do across AWS resources. Bucket policies are attached directly to S3 buckets and define who can access that specific bucket and what they can do. The key difference is bucket policies can grant access to principals outside your AWS account, like making a bucket publicly readable or granting access to another AWS account. I use IAM policies to manage what my users and services can do, and bucket policies when I need to grant cross-account access or set bucket-wide permissions. For example, I'd use a bucket policy to allow CloudFront to read from a bucket serving a static website, or to grant another team's AWS account access to specific objects."],
    },
    {
        text: "What is S3 versioning?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 versioning keeps multiple versions of an object in the same bucket. When enabled, every time you upload an object with the same key, S3 creates a new version instead of overwriting the existing one. This protects against accidental deletions and overwrites. If you delete an object, S3 just adds a delete marker but the previous versions remain. You can restore any previous version when needed. I enable versioning on buckets storing critical data like backups or important application files. It's also useful during deployments - if you upload a bad version of a static asset, you can easily roll back to a previous version. The tradeoff is you pay for storage of all versions, so I typically combine it with lifecycle policies to delete old versions after a certain period."],
    },
    {
        text: "What is S3 lifecycle management?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Lifecycle management lets you automatically transition objects between storage classes or delete them based on rules you define. You create lifecycle policies with rules that say things like 'move objects to Standard-IA after 30 days' or 'transition to Glacier after 90 days and delete after one year.' You can apply rules to all objects or filter by prefix or tags. This is really valuable for managing costs - instead of manually moving old data to cheaper storage, it happens automatically. For example, I set up policies for log buckets to move logs to IA after 30 days and to Glacier after 90 days, then delete them after a year. For versioned buckets, you can also have rules to delete old versions after a certain period. It's a set-it-and-forget-it way to optimize storage costs over time."],
    },
    {
        text: "How do you configure S3 for static website hosting?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["To host a static website on S3, you enable static website hosting on the bucket and specify your index document like index.html and optionally an error document. Then you upload your HTML, CSS, JavaScript, and other static assets to the bucket. The critical step is setting the right permissions - you need a bucket policy that allows public read access to the objects, since by default S3 buckets are private. S3 gives you a website endpoint URL that you can use to access your site. For production, I usually put CloudFront in front of it to get HTTPS support, better performance with edge caching, and the ability to use a custom domain with Route 53. This setup is great for single-page applications, documentation sites, or any static content. It's highly available, scalable, and very cost-effective compared to running web servers."],
    },
    {
        text: "What are presigned URLs and when would you use them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Presigned URLs give temporary access to private S3 objects without requiring AWS credentials. You generate a URL using your credentials that includes an expiration time, and anyone with that URL can perform the specified action like downloading or uploading a file until it expires. This is really useful for giving users temporary access to files. For example, if users upload profile pictures to your app, you can store them in a private S3 bucket, then generate presigned URLs when users need to view them. Or for file downloads, you generate a presigned URL that expires in a few minutes. This way your bucket stays private and secure, but users can still interact with objects through your application. I typically set short expiration times, like 5-15 minutes, to minimize security risks if URLs get leaked."],
    },
    {
        text: "What is S3 Transfer Acceleration?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Transfer Acceleration speeds up uploads to S3 by routing data through CloudFront's edge locations. Instead of uploading directly to your bucket's region, data goes to the nearest edge location and then travels to S3 over AWS's optimized network paths. This can significantly reduce latency for uploads from geographically distant locations. You enable it on a bucket and then use a special endpoint for transfers. It's most beneficial when you're transferring large files or have users spread across the globe uploading data to a centralized bucket. The tradeoff is additional cost per GB transferred, so I only use it when the performance improvement justifies the cost, like for applications with global users uploading media files or when doing large data migrations from remote locations."],
    },
    {
        text: "How do you handle cross-region replication?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["Cross-region replication automatically copies objects from a source bucket to a destination bucket in a different region. You need versioning enabled on both buckets, then you create a replication rule that specifies what to replicate - all objects or filtered by prefix or tags. You also set up an IAM role that gives S3 permission to replicate on your behalf. Objects uploaded after enabling replication are automatically copied; existing objects need a separate batch operation. I use cross-region replication for disaster recovery, to reduce latency by having data closer to users in different regions, or for compliance requirements around data locality. For example, you might replicate critical application data to another region so if the primary region has issues, you can failover. The data is replicated asynchronously, usually within minutes."],
    },
    {
        text: "What is the difference between S3 and EBS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.s3],
        answers: ["S3 is object storage accessed via HTTP APIs, while EBS is block storage that attaches to EC2 instances like a hard drive. S3 is designed for storing files you access occasionally over the network, with virtually unlimited storage and 11 nines of durability. EBS is designed for high-performance workloads that need low-latency block-level access, like databases or operating system volumes, but an EBS volume can only attach to one instance at a time in the same availability zone. I use S3 for backups, static assets, data lakes, and file storage - anywhere I need durable, scalable object storage. I use EBS for database storage, application file systems, and anything needing consistent IOPS or low latency. They're complementary - you might run a database on EBS and backup to S3."],
    },

    // Lambda
    {
        text: "What is AWS Lambda?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.serverless],
        answers: ["Lambda is AWS's serverless compute service where you run code without managing servers. You upload your function code, specify the runtime like Node.js or Python, and Lambda handles all the infrastructure - provisioning, scaling, patching, and high availability. You only pay for the compute time when your code is actually running. Lambda functions are event-driven, triggered by things like HTTP requests via API Gateway, file uploads to S3, messages in SQS, or scheduled events. Each function has a handler that processes events and can run for up to 15 minutes. It's great for microservices, API backends, data processing, automation tasks, and anything with variable or unpredictable load. The main advantages are no server management, automatic scaling, and cost efficiency for workloads that don't run continuously."],
    },
    {
        text: "What are Lambda triggers?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda triggers are the event sources that invoke your Lambda functions. Common triggers include API Gateway for HTTP requests, S3 for object events like uploads or deletions, DynamoDB Streams for database changes, SQS for processing messages from queues, SNS for pub-sub notifications, EventBridge for scheduled or custom events, and CloudWatch Events for AWS service events. Each trigger passes event data to your function in a specific format. For example, an S3 trigger includes details about the bucket and object key, while an API Gateway trigger includes the HTTP request details. You configure triggers when you set up your function, and Lambda handles polling or listening for events automatically. A single function can have multiple triggers, or you can have different functions for different event types."],
    },
    {
        text: "How do you handle Lambda cold starts?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.performance],
        answers: ["Cold starts happen when Lambda needs to spin up a new execution environment for your function, which adds latency to the first request. To minimize impact, I keep functions small and minimize dependencies - fewer packages mean faster initialization. Moving initialization code outside the handler function means it only runs once per container, not per invocation. Choosing faster languages helps too - compiled languages like Go or newer Node.js runtimes start quicker than Java. For critical paths, you can use provisioned concurrency which keeps execution environments warm and ready, though it costs more. Increasing memory allocation also gives more CPU which can speed up initialization. In practice, I optimize code and accept some cold starts for infrequent functions, and use provisioned concurrency only for latency-sensitive user-facing APIs."],
    },
    {
        text: "What are Lambda layers?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda layers are packages of libraries, custom runtimes, or other dependencies that you can share across multiple functions. Instead of bundling the same dependencies with every function, you create a layer once and attach it to multiple functions. This reduces deployment package size and makes it easier to share common code. For example, I create layers for commonly used libraries or SDKs, custom utility functions that multiple functions need, or monitoring tools. Layers are versioned and extracted to the /opt directory in the execution environment. You can attach up to five layers per function. This is especially useful for large dependencies that would otherwise make deployment packages unwieldy, and it makes updates easier - change the layer once instead of updating every function."],
    },
    {
        text: "How do you configure Lambda memory and timeout?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["You configure Lambda memory from 128 MB to 10 GB, and timeout from 1 second to 15 minutes. The key insight is that memory allocation also determines CPU power - more memory gives you proportionally more CPU and network bandwidth. So sometimes increasing memory can actually reduce costs by making your function run faster and finish in less time. I start with the default 128 MB and test, then increase if the function is timing out or running slow. For CPU-intensive tasks, more memory helps significantly. For timeout, I set it based on expected execution time plus buffer - if a function normally takes 2 seconds, I might set a 10 second timeout. You don't want it too high because you pay for execution time, but too low and legitimate requests might fail. Monitoring helps tune these values over time."],
    },
    {
        text: "What is API Gateway and how does it integrate with Lambda?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["API Gateway is AWS's fully managed service for creating REST and WebSocket APIs. It acts as the front door for your Lambda functions, handling HTTP requests and routing them to the appropriate function. You define resources and methods like GET /users or POST /orders, and map each to a Lambda function. API Gateway handles authentication, rate limiting, request validation, and response transformation. It also provides features like API keys, usage plans, and custom domain names. The integration is straightforward - API Gateway invokes your Lambda function with the HTTP request details, your function processes it and returns a response, and API Gateway sends that back to the client. Together they make a powerful serverless API solution with no servers to manage, automatic scaling, and you only pay for actual requests."],
    },
    {
        text: "What are Lambda environment variables and secrets management?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda],
        answers: ["Lambda environment variables let you pass configuration to your function without hardcoding values. You set key-value pairs in the Lambda configuration and access them in your code through process.env or equivalent. Lambda automatically encrypts them at rest using KMS. For sensitive data like API keys or database passwords, I use AWS Secrets Manager or Parameter Store instead of environment variables. Your function retrieves secrets at runtime using the AWS SDK. This keeps secrets out of your code and configurations, and you can rotate them without redeploying functions. You can also cache secrets within your function to reduce API calls. Some teams use environment variables for non-sensitive config like feature flags or region settings, and Secrets Manager for credentials. The key is never committing secrets to code and using appropriate encryption for sensitive data."],
    },
    {
        text: "How do you monitor Lambda functions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.lambda, ValidTag.enum.monitoring],
        answers: ["Lambda automatically integrates with CloudWatch for monitoring. Execution logs go to CloudWatch Logs, and metrics like invocations, duration, errors, and throttles go to CloudWatch Metrics. I set up CloudWatch alarms on error rates and duration to catch issues. For deeper insights, I use X-Ray for distributed tracing to see how requests flow through my functions and identify bottlenecks. I also add custom logging with structured JSON so I can search and analyze logs easily. For production, I instrument functions with custom metrics using CloudWatch embedded metrics or third-party tools like Datadog. The key metrics I watch are invocation count, error rate, duration, throttles, and concurrent executions. I also monitor costs since Lambda charges per invocation and duration. Good monitoring helps you understand function behavior and troubleshoot issues quickly."],
    },

    // IAM
    {
        text: "What is IAM and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam, ValidTag.enum.security],
        answers: ["IAM is AWS's Identity and Access Management service that controls who can access what in your AWS account. It works on a deny-by-default model - everything is blocked unless explicitly allowed. You create identities like users for individuals, groups to organize users, and roles for applications or services. Then you attach policies that define permissions using JSON documents. When a request comes in, IAM evaluates all applicable policies to determine if it's allowed. Policies specify actions like s3:GetObject, resources using ARNs, and optionally conditions like IP address or time of day. IAM is fundamental to AWS security - you use it to give developers access to specific resources, allow EC2 instances to access S3, or let Lambda functions write to DynamoDB. The key is following least privilege - only grant the minimum permissions needed."],
    },
    {
        text: "What are IAM users, groups, and roles?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM users represent individual people or services with permanent credentials like passwords or access keys. Groups are collections of users that you can attach policies to, making it easier to manage permissions - like a developers group or admins group. Roles are identities that can be assumed temporarily by users, applications, or AWS services. The key difference is roles don't have permanent credentials - they use temporary security credentials when assumed. I create users for people who need AWS console or CLI access, use groups to organize users by job function, and use roles for applications running on EC2 or Lambda. Roles are more secure than access keys because the credentials rotate automatically. For example, an EC2 instance assumes a role to get temporary credentials to access S3, rather than storing access keys on the instance."],
    },
    {
        text: "What are IAM policies and how do you write them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM policies are JSON documents that define permissions. They have statements with an effect of Allow or Deny, actions that specify what API calls are permitted, and resources that define what the actions apply to using ARNs. You can also add conditions for more granular control. For example, a policy might allow s3:GetObject on a specific bucket, or allow ec2:StartInstance but only during business hours. I write policies by starting with AWS managed policies as templates, then customize for specific needs. The key is being specific - instead of allowing all S3 actions, specify exactly what's needed like GetObject and PutObject. I use wildcards carefully and test policies with the IAM policy simulator before deploying. Good policy design follows least privilege and makes intent clear through naming and structure."],
    },
    {
        text: "What is the principle of least privilege in IAM?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam, ValidTag.enum.security],
        answers: ["Least privilege means giving users and services only the minimum permissions they need to do their job, nothing more. Instead of granting broad permissions like full S3 access, you'd give access to only the specific buckets and actions required. This limits the damage if credentials are compromised. In practice, I start by granting no permissions, then add what's needed as requirements become clear. I avoid using wildcard permissions in production and regularly audit to remove unused permissions. For example, if a Lambda function only reads from one DynamoDB table, its role should allow only GetItem and Query on that specific table, not all DynamoDB actions on all tables. IAM Access Analyzer helps identify overly permissive policies. The challenge is balancing security with developer productivity, but least privilege is essential for a strong security posture."],
    },
    {
        text: "What are IAM conditions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["IAM conditions add context-based logic to policies, letting you allow or deny actions based on factors beyond just who and what. You use condition operators like StringEquals, IpAddress, or DateGreaterThan with condition keys like aws:SourceIp, aws:CurrentTime, or resource tags. For example, you can allow EC2 actions only from your office IP range, require MFA for sensitive operations, or allow deleting objects only if they have a specific tag. I use conditions to enforce security requirements like ensuring S3 uploads are encrypted, restricting actions to specific times or locations, or requiring encryption in transit. Conditions make policies more dynamic and secure. They're powerful but need to be tested carefully to avoid accidentally blocking legitimate access."],
    },
    {
        text: "What are service-linked roles?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["Service-linked roles are predefined IAM roles that give AWS services the permissions they need to perform actions on your behalf. They're created and managed by AWS services automatically. For example, Auto Scaling creates a service-linked role to launch and terminate EC2 instances, or ECS creates one to manage tasks and services. The permissions are defined by AWS and you can't modify them, which ensures the service has exactly what it needs without giving excess permissions. These roles are created automatically when you use a service, or you can create them manually if needed. You can't delete them while the service is using them. Service-linked roles simplify setup since you don't have to figure out what permissions a service needs - AWS handles that. They're part of the security best practice of least privilege at the service level."],
    },
    {
        text: "What is the difference between identity-based and resource-based policies?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["Identity-based policies attach to IAM identities like users, groups, or roles and define what those identities can do. Resource-based policies attach to resources like S3 buckets or Lambda functions and define who can access those resources. The key difference is perspective - identity-based says 'this user can access these resources', while resource-based says 'these users can access this resource'. Resource-based policies are useful for cross-account access since you can grant access to principals from other accounts. For example, an S3 bucket policy is resource-based and can allow another account's users to access it. IAM policies on a user are identity-based. In many cases you need both - the identity needs permission to perform the action, and the resource needs to allow access to that identity."],
    },
    {
        text: "How do you audit IAM permissions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.iam],
        answers: ["I use several tools to audit IAM permissions regularly. IAM Access Analyzer identifies resources shared with external entities and helps find overly permissive policies. Credential reports show all users and their credential status, helping identify unused accounts. CloudTrail logs all API calls so you can see what actions are actually being used. IAM's Last Accessed information shows when policies or permissions were last used, making it easy to identify and remove unused permissions. I also use the IAM policy simulator to test whether specific actions would be allowed before deploying policies. For automation, I set up automated scans to check for things like users with console access but no MFA, unused access keys older than 90 days, or overly broad permissions. Regular audits help maintain security and ensure you're following least privilege."],
    },

    // RDS
    {
        text: "What is RDS and how does it differ from self-managed databases?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["RDS is AWS's managed relational database service supporting engines like MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. The main difference from self-managed databases on EC2 is that AWS handles the operational heavy lifting - automated backups, patching, failover, and scaling. With RDS, you don't manage the underlying infrastructure or database software, you just configure what you need and AWS keeps it running. You lose some flexibility like SSH access to the host or certain advanced configurations, but you gain reliability and reduced operational overhead. I choose RDS when I want to focus on the application rather than database administration, which is most of the time. I'd only self-manage on EC2 if I need specific database configurations that RDS doesn't support or have very specialized requirements."],
    },
    {
        text: "What are RDS Multi-AZ deployments?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum["high-availability"]],
        answers: ["Multi-AZ deployments provide high availability by automatically replicating your database to a standby instance in a different availability zone. RDS synchronously replicates data to the standby, so if the primary instance fails, RDS automatically fails over to the standby with minimal downtime, usually under a minute. The standby isn't used for read traffic - it's purely for failover. This protects against AZ failures, instance failures, and even during maintenance operations like patching. I enable Multi-AZ for production databases where downtime is unacceptable. The tradeoff is additional cost for the standby instance and slightly higher write latency due to synchronous replication. For development environments where some downtime is acceptable, I skip Multi-AZ to save costs. The failover is automatic and doesn't require changing connection strings."],
    },
    {
        text: "What are RDS read replicas?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum.scalability],
        answers: ["Read replicas are copies of your database that you can use to scale read traffic. RDS asynchronously replicates data from the source database to one or more read replicas, which means there can be some replication lag. You direct read queries to the replicas to offload the primary database, which is useful for read-heavy workloads. You can create up to five read replicas per source instance, and they can be in different regions for disaster recovery or reducing latency for global users. Unlike Multi-AZ standbys, read replicas actively serve traffic. You can also promote a read replica to become a standalone database. I use read replicas when I have analytics queries or reporting that would impact production performance, or when I need to serve read traffic from multiple geographic regions. The main limitation is replication lag - reads might be slightly behind writes."],
    },
    {
        text: "How does RDS backup and recovery work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["RDS provides automated backups and manual snapshots. Automated backups happen daily during a backup window you specify, and RDS also captures transaction logs throughout the day. This lets you restore to any point in time within your retention period, which can be 1 to 35 days. The backup process uses the standby instance in Multi-AZ deployments to avoid I/O impact. Manual snapshots are user-initiated and you keep them until you explicitly delete them - useful before major changes or for long-term retention. To restore, you create a new RDS instance from a snapshot or to a specific point in time. You can't restore over an existing instance. I typically set a 7-day retention for automated backups, take manual snapshots before major migrations, and copy critical snapshots to another region for disaster recovery. Backups are stored in S3 but managed by RDS."],
    },
    {
        text: "What is RDS Performance Insights?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum.performance],
        answers: ["Performance Insights is a monitoring tool that helps you analyze and troubleshoot database performance. It provides a dashboard showing database load over time and breaks down where that load is coming from - which SQL queries, users, or hosts are consuming resources. You can identify slow queries, see wait events that indicate bottlenecks, and drill into specific time periods to understand performance issues. It collects and displays up to two years of performance data. The key feature is the ability to quickly identify the top SQL queries causing load, then examine their execution plans and statistics. I enable Performance Insights on production databases to proactively monitor performance and quickly troubleshoot when issues arise. It's particularly useful during incidents to identify whether a performance problem is due to a problematic query, resource constraint, or external factor."],
    },
    {
        text: "How do you scale RDS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds, ValidTag.enum.scalability],
        answers: ["You can scale RDS vertically or horizontally. Vertical scaling means changing the instance type to get more CPU, memory, or I/O capacity - this requires downtime unless you're using Multi-AZ. You can also scale storage capacity and IOPS independently. Horizontal scaling for reads uses read replicas to distribute read traffic across multiple databases. For writes, you're limited to the capacity of the primary instance, though Aurora has some additional options. I also scale by optimizing queries, adding indexes, and using caching to reduce database load. For predictable load changes, I schedule scaling during maintenance windows. For variable workloads, Aurora Serverless can automatically scale capacity. The key is monitoring metrics like CPU, connections, and IOPS to know when to scale, and load testing to validate that scaling improves performance as expected."],
    },
    {
        text: "What is Amazon Aurora and how does it differ from RDS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.rds],
        answers: ["Aurora is AWS's cloud-native database engine that's MySQL and PostgreSQL compatible, but it's built from the ground up for the cloud. The main differences are performance and architecture - Aurora uses a distributed storage layer that replicates data six ways across three AZs automatically, making it much more resilient. It's faster than standard MySQL or PostgreSQL on RDS, with up to 5x throughput for MySQL and 3x for PostgreSQL. Aurora supports up to 15 read replicas with lower lag, and features like Aurora Serverless that automatically scale capacity. The storage automatically grows up to 128 TB. The tradeoff is cost - Aurora is more expensive than standard RDS. I choose Aurora for production workloads that need high performance, availability, and scale, and standard RDS for less demanding workloads where cost is a bigger concern."],
    },

    // Cognito
    {
        text: "What is AWS Cognito?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.auth],
        answers: ["Cognito is AWS's managed authentication and user management service. It handles user sign-up, sign-in, and access control for web and mobile apps without you having to build auth infrastructure from scratch. The main components are user pools for authentication and user directories, and identity pools for granting AWS credentials to users. Cognito supports email/password authentication, social login through providers like Google and Facebook, and SAML federation for enterprise identity providers. It includes features like MFA, password policies, account recovery, and email verification. I use Cognito when building apps that need user authentication because it's secure out of the box, scales automatically, and integrates well with other AWS services. It handles the security complexities so you can focus on your application logic."],
    },
    {
        text: "What are user pools vs identity pools?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito],
        answers: ["User pools are user directories that handle authentication - sign-up, sign-in, password management, and user profiles. They give you JWT tokens when users authenticate successfully. Identity pools handle authorization by providing temporary AWS credentials so authenticated users can access AWS resources like S3 or DynamoDB. You typically use both together - the user pool authenticates the user and issues tokens, then the identity pool exchanges those tokens for AWS credentials with specific IAM permissions. For example, a mobile app might use a user pool to let users log in, then use an identity pool to give them temporary credentials to upload photos to S3. User pools are about who the user is, identity pools are about what AWS resources they can access."],
    },
    {
        text: "How does Cognito authentication flow work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.auth],
        answers: ["The standard flow starts when a user signs up or signs in through your app. For sign-up, the user provides credentials and Cognito creates the account, optionally sending verification codes via email or SMS. For sign-in, Cognito validates the credentials and returns JWT tokens - an ID token with user attributes, an access token for authorizing API calls, and a refresh token for getting new tokens. Your app includes the ID or access token in requests to your backend, which validates the token signature to confirm it's legitimate. If you're using identity pools, you exchange the user pool token for temporary AWS credentials. The tokens expire after a set time, and you use the refresh token to get new ones without making the user log in again. The flow handles challenges like MFA or password changes as part of the authentication process."],
    },
    {
        text: "What are Cognito triggers?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito],
        answers: ["Cognito triggers are Lambda functions that run at specific points in the authentication flow, letting you customize behavior. There are pre-authentication triggers that run before sign-in to add custom validation, pre-sign-up triggers to validate or modify user data before account creation, custom message triggers to customize emails and SMS, post-confirmation triggers after verification, and pre-token-generation triggers to modify claims in the JWT. For example, I've used pre-sign-up triggers to enforce custom username policies, custom message triggers to send branded emails, and pre-token-generation to add custom claims like user roles to the token. Triggers give you flexibility to integrate with existing systems, add custom validation, or enrich the user data. They're synchronous, so they need to respond quickly."],
    },
    {
        text: "How do you implement social login with Cognito?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito, ValidTag.enum.oauth],
        answers: ["To implement social login, you configure identity providers in your Cognito user pool by registering your app with providers like Google, Facebook, or Amazon and getting OAuth credentials. You add these credentials to Cognito and specify which attributes to map from the provider to your user pool. When users choose social login, they're redirected to the provider's login page, authenticate there, and get redirected back to your app with tokens. Cognito creates or updates the user in the pool and issues its own tokens. The user can then log in with either their social account or a password if you've enabled both. I typically enable multiple social providers to give users options, and use attribute mapping to ensure consistent user data regardless of how they sign in. Cognito handles the OAuth flow complexity automatically."],
    },
    {
        text: "What is Cognito hosted UI?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito],
        answers: ["Cognito hosted UI is a pre-built, customizable login page that AWS hosts for you. Instead of building your own sign-in and sign-up forms, you can redirect users to the hosted UI and Cognito handles the interface. It supports all authentication methods including username/password, social providers, and SAML, and includes pages for sign-in, sign-up, forgot password, and MFA. You can customize the logo, CSS, and some layout aspects to match your branding. After authentication, users are redirected back to your app with tokens. I use the hosted UI when I need to get authentication working quickly or when the built-in pages meet my needs. For more customization, you can build your own UI using the Cognito SDK, but the hosted UI is faster to set up and maintained by AWS."],
    },
    {
        text: "How do you customize Cognito emails and SMS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cognito],
        answers: ["For basic customization, you can edit the message templates directly in the Cognito console, changing the text and including placeholders for dynamic values like the verification code. For more advanced customization like HTML emails or custom logic, you use a custom message Lambda trigger that receives the message parameters and returns your custom content. You can also use Amazon SES for email sending instead of Cognito's default sender to get better deliverability, custom from addresses, and email tracking. For SMS, you configure the SNS settings and can customize the message template. The Lambda trigger approach gives you full control - I've used it to send branded HTML emails, include user-specific content, or integrate with third-party email services. Just make sure the Lambda function is fast and handles the verification code securely."],
    },

    // Other AWS
    {
        text: "What is CloudFront and how do you configure it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cloudfront, ValidTag.enum.cdn],
        answers: ["CloudFront is AWS's content delivery network that caches and serves content from edge locations around the world, reducing latency for users. You create a distribution that points to an origin like an S3 bucket, ALB, or custom server. CloudFront caches content at edge locations based on TTL settings and cache behaviors. You can configure multiple origins, path patterns to route requests, custom headers, HTTPS certificates, and access restrictions. Features include custom error pages, geographic restrictions, and Lambda@Edge for running code at the edge. I use CloudFront for serving static websites from S3, accelerating API responses, or distributing media files globally. The configuration includes setting cache policies to balance freshness and performance, invalidating cached content when needed, and using signed URLs for private content. It integrates well with other AWS services and significantly improves performance for global users."],
    },
    {
        text: "What is Route 53 and how does DNS work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.route53],
        answers: ["Route 53 is AWS's DNS service that translates domain names to IP addresses. When you register a domain or configure DNS, you create a hosted zone with record sets like A records for IPv4 addresses, CNAME records for aliasing, MX records for email, and Alias records which are AWS-specific and can point to CloudFront, ALB, or S3 without additional charges. Route 53 supports routing policies like simple routing, weighted for A/B testing, latency-based to route users to the fastest region, failover for disaster recovery, and geolocation for serving different content by location. Health checks monitor endpoints and trigger failover if needed. I use Route 53 for domain management, traffic routing across regions, and blue-green deployments by switching DNS records. The global network ensures low-latency DNS resolution."],
    },
    {
        text: "What is SQS and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.sqs, ValidTag.enum["message-queues"]],
        answers: ["SQS is AWS's message queuing service that decouples components by allowing asynchronous communication. Producers send messages to a queue, and consumers poll and process them independently. There are two types: standard queues offer maximum throughput with at-least-once delivery and best-effort ordering, while FIFO queues guarantee exactly-once processing and preserve message order. Messages can have a visibility timeout so they're hidden while being processed, and a retention period up to 14 days. I use SQS to decouple microservices, buffer workloads during traffic spikes, distribute tasks to workers, or implement retry logic with dead-letter queues for failed messages. For example, you might queue image processing jobs triggered by uploads, or buffer orders before processing them. SQS is fully managed, scales automatically, and is great for making systems more resilient."],
    },
    {
        text: "What is SNS and how does it differ from SQS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.sns, ValidTag.enum.sqs],
        answers: ["SNS is a pub-sub messaging service where publishers send messages to topics, and multiple subscribers receive them simultaneously. The key difference is SNS pushes messages to subscribers while SQS requires consumers to poll. SNS is for broadcasting messages to multiple consumers - like sending a notification to email, SMS, Lambda functions, and SQS queues all at once. SQS is for point-to-point communication where you want to decouple and queue work. A common pattern is to combine them - use SNS to fan out messages to multiple SQS queues, each consumed by different services. For example, an order event might publish to SNS, which triggers a Lambda for email notifications, pushes to SQS for inventory updates, and sends to another queue for analytics. SNS is for real-time notifications, SQS is for reliable asynchronous processing."],
    },
    {
        text: "What is EventBridge?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum["event-driven"]],
        answers: ["EventBridge is AWS's serverless event bus that routes events between AWS services, your applications, and SaaS providers. You can create rules that pattern-match events and route them to targets like Lambda, Step Functions, SQS, or SNS. It supports scheduled events like cron jobs, AWS service events like EC2 state changes, custom events from your applications, and events from SaaS partners. The event patterns let you filter based on event content to only trigger relevant targets. I use EventBridge for building event-driven architectures - for example, triggering workflows when S3 objects are created, running scheduled maintenance jobs, or integrating with external systems. It's more flexible than CloudWatch Events with better filtering, schema registry, and SaaS integrations. EventBridge makes it easy to build loosely coupled systems that react to events."],
    },
    {
        text: "What is DynamoDB and when would you choose it?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.dynamodb],
        answers: ["DynamoDB is AWS's fully managed NoSQL database that provides single-digit millisecond latency at any scale. It's a key-value and document database where you define a partition key and optionally a sort key, and it distributes data automatically. You can use on-demand pricing for unpredictable workloads or provisioned capacity for consistent traffic. Features include auto-scaling, global tables for multi-region replication, DynamoDB Streams for change data capture, and transactions for ACID operations. I choose DynamoDB for high-throughput applications with simple access patterns, like user sessions, gaming leaderboards, mobile backends, or IoT data. It excels when you need predictable performance at scale and can model your data to use partition keys effectively. For complex queries or joins, RDS might be better. The key is designing your schema around your access patterns rather than normalizing data."],
    },
    {
        text: "What is ElastiCache?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.elasticache, ValidTag.enum.caching],
        answers: ["ElastiCache is AWS's managed in-memory caching service supporting Redis and Memcached. It provides microsecond latency for read-heavy workloads by caching frequently accessed data in memory. Redis supports advanced data structures, persistence, replication, and clustering, while Memcached is simpler and good for basic caching. Common use cases include database query caching, session storage, real-time analytics, and leaderboards. I use ElastiCache to reduce database load by caching query results, improve application performance by storing frequently accessed data, or as a session store for distributed applications. For example, caching product catalogs in an e-commerce site or user profiles in a social app. ElastiCache handles provisioning, patching, monitoring, and failure recovery. The key is implementing proper cache invalidation strategies and monitoring hit rates to ensure effectiveness."],
    },
    {
        text: "What are CloudWatch metrics, logs, and alarms?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.cloudwatch, ValidTag.enum.monitoring],
        answers: ["CloudWatch is AWS's monitoring and observability service with three main components. Metrics are numerical data points collected over time like CPU utilization or request count - AWS services publish metrics automatically, and you can publish custom metrics. Logs capture text data from applications, Lambda functions, or AWS services, organized into log groups and streams. Alarms watch metrics and trigger actions like SNS notifications or auto-scaling when thresholds are breached. I set up CloudWatch to monitor application health, track custom business metrics, aggregate logs for analysis, and alert on-call engineers when issues occur. For example, I'll create alarms for high error rates, configure log insights queries to analyze patterns, and build dashboards to visualize system health. CloudWatch integrates with all AWS services and is essential for maintaining operational visibility."],
    },
    {
        text: "What is AWS Secrets Manager vs Parameter Store?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.aws, ValidTag.enum.security],
        answers: ["Both store configuration and secrets, but with different features and pricing. Secrets Manager is designed specifically for secrets like database credentials and API keys, with automatic rotation, fine-grained access control, and cross-region replication. It costs per secret stored and API call. Parameter Store is part of Systems Manager and can store any configuration data or secrets. The free tier supports up to 10,000 parameters with standard throughput, and advanced parameters cost per parameter. Parameter Store doesn't have built-in rotation but is cheaper. I use Secrets Manager for credentials that need rotation like database passwords or API keys, where the automatic rotation justifies the cost. I use Parameter Store for general configuration like feature flags, environment settings, or secrets that don't need rotation. Both encrypt data and integrate with IAM for access control."],
    },

    // GCP
    {
        text: "What are the core GCP services and how do they map to AWS?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.aws],
        answers: ["The core GCP services parallel AWS quite closely. Compute Engine is like EC2 for virtual machines, Cloud Run is container-based serverless similar to Fargate, Cloud Functions matches Lambda for serverless functions. Cloud Storage is equivalent to S3 for object storage, Cloud SQL is like RDS for managed databases, and Cloud Spanner is a globally distributed database unique to GCP. For networking, VPC works similarly to AWS VPC. BigQuery is GCP's data warehouse comparable to Redshift but serverless. Pub/Sub is like SNS/SQS combined for messaging. Firebase is mobile/web app platform with Firestore as a NoSQL database similar to DynamoDB. The main philosophical difference is GCP tends toward more managed, serverless options and has stronger data analytics services, while AWS offers more configuration options and the broadest service catalog. Both achieve similar outcomes with different approaches."],
    },
    {
        text: "What is Cloud Run?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-run"]],
        answers: ["Cloud Run is GCP's serverless container platform where you deploy containerized applications without managing infrastructure. You give it a container image and Cloud Run handles everything - deploying, scaling to zero when idle, scaling up based on requests, and load balancing. It's built on Knative so you can port to Kubernetes if needed. You're billed per request and compute time with automatic scaling from zero to thousands of instances. Cloud Run is great for APIs, web apps, microservices, or any stateless workload you can containerize. Compared to Cloud Functions, Cloud Run gives you more control since you bring your own container, supports any language or runtime, and can handle larger deployments up to 60 minutes per request. I use it when I want the simplicity of serverless but need more flexibility than functions provide."],
    },
    {
        text: "What is Cloud Functions?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-functions"]],
        answers: ["Cloud Functions is GCP's serverless functions platform, similar to AWS Lambda. You write individual functions in supported languages like Node.js, Python, or Go, and they run in response to events like HTTP requests, Cloud Storage changes, Pub/Sub messages, or Firebase events. The runtime, scaling, and infrastructure are fully managed. Functions can run for up to 9 minutes and automatically scale based on load. I use Cloud Functions for event-driven tasks like processing uploaded files, triggering workflows, sending notifications, or building webhooks. The advantages are zero infrastructure management, automatic scaling, and pay-per-use pricing. The main limitation compared to Cloud Run is less control over the environment and dependencies. Cloud Functions is perfect for small, focused tasks that respond to events."],
    },
    {
        text: "What is Cloud Storage?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["cloud-storage"]],
        answers: ["Cloud Storage is GCP's object storage service, equivalent to S3. It stores data as objects in buckets with global unique names and provides different storage classes: Standard for frequently accessed data, Nearline for monthly access, Coldline for quarterly, and Archive for annual access. Features include versioning, lifecycle management, access controls via IAM and ACLs, signed URLs for temporary access, and strong consistency. Cloud Storage integrates well with other GCP services and can trigger Cloud Functions on object events. I use it for backups, static website hosting, data lakes, media storage, and archival. The pricing model charges for storage, operations, and network egress. Compared to S3, Cloud Storage has simpler storage classes and better performance for certain workloads, but both services are quite similar in capabilities."],
    },
    {
        text: "What is Cloud SQL vs Cloud Spanner?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp],
        answers: ["Cloud SQL is GCP's managed relational database service for MySQL, PostgreSQL, and SQL Server, similar to AWS RDS. It's regional, handles automatic backups, replication, and patching. Cloud Spanner is a globally distributed, horizontally scalable relational database that offers strong consistency and SQL support across regions - unique to GCP. Cloud SQL is for traditional relational workloads within a region with predictable performance at lower cost. Cloud Spanner is for applications needing global distribution, automatic sharding, and the ability to scale horizontally while maintaining ACID transactions. For example, I'd use Cloud SQL for a regional web app's database, but Cloud Spanner for a global application needing consistent reads and writes across continents. Spanner is more expensive but provides capabilities traditional databases can't match at scale."],
    },
    {
        text: "What is BigQuery?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp],
        answers: ["BigQuery is GCP's fully managed, serverless data warehouse designed for analytics on massive datasets using SQL. You load data from Cloud Storage, streaming sources, or other databases, then run SQL queries that can analyze terabytes in seconds. It separates storage and compute so you can store large amounts of data cheaply and only pay for queries when you run them. BigQuery supports nested and repeated fields, streaming inserts, partitioning and clustering for performance, and integration with BI tools. I use BigQuery for data analytics, business intelligence, machine learning with BigQuery ML, and log analysis. It's great when you need to query large datasets without managing infrastructure or optimizing indexes. The serverless nature means it scales automatically and you don't worry about capacity planning, just pay for storage and queries."],
    },
    {
        text: "What is Firebase and how does it integrate with GCP?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.firestore],
        answers: ["Firebase is Google's mobile and web app development platform with tools for authentication, real-time databases, file storage, hosting, and analytics. Firestore is Firebase's NoSQL document database offering real-time synchronization and offline support. Firebase integrates tightly with GCP - Firebase projects are GCP projects, you can use GCP services from Firebase apps, and Firebase Authentication works with Cloud Identity. Firestore data can be analyzed in BigQuery, Cloud Functions can respond to Firebase events, and you can use Cloud Storage from Firebase apps. I use Firebase for rapid mobile app development because it provides backend services out of the box. The real-time database and authentication are particularly strong. For web apps, Firebase Hosting with Cloud Functions makes deployment easy. The GCP integration means you can start with Firebase and expand to full GCP services as needed."],
    },
    {
        text: "What is Cloud IAM?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum.security],
        answers: ["Cloud IAM controls access to GCP resources through policies that bind members to roles at different hierarchy levels. Members can be users, service accounts, groups, or domains. Roles define permissions - there are primitive roles like Owner, Editor, and Viewer, and predefined roles for specific services with granular permissions. You can also create custom roles. Policies are set at the project, folder, or organization level and inherited down the hierarchy. Service accounts are for applications to authenticate to GCP services. The principle is similar to AWS IAM but the structure differs - GCP uses resource hierarchy and role binding while AWS uses policies attached to identities or resources. I use Cloud IAM to grant developers access to specific projects, give applications service account permissions, and implement least privilege. The organization hierarchy makes multi-project management cleaner than AWS."],
    },
    {
        text: "What is Pub/Sub?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.gcp, ValidTag.enum["pub-sub"]],
        answers: ["Pub/Sub is GCP's messaging service for asynchronous communication between services. Publishers send messages to topics, and subscribers receive messages from subscriptions to those topics. It's similar to SNS/SQS combined but with more flexibility. Pub/Sub offers at-least-once delivery, automatic scaling, global distribution, and message ordering within a key. You can have push subscriptions that deliver to HTTP endpoints or pull subscriptions where consumers poll. Features include message filtering, dead-letter topics, and exactly-once delivery. I use Pub/Sub for event-driven architectures, streaming data ingestion, distributing tasks to workers, or decoupling microservices. For example, you might publish user events that trigger multiple downstream processes like analytics, notifications, and data warehouse updates. Pub/Sub handles massive scale and ensures reliable message delivery without managing infrastructure."],
    },

    // Docker (mid-level only - basics in junior-advanced, advanced in mid-advanced)
    {
        text: "What are Docker volumes and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.volumes],
        answers: ["Docker volumes are the preferred way to persist data generated by containers. Unlike container filesystems which are ephemeral, volumes persist independently of container lifecycles. There are named volumes managed by Docker, bind mounts that map to specific host paths, and tmpfs mounts for temporary in-memory storage. Named volumes are created with 'docker volume create' or automatically when specified in a run command, stored in Docker's directory structure, and can be shared between containers. Bind mounts directly mount a host directory into the container, useful for development. I use named volumes for database data, application logs, or any data that needs to survive container restarts. Bind mounts are great for local development where you want code changes to immediately reflect in the container. Volumes are managed independently and can be backed up, migrated, or shared across containers safely."],
    },
    {
        text: "What is Docker networking?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.networks],
        answers: ["Docker networking allows containers to communicate with each other and the outside world. There are several network types: bridge is the default where containers on the same bridge network can communicate, host removes network isolation and uses the host's network directly, none disables networking, and overlay enables communication across multiple Docker hosts. When containers are on the same user-defined bridge network, they can communicate using container names as hostnames thanks to built-in DNS. Port mapping with -p exposes container ports to the host. I create custom bridge networks for applications so containers can communicate securely by name, use host networking when I need maximum network performance, and overlay networks for Docker Swarm or multi-host setups. Understanding networking is crucial for microservices where containers need to communicate without hard-coded IPs."],
    },
    {
        text: "What are Docker health checks?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker],
        answers: ["Health checks let you define a command that Docker runs periodically to check if a container is healthy. You specify them in the Dockerfile with HEALTHCHECK or in docker-compose.yml. The check command returns 0 for healthy or 1 for unhealthy. Docker tracks the health status and orchestrators like Kubernetes can use this to restart unhealthy containers or remove them from load balancers. For example, a web server health check might curl its health endpoint. I define health checks to ensure containers are not just running but actually functional - the process might be up but unable to serve requests. This is crucial in production where you want automatic recovery from failures. Health checks run at intervals you specify and have configurable timeout and retries. They're a simple way to add resilience by catching issues before users do."],
    },

    // Kubernetes (mid-level only - basics in junior-advanced, advanced in mid-advanced)
    {
        text: "What is the Kubernetes architecture (control plane, nodes)?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Kubernetes has a control plane and worker nodes. The control plane manages the cluster and includes the API server which is the front-end receiving commands, etcd for storing cluster state, the scheduler which assigns pods to nodes, and controller managers that maintain desired state. Worker nodes run your workloads and include the kubelet agent that communicates with the control plane, kube-proxy for networking, and a container runtime like Docker or containerd. When you deploy an application, you send a manifest to the API server, it stores it in etcd, the scheduler chooses nodes to run pods, and kubelets on those nodes start the containers. Controllers continuously watch state and make corrections if reality drifts from desired state. The control plane can be highly available with multiple instances. Understanding this architecture helps troubleshoot issues and design robust deployments."],
    },
    {
        text: "What is a ReplicaSet?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["A ReplicaSet ensures a specified number of pod replicas are running at all times. If pods die, the ReplicaSet creates replacements. If there are too many, it deletes extras. You define the desired replica count and a pod template, and the ReplicaSet maintains that state. However, you typically don't create ReplicaSets directly - deployments create and manage them for you. The advantage of deployments over ReplicaSets is deployments handle updates elegantly by creating new ReplicaSets and transitioning traffic. ReplicaSets just maintain a static set of pods. Under the hood, when you update a deployment, it creates a new ReplicaSet for the new version while keeping the old one around for rollback. I interact with deployments which manage ReplicaSets automatically, rarely working with ReplicaSets directly."],
    },
    {
        text: "What are ConfigMaps and Secrets?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.configmaps, ValidTag.enum.secrets],
        answers: ["ConfigMaps and Secrets decouple configuration from container images. ConfigMaps store non-sensitive configuration like feature flags, URLs, or config files. Secrets store sensitive data like passwords, tokens, or keys, and are base64 encoded. You create them separately and inject them into pods as environment variables or mounted files. This lets you use the same image across environments with different configurations. The key difference is Secrets are handled more carefully - encrypted at rest in etcd if configured, access-controlled, and not logged. However, Secrets aren't super secure by default, so for production I often use external secret managers like Vault or cloud-provider solutions. I use ConfigMaps for app config and Secrets for credentials, mounting them as volumes when I need full config files or env vars for simple values."],
    },
    {
        text: "What are volumes in Kubernetes and how do they differ from Docker volumes?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.docker],
        answers: ["Kubernetes volumes provide storage to pods, with many more types than Docker volumes. Docker volumes are simple persistent storage. Kubernetes volumes support emptyDir for temporary storage that lives with the pod, hostPath for mounting host directories, persistentVolumeClaims for durable storage that outlives pods, configMaps and secrets for configuration, and cloud-provider volumes like AWS EBS or GCP Persistent Disks. The volume lifecycle is tied to the pod by default - when the pod dies, emptyDir volumes are deleted. For persistence, you use PersistentVolumes and PersistentVolumeClaims which decouple storage from pod lifecycle. PVs are cluster resources provisioned by admins or dynamically, and PVCs are requests for storage by users. This abstraction lets you write pod specs without knowing the underlying storage. I use emptyDir for temp data, PVCs for databases, and ConfigMaps for configuration files."],
    },
    {
        text: "What are labels and selectors?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Labels are key-value pairs attached to Kubernetes objects like pods for identification and organization. Selectors query objects by their labels. For example, you might label pods with app=nginx and environment=prod. Services use selectors to find which pods to route traffic to, and deployments use them to manage pods. This loose coupling is powerful - you can change which pods a service targets by changing labels without modifying the service itself. Labels can be added, modified, or removed at any time. You can query with kubectl using label selectors like 'kubectl get pods -l app=nginx'. I use labels extensively for organization, selecting resources, and routing traffic. Common labels include app name, version, environment, and tier. They're fundamental to how Kubernetes objects relate to each other and how you organize and query resources."],
    },

    // CI/CD (mid-level only - basics in junior-advanced, advanced in mid-advanced/senior)
    {
        text: "What is a CI/CD pipeline and what stages does it typically have?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum.pipeline],
        answers: ["A CI/CD pipeline is an automated workflow that takes code from version control through to production. Think of it as an assembly line for software. Each stage performs specific checks or transformations, and the code only moves forward if it passes that stage. A typical pipeline starts with a source stage that triggers when code is pushed. Then there's a build stage where the code is compiled or bundled, dependencies are installed, and artifacts are created. Next comes the test stage, which usually runs unit tests first, then integration tests, and sometimes end-to-end tests. After testing, there's often a security scanning stage that checks for vulnerabilities in dependencies and the code itself. Then you might have a staging deployment stage where the application is deployed to a test environment for further validation. Finally, there's a production deployment stage. Some pipelines include additional stages like performance testing, database migrations, or approval gates. The key principle is that each stage acts as a quality gate. If tests fail or security scans find critical issues, the pipeline stops and the team is notified. This gives you confidence that anything reaching production has been thoroughly validated."],
    },
    {
        text: "How do you implement automated testing in CI/CD?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum.testing],
        answers: ["Implementing automated testing in CI/CD is all about building a reliable safety net that catches issues before they reach production. I structure tests in a pyramid shape. At the base, you have unit tests which are fast, numerous, and test individual functions or components in isolation. These run first because they give the quickest feedback. Above that are integration tests that verify different parts of the system work together correctly, like testing that your API endpoints correctly interact with the database. At the top are end-to-end tests that simulate real user behavior, like clicking through a checkout flow. In the pipeline configuration, I set up the testing stage to run after the build succeeds. Unit tests run first, and if they fail, the pipeline stops immediately. Integration tests run next in parallel where possible to save time. E2E tests typically run against a deployed staging environment. I also include test coverage reporting to track whether we're maintaining good coverage over time. For flaky tests, I might configure retries, but the real solution is fixing the flakiness. The key is making tests fast enough that developers get feedback quickly and reliable enough that failures are always meaningful."],
    },
    {
        text: "What are GitHub Actions and how do you configure them?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum["github-actions"]],
        answers: ["GitHub Actions is GitHub's built-in CI/CD platform that lets you automate workflows directly in your repository. It's tightly integrated with GitHub, so you get seamless triggering based on GitHub events like pushes, pull requests, releases, or even issue comments. Configuration is done through YAML files in the .github/workflows directory. Each workflow file defines when it runs, what jobs it contains, and what steps each job executes. A basic workflow might trigger on push to main, spin up an Ubuntu runner, checkout the code, install dependencies, run tests, and deploy. Jobs run in parallel by default but can have dependencies on each other. Each job runs on a fresh virtual machine called a runner. GitHub provides hosted runners for Linux, macOS, and Windows, or you can use self-hosted runners for specialized needs. One of the powerful features is the marketplace of pre-built actions. Instead of writing scripts from scratch, you can use community actions for common tasks like setting up Node, caching dependencies, or deploying to AWS. You reference these actions with the uses keyword. Secrets are managed through the repository settings and accessed as environment variables. Overall, it's a great choice when you're already using GitHub since there's minimal setup and the integration is smooth."],
    },
    {
        text: "What is the difference between GitHub Actions, Jenkins, CircleCI, and GitLab CI?",
        level: Level.enum.mid,
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"], ValidTag.enum["github-actions"], ValidTag.enum.jenkins, ValidTag.enum["gitlab-ci"]],
        answers: ["Each of these CI/CD tools has its strengths and fits different contexts. GitHub Actions is tightly integrated with GitHub, which makes it ideal if that's where your code lives. Configuration is simple YAML files, and there's a huge marketplace of pre-built actions. It's free for public repos and has generous free tier for private. The downside is you're locked into the GitHub ecosystem. Jenkins is the veteran. It's open-source, extremely flexible, and can do almost anything through its massive plugin ecosystem. However, it requires self-hosting and maintenance, and the configuration can get complex. It's often the choice for enterprises with existing Jenkins infrastructure or specialized requirements. CircleCI is a cloud-native option known for good performance and caching. It works with multiple git providers and has powerful parallelism features. The configuration is clean YAML, and it scales well for larger teams. GitLab CI is integrated into GitLab, similar to how GitHub Actions is integrated with GitHub. If you use GitLab for source control, it's the natural choice. It has a complete DevOps platform including container registry, security scanning, and more. The choice often comes down to where your code lives and what ecosystem you're already using. For new projects on GitHub, GitHub Actions is usually the simplest path. For complex enterprise needs, Jenkins offers the most flexibility."],
    },
];
