import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const juniorAdvanced: QuestionForCategoryAndLevel<
    typeof Category.enum.devops,
    typeof Level.enum["junior-advanced"]
>[] = [
    // Docker Basics
    {
        text: "What is Docker and what problems does it solve?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.containers],
        answers: ["Docker is a containerization platform that packages applications with their dependencies into portable containers. The main problems it solves are environment consistency - 'works on my machine' becomes 'works everywhere' - and efficient resource utilization compared to VMs. Containers share the host OS kernel but isolate application processes, making them lightweight and fast to start. Docker solves dependency hell by bundling everything the app needs, simplifies deployment by shipping the same container across environments, and makes scaling easier since containers are identical and portable. I use Docker to ensure development matches production, make onboarding easier with consistent environments, and deploy applications across different infrastructure. The Docker ecosystem includes the Docker Engine for running containers, Docker Hub for sharing images, and tooling for building and orchestrating containers.",
            "Docker is essentially a way to package your application along with all its dependencies into a single unit called a container. The big problem it solves is the classic 'it works on my machine' issue. Before Docker, developers would spend hours debugging environment differences between local machines, staging, and production. With Docker, you build once and run anywhere because the container includes everything needed. It's also much more efficient than virtual machines since containers share the host operating system kernel rather than running their own OS. This makes them start in seconds and use far less memory. In my experience, Docker has transformed how teams deploy software - you get consistent environments, easier scaling, and simpler dependency management all in one tool."],
    },
    {
        text: "What is a Docker image vs a container?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.images, ValidTag.enum.containers],
        answers: ["A Docker image is a read-only template that contains the application code, runtime, libraries, and dependencies - basically a snapshot of everything needed to run the application. A container is a running instance of an image. Think of an image as a class and a container as an instance of that class. You build an image once using a Dockerfile, then you can create many containers from that image. Images are immutable and stored in layers, while containers have a thin writable layer on top where runtime changes happen. When you run 'docker run', you're creating a container from an image. Multiple containers can run from the same image simultaneously, each isolated from the others. Images are what you push to registries and share, containers are the actual running workloads.",
            "The simplest way to think about it is that an image is like a blueprint and a container is the actual running application. An image is static - it contains your code, dependencies, and configuration all packaged up. When you execute that image, it becomes a container, which is a live process running on your system. You can spin up multiple containers from the same image, and they'll all be identical at startup but can have different runtime state. Images are what you build and distribute, containers are what actually runs in production. I like to think of it like a recipe versus an actual dish - the recipe doesn't change, but you can make many dishes from it."],
    },
    {
        text: "What is a Dockerfile and how do you write one?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.dockerfile],
        answers: ["A Dockerfile is a text file with instructions for building a Docker image. You start with a FROM instruction specifying a base image, then add layers with instructions like RUN for executing commands, COPY for adding files, WORKDIR to set the working directory, ENV for environment variables, EXPOSE to document ports, and CMD or ENTRYPOINT to specify what runs when the container starts. Each instruction creates a new layer. I write Dockerfiles by choosing an appropriate base image, installing dependencies, copying application code, and specifying the startup command. Best practices include ordering instructions from least to most frequently changing for better caching, minimizing layers, cleaning up in the same RUN command to reduce size, and using specific base image tags rather than 'latest'. A well-written Dockerfile produces small, secure, reproducible images.",
            "A Dockerfile is basically a script that tells Docker how to build your image step by step. It starts with FROM to specify your base image - like starting from Node or Python. Then you use instructions like COPY to bring in your source files, RUN to execute commands like installing packages, and CMD to define what command runs when the container starts. The key thing to understand is each instruction creates a layer, and Docker caches these layers. So when writing a Dockerfile, I put things that change infrequently at the top and things that change often at the bottom. That way, rebuilds are fast because Docker reuses cached layers. I also try to keep images small by cleaning up package manager caches and using minimal base images when possible."],
    },
    {
        text: "What is the difference between CMD and ENTRYPOINT?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.dockerfile],
        answers: ["Both define what command runs when a container starts, but they work differently. CMD provides default arguments that can be overridden when you run the container. ENTRYPOINT sets the main command that always runs, and any docker run arguments get appended to it. If you use both, CMD provides default arguments to ENTRYPOINT. For example, with ENTRYPOINT ['python'] and CMD ['app.py'], running the container executes 'python app.py', but you can override the script name. Use ENTRYPOINT when the container should always run a specific executable, like a command-line tool. Use CMD for applications where you might want to override the command or provide different arguments. I typically use ENTRYPOINT for the main executable and CMD for default arguments, giving flexibility while ensuring the container has a clear primary purpose.",
            "The main distinction is that CMD can be completely replaced when running the container, while ENTRYPOINT is fixed and arguments get appended to it. If I set CMD to 'python app.py', someone can run the container with a different command entirely. But if I set ENTRYPOINT to 'python', whatever they pass becomes arguments to python. In practice, I use ENTRYPOINT when my container is meant to behave like a specific executable - like a CLI tool where users pass arguments. I use CMD when I want a sensible default that can be overridden. You can combine them too - ENTRYPOINT for the command, CMD for default arguments. This gives you a working default but still allows customization."],
    },
    {
        text: "What is the difference between COPY and ADD?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.dockerfile],
        answers: ["Both copy files from your build context into the image, but ADD has extra features that make COPY the preferred option in most cases. COPY simply copies files or directories from source to destination. ADD does the same but also auto-extracts tar files and can download files from URLs. The Docker best practice is to use COPY unless you specifically need ADD's features. Using COPY makes Dockerfiles more explicit and predictable. If I need to add a tar file and extract it, I'll use ADD, but for regular files I use COPY. The auto-extraction can be surprising if you didn't expect it. For downloading URLs, it's usually better to use RUN with curl or wget for more control and better caching. Stick with COPY for clarity unless you have a specific reason for ADD.",
            "COPY is straightforward - it just copies files from your local machine into the image. ADD does the same thing but has two extra behaviors: it can extract tar archives automatically, and it can fetch files from URLs. The general recommendation is to prefer COPY because it's more predictable. When someone reads your Dockerfile, COPY does exactly what it says. ADD might surprise you by extracting a tar file when you expected to copy it as-is. If I need to download something, I'd rather use RUN with curl because I have more control over caching and error handling. I only use ADD when I specifically want its auto-extract behavior for tar files."],
    },
    {
        text: "What is Docker Compose?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum["docker-compose"]],
        answers: ["Docker Compose is a tool for defining and running multi-container applications using a YAML file. Instead of running multiple docker run commands, you define services, networks, and volumes in a docker-compose.yml file and start everything with 'docker compose up'. Each service is a container with its configuration like image, environment variables, volumes, and ports. Compose handles creating networks so services can communicate, manages dependencies with depends_on, and makes local development much easier. I use Compose for development environments where I need a database, cache, and application all running together, or for deploying simple multi-container apps. The compose file serves as documentation for how services connect. Compose is great for local development and testing, though for production orchestration at scale, Kubernetes is more appropriate.",
            "Docker Compose lets you define your entire application stack in a single YAML file and spin it all up with one command. Instead of manually running separate docker commands for your web app, database, and Redis cache, you describe them all in docker-compose.yml and run 'docker compose up'. It automatically creates a network so your containers can talk to each other by service name. For example, your app can connect to the database just by using 'postgres' as the hostname. I find it invaluable for local development because I can replicate a production-like environment on my laptop in seconds. It's also great for onboarding new developers - they just clone the repo and run compose up."],
    },
    {
        text: "What is the difference between docker run and docker exec?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker],
        answers: ["'docker run' creates and starts a new container from an image, while 'docker exec' runs a command in an already running container. When you run 'docker run nginx', you're starting a new nginx container. When you use 'docker exec -it container_name bash', you're opening a shell in an existing container. The run command is for starting containers, exec is for interacting with running containers. I use 'docker run' to start application containers and 'docker exec' for debugging, running one-off commands, or accessing a shell in a running container to inspect logs or check configuration. A common pattern is 'docker run' to start a database container, then 'docker exec' to run database commands or migrations. You can't exec into a stopped container - it must be running. Exec is invaluable for troubleshooting without rebuilding images.",
            "Think of 'docker run' as launching a brand new container from an image - it's how you start things up. 'docker exec' is for jumping into a container that's already running to do something inside it. If I need to debug why my app isn't working, I'd use exec to get a shell inside the container and poke around. If I need to start a fresh instance of my service, I use run. A practical example: I might 'docker run' my Postgres container to start the database, then 'docker exec' into it to run psql commands or check logs. The exec command is one of my most-used debugging tools because it lets me inspect what's actually happening inside a running container."],
    },
    {
        text: "What is the difference between containers and virtual machines?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker, ValidTag.enum.containers],
        answers: ["The fundamental difference is that containers share the host OS kernel while VMs each run a full operating system. A VM includes the application, binaries, libraries, and an entire guest OS, running on a hypervisor. Containers include the application and dependencies but share the kernel, making them much lighter - megabytes versus gigabytes. Containers start in seconds, VMs take minutes. You can run many more containers on a host than VMs because of the lower overhead. VMs provide stronger isolation since each has its own kernel, while containers share the kernel but use namespaces and cgroups for isolation. I use containers for microservices, CI/CD, and most modern applications where portability and density matter. VMs are better when you need different operating systems, stronger security isolation, or legacy applications. Containers are about efficiency and portability, VMs about isolation and flexibility.",
            "VMs virtualize the hardware - each VM runs its own complete operating system on top of a hypervisor. Containers virtualize the operating system - they share the host's kernel and just isolate the application processes. This makes containers way more lightweight. A VM might be gigabytes and take minutes to boot, while a container is megabytes and starts in milliseconds. You can fit many more containers on a single host. The tradeoff is isolation - VMs have stronger boundaries because each has its own kernel, while containers share one. For most modern applications, containers give you better resource efficiency and faster deployments. I'd only reach for VMs when I need to run different operating systems or require the stronger isolation guarantees that VMs provide."],
    },
    {
        text: "What is a Docker registry?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.docker],
        answers: ["A Docker registry is a storage and distribution system for Docker images. Docker Hub is the default public registry, but you can also use private registries like AWS ECR, Google Container Registry, Azure Container Registry, or self-hosted registries. Registries organize images by repository names and tags. When you run 'docker pull nginx', it downloads from Docker Hub. You push images to registries with 'docker push' after building them locally. I use public registries for open-source base images and private registries for proprietary application images. Private registries offer access control, vulnerability scanning, and integration with CI/CD pipelines. For production, I prefer managed registries like ECR that integrate with cloud IAM. Registries can be geo-replicated for faster pulls across regions. Understanding registries is essential for distributing container images across environments and teams.",
            "A registry is like a repository for Docker images - it's where you store and share them. Docker Hub is the public default, similar to how GitHub is for code. You push images there after building them and pull them when you need to run containers. For company applications, you typically use a private registry like AWS ECR, Google Artifact Registry, or Harbor. This gives you control over who can access your images and adds security features like vulnerability scanning. In a typical workflow, your CI pipeline builds an image, pushes it to your registry, then your deployment pulls it from there. The registry is the central place where all your environments get their images from."],
    },

    // CI/CD Basics
    {
        text: "What is CI/CD and why is it important?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"]],
        answers: [
            "CI/CD stands for Continuous Integration and Continuous Delivery, or sometimes Continuous Deployment. It's essentially the practice of automating the process of getting code from a developer's machine into production. Continuous Integration means developers frequently merge their code changes into a shared repository, and each merge triggers automated builds and tests. This catches integration issues early when they're easier to fix. Continuous Delivery extends this by automatically preparing code for release to production. The code is always in a deployable state, and releases can happen at the push of a button. Continuous Deployment goes one step further and automatically deploys every change that passes the automated tests. The importance really comes down to speed and reliability. Before CI/CD, teams might spend days or weeks doing integration and manual testing before a release. Now, we can deploy multiple times per day with confidence because every change is automatically validated. It reduces human error, gives developers faster feedback, and lets teams ship features to users much more quickly. It's become essential for any team that wants to move fast without breaking things.",
            "CI/CD automates the entire journey from code commit to production deployment. The CI part - Continuous Integration - means every time someone pushes code, it automatically gets built and tested. This catches bugs immediately rather than discovering them days later during manual integration. The CD part can mean Continuous Delivery, where code is always ready to deploy with one click, or Continuous Deployment, where it actually deploys automatically after tests pass. Why does this matter? It dramatically reduces the risk and pain of releases. Instead of big scary monthly deployments, you're shipping small changes constantly. Each change is tested automatically, so you have confidence it works. If something does break, it's easy to identify which small change caused it. Teams using CI/CD ship faster, with fewer bugs, and spend less time on manual testing and deployment tasks.",
        ],
    },
    {
        text: "What is the difference between continuous integration, continuous delivery, and continuous deployment?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"]],
        answers: [
            "These three practices build on each other. Continuous Integration is the foundation. It's the practice where developers frequently merge their code into a shared repository, ideally multiple times a day. Each merge triggers an automated build and test suite. The goal is to detect integration problems early and keep the codebase in a consistently working state. Continuous Delivery builds on CI by ensuring that code is always in a releasable state. After passing automated tests, the code goes through additional stages like staging environment deployment, performance testing, and security scans. At the end, you have a production-ready artifact that can be deployed with a single manual approval. The key point is that deployment to production still requires human intervention. Continuous Deployment takes it one step further by removing that manual step. Every change that passes all automated tests automatically goes to production without human intervention. This requires a very mature testing and monitoring setup because there's no human gate before production. Most companies start with CI, progress to Continuous Delivery, and some eventually move to Continuous Deployment once they have confidence in their automated quality gates.",
            "Think of them as three levels of automation. Continuous Integration is about merging code frequently and running automated tests on every merge - it ensures your code integrates cleanly with everyone else's work. Continuous Delivery takes it further by making sure the code is always deployable - you could release to production anytime with the push of a button. Continuous Deployment goes all the way and actually pushes to production automatically after tests pass - no human approval needed. Most teams start with CI because it's the foundation. Moving to Continuous Delivery requires good test coverage and confidence in your pipeline. Full Continuous Deployment needs even more maturity - excellent monitoring, feature flags, and the ability to roll back quickly. Not every team needs to go all the way to Continuous Deployment, but having at least Continuous Delivery makes releases much less stressful.",
        ],
    },
    {
        text: "What are build artifacts?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"]],
        answers: [
            "Build artifacts are the output files produced by your build process that are needed to run or deploy your application. They're essentially the packaged, production-ready version of your code. For a JavaScript application, artifacts might include the bundled and minified JavaScript files, CSS, and static assets that webpack or another bundler produces. For a Java application, it would be the JAR or WAR file. For containerized applications, the artifact is typically a Docker image. The important thing about artifacts is that they're immutable and versioned. Once you build an artifact, you don't rebuild it for different environments. The same artifact that passed testing in staging is the exact same artifact you deploy to production. This eliminates works on my machine problems and ensures consistency. Artifacts are stored in artifact repositories like AWS S3, Google Cloud Storage, or specialized tools like Artifactory or Nexus. They're usually tagged with the git commit hash or build number so you can trace exactly what code went into them. If you need to rollback, you just redeploy a previous artifact. This traceability and reproducibility is a key benefit of having proper artifact management.",
            "Build artifacts are the deployable outputs from your build process - what you actually ship to production. Depending on your stack, this could be compiled binaries, bundled JavaScript files, JAR files, or Docker images. The key principle is build once, deploy everywhere. You build your artifact once in CI, and that exact artifact gets promoted through staging to production. You don't rebuild for each environment because that could introduce inconsistencies. Artifacts should be versioned and stored somewhere reliable like S3, Artifactory, or a container registry. This way you can trace exactly what's running in production and easily roll back to a previous version if needed. Good artifact management is fundamental to reliable deployments.",
        ],
    },
    {
        text: "What are deployment environments (dev, staging, production)?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum["ci-cd"]],
        answers: [
            "Deployment environments are separate instances of your application that serve different purposes in the development and release process. The most common setup has three environments. Development or dev is where developers test their changes. It might get deployed on every commit or pull request. Data can be reset frequently, and it's okay if it breaks. The goal is rapid feedback during development. Staging is a production-like environment for final testing before release. It should mirror production as closely as possible in terms of configuration, data structure, and infrastructure. This is where you catch issues that only appear in production-like conditions. QA testing, performance testing, and stakeholder demos happen here. Production is the live environment serving real users. Changes here need to be carefully managed with proper monitoring and rollback capabilities. Some teams add additional environments. Preview environments spin up for each pull request so reviewers can see changes in action. QA environments are dedicated for quality assurance testing. UAT environments are for user acceptance testing with stakeholders. The pipeline promotes the same artifact through environments in sequence. Code passes through dev and staging before reaching production, with each environment adding confidence that the release is ready.",
            "Environments are isolated instances of your application serving different purposes. Development is the playground - developers deploy frequently to test changes, and it's okay if things break. Staging mimics production as closely as possible and is where you do final validation before release. Production is where real users interact with your application and needs the most care. The idea is progressive confidence - code proves itself in dev, gets more rigorous testing in staging, then goes to production. Each environment should be as similar as possible to catch environment-specific issues early. Some teams also have preview environments that spin up automatically for pull requests, which is great for code review. The key is that you deploy the same artifact through each environment, only changing configuration like database URLs or API keys.",
        ],
    },

    // Monitoring Basics
    {
        text: "What is application monitoring and why is it important?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.monitoring, ValidTag.enum.observability],
        answers: [
            "Application monitoring is the practice of tracking your application's behavior, performance, and health in real-time. It's about having visibility into what's happening inside your running systems so you can detect problems, understand user experience, and make data-driven decisions. It's critically important for several reasons. First, you can't fix what you can't see. Without monitoring, you only learn about problems when users complain, and by then the damage is done. Good monitoring lets you detect issues before they impact users, or at least respond quickly when they do. Second, it provides data for debugging. When something goes wrong at three in the morning, you need to quickly understand what changed and what's failing. Dashboards, logs, and traces give you that context. Third, it helps with capacity planning. By tracking resource usage over time, you can predict when you'll need to scale up. Fourth, it creates accountability and helps measure SLAs. You can prove to stakeholders that the system is meeting performance targets, or identify when it's not. The key components are typically metrics, logs, and traces, often called the three pillars of observability. Together they give you a complete picture of system behavior.",
            "Monitoring gives you visibility into what's actually happening in your production systems. Without it, you're flying blind - you only find out about problems when users start complaining. Good monitoring lets you detect issues proactively, often before users notice. It answers questions like: Is the application up? How fast is it responding? Are there errors? How much load are we handling? When something breaks at 2am, monitoring data helps you quickly diagnose what went wrong. It also helps with planning - by tracking trends, you can predict when you'll need more capacity. I think of monitoring as essential infrastructure, not optional. It combines metrics for numerical measurements, logs for detailed events, and traces for following requests through the system. Together these give you the complete picture you need to run reliable services.",
        ],
    },
    {
        text: "What are log levels and how do you use them?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.logging],
        answers: [
            "Log levels are a way to categorize log messages by their severity or importance. The standard levels from least to most severe are typically DEBUG, INFO, WARN, ERROR, and sometimes FATAL or CRITICAL. DEBUG is for detailed information useful during development or troubleshooting. Things like variable values, function entry and exit, or detailed state information. You usually disable this in production because it's too verbose. INFO is for general operational messages that confirm things are working normally. User logged in, payment processed, job completed. These help you understand the normal flow of your application. WARN is for potentially harmful situations that aren't errors yet. A deprecated API being called, a retry that succeeded, or approaching a resource limit. These are things you might want to investigate but aren't immediately critical. ERROR is for actual problems that prevented an operation from completing. Failed database queries, unhandled exceptions, or API failures. These typically need attention. The key is using levels consistently so you can filter logs appropriately. In production, you might only collect INFO and above to reduce noise. During an incident, you might temporarily enable DEBUG to get more detail. Most logging frameworks let you set the level per component or module so you can get detailed logs from a specific area without overwhelming yourself with output from everywhere.",
            "Log levels let you categorize messages by severity so you can filter what you actually want to see. DEBUG is the most verbose - useful for development but too noisy for production. INFO logs normal operations like user actions or successful transactions. WARN indicates something unexpected happened but the system handled it - like a retry that eventually succeeded. ERROR means something actually failed and needs attention. In production, I typically set the log level to INFO so I see operational data without being overwhelmed. If I'm debugging a specific issue, I might temporarily enable DEBUG for that component. The power of levels is filtering - during an incident, I can quickly find all ERROR logs without wading through thousands of INFO messages. Consistency in how your team uses levels is crucial for this filtering to be useful.",
        ],
    },
    {
        text: "What is the difference between logging, metrics, and tracing?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.monitoring, ValidTag.enum.logging, ValidTag.enum.observability],
        answers: [
            "Logs, metrics, and traces are the three pillars of observability, and they each serve different purposes. Logs are discrete events with timestamps. They record what happened at a specific moment, like a user login, an error, or a request being processed. They're great for debugging specific issues because they capture rich context. The downside is they can be expensive to store and search at scale. Metrics are numeric measurements over time. Things like CPU usage, request count, error rate, or response latency. They're aggregated, so instead of recording every event, you're tracking summaries like averages, percentiles, or counts per time window. Metrics are efficient to store and query, making them great for dashboards, alerting, and understanding trends. But they lose individual event detail. Traces follow a single request as it flows through your system across multiple services. Each step in the request is a span, and together they form a trace. This is essential for microservices because a single user action might touch ten different services. Tracing shows you exactly where time was spent and where failures occurred in that chain. In practice, you use all three together. Metrics tell you something is wrong, traces help you find where, and logs tell you why.",
            "These are the three pillars of observability, each giving you different insights. Logs are detailed text records of events - great for understanding exactly what happened, but can be expensive at scale. Metrics are numerical data over time like request rates or latency percentiles - efficient to store and perfect for dashboards and alerts. Traces track individual requests as they flow through your services - essential for microservices to see where time is spent. They work together in a debugging workflow: metrics alert you that error rates are up, traces show you which service in the chain is failing, and logs give you the detailed error message explaining why. No single one is complete on its own, but together they give you full observability.",
        ],
    },
    {
        text: "What is structured logging?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.logging],
        answers: [
            "Structured logging means outputting log messages in a consistent, machine-parseable format rather than free-form text strings. Instead of logging something like 'User john@example.com logged in from IP 192.168.1.1', you log a structured object like a JSON message with fields for event type, user email, IP address, and timestamp. The benefits are huge for operability. First, you can easily search and filter. Want all login events from a specific IP? With structured logs, that's a simple query. With plain text, you're writing fragile regex patterns. Second, you can aggregate and analyze. Count logins per hour, find the most common error types, or build dashboards. Third, it's consistent. New team members can understand log formats without reading the code that produced them. Tools like the ELK stack, Datadog, or CloudWatch Logs Insights work much better with structured data. In implementation, I typically use a logging library configured to output JSON. Each log call includes both a message and a set of key-value pairs for context. I establish conventions for common fields like request ID, user ID, and service name so they're consistent across the codebase. The request ID is particularly important because it lets you correlate all logs from a single user request across different services.",
            "Structured logging means outputting logs in a consistent format like JSON rather than plain text. Instead of logging 'User alice logged in', you log an object with fields like user, action, timestamp, and ip_address. This makes logs much more useful because you can query them like a database. Need all logins from a specific IP? That's a simple filter. Want to count errors by type? Easy aggregation. With plain text, you'd be writing fragile regex patterns. It also ensures consistency across your codebase - everyone uses the same field names. I always include a request ID in every log so I can trace all logs for a single request across services. Most logging tools like Datadog or CloudWatch work way better with structured data. It's a small change in how you log but a huge improvement in how you can analyze and debug.",
        ],
    },

    // Kubernetes Basics
    {
        text: "What is Kubernetes and what problems does it solve?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Kubernetes is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications. It solves problems that arise when running containers at scale - service discovery, load balancing, self-healing, rolling updates, resource management, and configuration management. With Docker alone, you'd manually start containers on servers and handle failures. Kubernetes abstracts away the infrastructure, letting you declare desired state and it maintains it. If a container crashes, Kubernetes restarts it. If a node fails, it reschedules containers elsewhere. It handles scaling by adding or removing pod replicas based on load. I use Kubernetes for production applications that need high availability, automatic scaling, zero-downtime deployments, and multi-cloud portability. The learning curve is steep, but it's the industry standard for container orchestration and provides a consistent platform regardless of underlying infrastructure.",
            "Kubernetes is the industry standard for running containers in production. Once you have more than a handful of containers, you need something to manage them - that's what Kubernetes does. You tell it what you want to run and how many instances, and it makes sure that happens. If a container crashes, Kubernetes restarts it. If a server dies, it moves containers to healthy servers. It handles load balancing, rolling deployments, and scaling automatically. The key concept is declarative configuration - you describe your desired state in YAML files, and Kubernetes continuously works to maintain that state. It abstracts away the infrastructure so your applications run the same way whether you're on AWS, GCP, or your own data centers. There's definitely a learning curve, but for any serious production workload, it's worth the investment."],
    },
    {
        text: "What are pods and why are they the smallest deployable unit?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.pods],
        answers: ["Pods are the smallest deployable unit in Kubernetes - a group of one or more containers that share storage and network resources. Containers in a pod share an IP address, can communicate via localhost, and share volumes. Pods are the smallest unit because Kubernetes manages and scales at the pod level, not individual containers. Why group containers? Sometimes containers are tightly coupled and should be co-located. A common pattern is a main application container with a sidecar that handles logging, proxying, or other cross-cutting concerns. They need to scale together and share resources. Single-container pods are most common though. Pods are ephemeral - they can be killed and replaced at any time. You don't manage pods directly in production; you use higher-level controllers like Deployments that manage pods for you and ensure the right number are running. Understanding pods is fundamental to Kubernetes because every workload runs in pods.",
            "A pod is a wrapper around one or more containers that Kubernetes treats as a single unit. Containers in the same pod share an IP address, can talk over localhost, and can share storage volumes. Most pods run just one container, but you might put multiple containers in a pod when they're tightly coupled - like an app server with a logging sidecar. The reason pods are the smallest unit rather than individual containers is that Kubernetes needs a grouping mechanism for scheduling and networking. When you scale, you're adding more pod replicas, not individual containers. Importantly, pods are designed to be disposable - Kubernetes can kill and recreate them anytime. That's why you don't manage pods directly; you use Deployments or other controllers that maintain the desired number of pod replicas for you."],
    },
    {
        text: "What are deployments and how do they manage pods?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.deployments],
        answers: ["A Deployment is a Kubernetes resource that manages the lifecycle of pods, providing declarative updates and rollout capabilities. Instead of managing pods directly, you create a Deployment that specifies the desired state - what container image to run, how many replicas, and configuration. Kubernetes then ensures this state is maintained. If a pod dies, the Deployment creates a new one. When you update the Deployment, it performs a rolling update by gradually replacing old pods with new ones. Deployments create ReplicaSets under the hood that actually manage the pods. The key benefits are declarative configuration, rolling updates with zero downtime, easy rollbacks if something goes wrong, and automatic healing. I use Deployments for stateless applications, web servers, and APIs. You define the Deployment in YAML, apply it with kubectl, and Kubernetes handles the rest. It's the primary way to run applications in production on Kubernetes.",
            "A Deployment is the standard way to run applications in Kubernetes. You describe what you want - which container image, how many replicas, resource limits - and the Deployment makes it happen and keeps it that way. If a pod crashes, the Deployment automatically creates a replacement. When you push a new version, it handles rolling updates, gradually replacing old pods with new ones so there's no downtime. If something goes wrong, you can roll back instantly. Under the hood, Deployments manage ReplicaSets which manage Pods, but you typically just interact with Deployments. For any stateless application - web servers, API backends, workers - Deployments are what you use. They're the main workload primitive in Kubernetes."],
    },
    {
        text: "What are services and what types are there (ClusterIP, NodePort, LoadBalancer)?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes, ValidTag.enum.services],
        answers: ["A Kubernetes Service provides a stable network endpoint to access a group of pods. Pods are ephemeral with changing IPs, so Services give you a consistent way to reach them. ClusterIP is the default type - it creates an internal IP accessible only within the cluster. Other pods can reach the service, but external traffic can't. This is used for internal communication between services. NodePort exposes the service on each node's IP at a static port. External traffic can reach the service via any node's IP and that port. It's simpler than LoadBalancer but requires you to know node IPs. LoadBalancer provisions an external load balancer from your cloud provider and routes external traffic to the service. This is how you expose applications to the internet in production. Services use label selectors to find pods and automatically load balance across them. They're essential for service discovery - pods find each other through service names.",
            "Services provide a stable way to access pods, which is essential because pods come and go constantly with different IPs. A Service gives you a fixed DNS name and IP that routes to whatever pods match its label selector. ClusterIP is for internal traffic - other pods can reach it, but nothing outside the cluster can. NodePort opens a port on every node so external traffic can get in, though you need to know node IPs. LoadBalancer is for production - it provisions an actual cloud load balancer with a public IP. Within the cluster, services also enable DNS-based discovery, so your frontend can simply connect to 'backend-service' rather than tracking pod IPs. I use ClusterIP for internal services like databases, and LoadBalancer for anything user-facing."],
    },
    {
        text: "What are namespaces?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["Namespaces are a way to divide cluster resources between multiple users, teams, or projects. They're like virtual clusters within a physical cluster. Resources like pods, services, and deployments live within a namespace and have names that must be unique within that namespace, but not across namespaces. You might have 'development', 'staging', and 'production' namespaces, or namespaces per team. Namespaces help with resource isolation and access control - you can set resource quotas per namespace and use RBAC to control who can access what. They also make cleanup easier - deleting a namespace deletes all resources within it. The default namespace is used if you don't specify one, but I always create explicit namespaces for organization. Some resources like nodes and persistent volumes are cluster-wide and not namespaced. Namespaces are essential for multi-tenant clusters and organizing workloads in larger deployments.",
            "Namespaces let you partition a single Kubernetes cluster into virtual clusters. It's a way to organize and isolate resources. For example, you might have namespaces for different environments - dev, staging, prod - or for different teams. Resources within a namespace must have unique names, but the same name can exist in different namespaces. This provides logical separation and makes access control easier with RBAC - you can give a team access only to their namespace. You can also set resource quotas per namespace to prevent one team from consuming all cluster resources. Deleting a namespace removes everything in it, which is handy for cleanup. I always use explicit namespaces rather than the default one for better organization."],
    },
    {
        text: "What is kubectl and what are common commands?",
        level: Level.enum["junior-advanced"],
        category: Category.enum.devops,
        tags: [ValidTag.enum.kubernetes],
        answers: ["kubectl is the command-line tool for interacting with Kubernetes clusters. It's how you create, inspect, update, and delete resources. Common commands include 'kubectl get pods' to list pods, 'kubectl describe pod name' for detailed info, 'kubectl logs pod-name' to view logs, 'kubectl apply -f file.yaml' to create or update resources from a file, 'kubectl delete pod name' to delete resources, 'kubectl exec -it pod-name -- bash' to get a shell in a container, and 'kubectl port-forward' to access services locally. I use 'kubectl get all' to see everything in a namespace, 'kubectl config get-contexts' to see available clusters, and 'kubectl rollout status' to watch deployments. The '-n namespace' flag specifies a namespace. 'kubectl explain' shows documentation for resource types. Learning kubectl is essential for Kubernetes work - it's your primary interface for cluster management.",
            "kubectl is your main interface to Kubernetes - everything you do with the cluster goes through it. The most common commands I use daily are 'kubectl get pods' to see what's running, 'kubectl logs' to check application logs, 'kubectl describe' for detailed resource info when debugging, and 'kubectl apply -f' to deploy changes from YAML files. For troubleshooting, 'kubectl exec' lets you get a shell inside a container, and 'kubectl port-forward' lets you access services locally without exposing them. I also frequently use 'kubectl rollout status' to watch deployments and 'kubectl get events' to see what's happening in the cluster. The '-n' flag is important for specifying which namespace to work in. Once you're comfortable with kubectl, managing Kubernetes becomes much more intuitive."],
    },
];
