import { Category, Level, ValidTag } from "../../../db/constants";
import type { QuestionForCategoryAndLevel } from "../../../lib/types";

export const mid: QuestionForCategoryAndLevel<
    typeof Category.enum.databases,
    typeof Level.enum.mid
>[] = [
    // Database General
    {
        text: "What is the difference between SQL and NoSQL databases?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.mongodb],
        answers: ["SQL databases are relational and use structured schemas with tables, rows, and columns. They enforce relationships through foreign keys and support complex joins and transactions with strong ACID guarantees. NoSQL databases are more flexible - they can be document-based like MongoDB, key-value stores like Redis, or graph databases. They typically prioritize scalability and flexibility over strict consistency. SQL is great when you have well-defined relationships and need complex queries, while NoSQL shines when you need to scale horizontally, have varying data structures, or require extremely high write throughput. The choice really comes down to whether you value strict consistency and relational integrity versus flexibility and horizontal scalability."],
    },
    {
        text: "What are ACID properties and why do they matter?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.acid, ValidTag.enum.transactions],
        answers: ["ACID stands for Atomicity, Consistency, Isolation, and Durability. Atomicity means a transaction either completes fully or not at all - no partial states. Consistency ensures the database moves from one valid state to another. Isolation means concurrent transactions don't interfere with each other. Durability guarantees that once a transaction commits, it persists even if the system crashes. These properties matter because they prevent data corruption and ensure reliability. For example, in a banking application, if you're transferring money between accounts, ACID properties ensure the money isn't lost or duplicated if something goes wrong mid-transaction. It's the foundation of data integrity in critical systems."],
    },
    {
        text: "What is the CAP theorem?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum["cap-theorem"]],
        answers: ["The CAP theorem states that in a distributed database system, you can only guarantee two out of three properties: Consistency, Availability, and Partition tolerance. Consistency means all nodes see the same data at the same time. Availability means every request gets a response. Partition tolerance means the system continues to operate despite network failures between nodes. In practice, partition tolerance is mandatory in distributed systems because network failures happen, so you're really choosing between consistency and availability. For example, traditional SQL databases often choose consistency over availability, while systems like Cassandra choose availability. It's not always a binary choice though - modern systems often tune this tradeoff based on the use case."],
    },
    {
        text: "What is the difference between horizontal and vertical scaling for databases?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.scalability, ValidTag.enum.sharding],
        answers: ["Vertical scaling means adding more resources to a single server - more CPU, RAM, or faster disks. It's straightforward and doesn't require application changes, but there's a physical limit to how much you can scale. Horizontal scaling means adding more servers and distributing the load across them. It's more complex because you need to handle data distribution and coordination, but theoretically you can scale indefinitely. For databases, vertical scaling is easier initially but gets expensive and eventually hits limits. Horizontal scaling through techniques like sharding or replication is how you handle massive scale, though it introduces complexity around data consistency and query routing. Most systems start vertical and add horizontal scaling as needed."],
    },
    {
        text: "What is database sharding and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sharding, ValidTag.enum.scalability],
        answers: ["Sharding is partitioning your data across multiple database servers based on some key, like user ID or geographic region. Each shard holds a subset of the total data. You'd use it when a single database can't handle your read or write load, or when your data is too large for one machine. For example, you might shard users by their ID range - users 1 to 1 million on shard 1, 1 million to 2 million on shard 2. The challenges are choosing a good shard key that distributes load evenly, handling queries that span multiple shards, and rebalancing when shards get uneven. It adds operational complexity, so you only do it when you truly need that level of scale."],
    },
    {
        text: "What is replication and what are the different strategies?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.replication],
        answers: ["Replication is copying data across multiple database servers to improve availability and read performance. The main strategies are primary-replica, where one server handles writes and others replicate the data for reads, and multi-primary, where multiple servers can accept writes. With primary-replica, you get consistency but the primary is a single point of failure for writes. You can do synchronous replication where the primary waits for replicas to confirm, giving consistency but higher latency, or asynchronous where it's faster but replicas might lag. Multi-primary is more complex but handles failover better. The strategy you choose depends on your consistency requirements, read-write ratio, and how you handle conflicts."],
    },
    {
        text: "What is connection pooling and why is it important?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.performance],
        answers: ["Connection pooling is maintaining a set of reusable database connections rather than creating a new one for each request. Opening a database connection is expensive - it involves network handshakes, authentication, and resource allocation. With a pool, connections are created upfront and reused across requests. When your application needs a connection, it borrows one from the pool and returns it when done. This dramatically improves performance, especially under high load. You configure the pool size based on your database's capacity and application's concurrency needs. Too few connections and requests queue up, too many and you overwhelm the database. It's a standard practice in production applications."],
    },
    {
        text: "What are database transactions?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.transactions],
        answers: ["A transaction is a sequence of database operations that are executed as a single unit of work. It follows ACID properties - all operations succeed together or fail together. You start a transaction, perform multiple operations like inserts, updates, or deletes, then either commit if everything worked or rollback if something failed. For example, transferring money between accounts involves debiting one account and crediting another - you want both to happen or neither. Transactions ensure data integrity in these scenarios. They also provide isolation so concurrent transactions don't interfere with each other. Most databases support transactions, though the isolation levels and performance characteristics vary."],
    },
    {
        text: "What is the difference between optimistic and pessimistic locking?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.concurrency],
        answers: ["Pessimistic locking assumes conflicts will happen, so it locks the data when you read it, preventing others from modifying it until you're done. It's safer but can hurt performance because it blocks concurrent access. Optimistic locking assumes conflicts are rare, so it doesn't lock on read. Instead, it checks if the data changed before committing. Typically this uses a version number or timestamp - if the version changed since you read it, the update fails and you retry. Optimistic locking gives better concurrency but requires handling retry logic. I use pessimistic locking for critical operations with high conflict probability, and optimistic for most other cases where conflicts are unlikely."],
    },
    {
        text: "What is a deadlock and how do you prevent it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.deadlocks, ValidTag.enum.concurrency],
        answers: ["A deadlock occurs when two or more transactions are waiting for each other to release locks, creating a cycle that prevents any of them from proceeding. For example, transaction A locks row 1 and waits for row 2, while transaction B locks row 2 and waits for row 1. To prevent deadlocks, you can acquire locks in a consistent order across all transactions, keep transactions short, use appropriate isolation levels, or implement timeouts. Most databases can detect deadlocks and will automatically abort one transaction to break the cycle. When that happens, you need to handle it in your application by retrying the transaction. The key is designing your application to minimize lock contention in the first place."],
    },

    // SQL and PostgreSQL
    {
        text: "What is normalization and when would you denormalize?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.normalization, ValidTag.enum.denormalization, ValidTag.enum["database-design"]],
        answers: ["Normalization is organizing data to reduce redundancy and improve data integrity. You split data into related tables and use foreign keys to establish relationships. The normal forms - first, second, third - progressively eliminate different types of redundancy. It prevents update anomalies where changing one piece of data requires updating multiple places. However, you'd denormalize when read performance becomes critical. Joins across many tables can be slow, so sometimes you duplicate data to avoid those joins. For example, storing a user's name directly on their posts rather than joining to the users table. I denormalize for reporting tables, caching frequently accessed combinations, or when read-heavy workloads outweigh the costs of data redundancy and maintaining consistency."],
    },
    {
        text: "What are joins and what are the different types?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.joins],
        answers: ["Joins combine data from multiple tables based on related columns. An INNER JOIN returns only rows where there's a match in both tables. LEFT JOIN returns all rows from the left table and matching rows from the right, with nulls where there's no match. RIGHT JOIN is the opposite. FULL OUTER JOIN returns all rows from both tables with nulls where there's no match. CROSS JOIN returns the cartesian product of both tables. In practice, I use INNER JOIN when I only want matching records, LEFT JOIN when I need all records from the primary table regardless of whether related data exists, and occasionally FULL OUTER JOIN for specific reconciliation scenarios. Understanding joins is fundamental to working with relational databases effectively."],
    },
    {
        text: "What are indexes and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.indexing],
        answers: ["An index is a data structure that improves query performance by allowing the database to find rows quickly without scanning the entire table. Most databases use B-tree indexes, which organize data in a sorted tree structure. When you query with a WHERE clause or JOIN on an indexed column, the database uses the index to jump directly to relevant rows. It's similar to a book's index - instead of reading every page, you look up the term and go to specific pages. The tradeoff is that indexes take up space and slow down writes because the index must be updated. You want to index columns frequently used in WHERE clauses, JOIN conditions, and ORDER BY clauses, but not over-index because each index has a maintenance cost."],
    },
    {
        text: "What is the difference between a clustered and non-clustered index?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.indexing],
        answers: ["A clustered index determines the physical order of data in the table. The table data is stored in the order of the clustered index, usually the primary key. You can only have one clustered index per table because the data can only be physically sorted one way. A non-clustered index is a separate structure that points to the data. It contains the indexed columns and a pointer back to the actual row. You can have multiple non-clustered indexes. Clustered indexes are faster for range queries and retrieving the full row because the data is right there. Non-clustered indexes are better when you only need specific columns. In PostgreSQL, this concept differs slightly - the primary key isn't necessarily clustered, but you can manually cluster a table on an index."],
    },
    {
        text: "What are some indexing strategies you would use and why?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.indexing, ValidTag.enum.performance],
        answers: ["I start by indexing foreign keys because they're used in joins. Then I look at the most common queries and index columns in WHERE clauses, especially for high-cardinality columns where the index significantly narrows results. For queries with multiple conditions, composite indexes can be powerful. I also consider covering indexes that include all columns a query needs, avoiding table lookups entirely. For text search, full-text indexes are more efficient than LIKE queries. I monitor slow query logs to identify missing indexes. But I'm careful not to over-index - each index slows down writes and uses storage. I'll drop unused indexes and sometimes create indexes only for specific reporting periods, then drop them when done."],
    },
    {
        text: "What is a composite index and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum.indexing],
        answers: ["A composite index covers multiple columns in a specific order. For example, an index on (last_name, first_name, date_of_birth). The column order matters - the database can use this index for queries filtering on just last_name, or last_name and first_name, but not just first_name alone. It follows the leftmost prefix rule. I use composite indexes when queries frequently filter on multiple columns together. For instance, if you often query orders by customer_id and status, a composite index on (customer_id, status) is more efficient than separate indexes. The key is ordering columns by selectivity and query patterns - put the most selective column first, or the one used in all queries if there are multiple use cases."],
    },
    {
        text: "What is query optimization and how do you read an execution plan?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql, ValidTag.enum["query-optimization"], ValidTag.enum.performance],
        answers: ["Query optimization is improving query performance by restructuring the query or adding indexes. The execution plan shows how the database will execute your query - what indexes it'll use, how it'll join tables, and estimated costs. I use EXPLAIN or EXPLAIN ANALYZE to see the plan. Key things I look for are sequential scans on large tables, which usually mean missing indexes, nested loop joins that might be inefficient, and the estimated rows versus actual rows to spot bad statistics. High-cost operations bubble up as bottlenecks. Once I identify the problem, I might add an index, rewrite the query to be more selective, or break it into smaller queries. Reading execution plans is critical for diagnosing performance issues in production."],
    },
    {
        text: "What are views and materialized views?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["A view is a stored query that acts like a virtual table. When you query a view, the database executes the underlying query. It's useful for simplifying complex queries, encapsulating logic, or restricting access to specific columns. A materialized view actually stores the query results physically, like a cached table. You refresh it periodically to update the data. Regular views have no storage overhead but run the query every time, while materialized views are fast to query but can have stale data and take up space. I use regular views for abstraction and security, and materialized views for expensive aggregations or reports where slightly stale data is acceptable and query performance is critical."],
    },
    {
        text: "What are stored procedures and triggers?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["Stored procedures are precompiled SQL code stored in the database that you can call like functions. They can accept parameters, perform complex logic, and return results. They're useful for encapsulating business logic, reducing network overhead by processing data server-side, and enforcing consistency. Triggers are special stored procedures that automatically execute in response to events like INSERT, UPDATE, or DELETE on a table. They're useful for maintaining data integrity, audit logging, or cascading changes. I use them sparingly though - they can make the system harder to debug and understand because logic is hidden in the database. For most business logic, I prefer keeping it in the application layer where it's more visible and testable."],
    },
    {
        text: "What are CTEs (Common Table Expressions)?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["CTEs are temporary named result sets defined with a WITH clause that you can reference in your main query. They make complex queries more readable by breaking them into logical steps. For example, instead of nested subqueries, you define each step as a CTE and reference it by name. They can also be recursive, which is powerful for hierarchical data like organizational charts or threaded comments. The query engine may or may not materialize them depending on optimization. I use CTEs to improve readability when I have multiple subqueries, to avoid repeating the same subquery, and for recursive operations. They're especially helpful when building up complex analytics queries step by step."],
    },
    {
        text: "What are window functions?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["Window functions perform calculations across a set of rows related to the current row, without collapsing them into a single result like GROUP BY does. You define a window with OVER, optionally partitioning and ordering the data. Common examples are ROW_NUMBER for ranking, LAG and LEAD for accessing previous or next rows, and aggregate functions like SUM or AVG over a window. For instance, you can calculate running totals, rank items within categories, or compare each row to an average without losing the individual rows. They're incredibly powerful for analytics and reporting. I use them frequently for leaderboards, time-series analysis, and any scenario where I need aggregates while maintaining row-level detail."],
    },
    {
        text: "What are constraints and what types are there?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["Constraints enforce rules on data to maintain integrity. PRIMARY KEY uniquely identifies each row and doesn't allow nulls. FOREIGN KEY ensures referential integrity by requiring values to exist in another table. UNIQUE ensures no duplicate values in a column. NOT NULL prevents null values. CHECK validates that values meet specific conditions, like age being positive. DEFAULT provides a value if none is specified. Constraints are enforced by the database, so invalid data can't be inserted regardless of which application tries. They're critical for data integrity. I define them in the schema rather than relying solely on application-level validation, because they provide a guaranteed safety net and document the data rules at the database level."],
    },
    {
        text: "What is the difference between DELETE, TRUNCATE, and DROP?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.sql],
        answers: ["DELETE removes specific rows based on a WHERE clause. It's logged, can be rolled back in a transaction, and triggers fire for each row. TRUNCATE removes all rows from a table quickly without logging individual row deletions. It's much faster for clearing large tables but can't be rolled back in all databases and doesn't fire triggers. DROP removes the entire table structure and data from the database. DELETE is for selective removal and maintaining history, TRUNCATE is for quickly clearing all data while keeping the table structure, and DROP is for removing the table entirely. I use DELETE for normal operations, TRUNCATE for clearing staging tables or during testing, and DROP when retiring tables during migrations."],
    },
    {
        text: "What are database migrations and how do you manage them?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.migrations],
        answers: ["Migrations are version-controlled changes to your database schema. Each migration is a script that modifies the schema - adding tables, columns, indexes, or transforming data. They're applied sequentially and tracked so the database knows which migrations have run. I use migration tools like Prisma Migrate, Flyway, or Knex that apply migrations automatically and maintain a history. Each migration should be reversible when possible, though that's not always practical. Best practices include testing migrations on a copy of production data, making backward-compatible changes when possible to allow zero-downtime deployments, and keeping migrations small and focused. They're essential for coordinating schema changes across development, staging, and production environments."],
    },
    {
        text: "What is PostgreSQL's JSONB and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.postgresql],
        answers: ["JSONB is PostgreSQL's binary JSON data type that stores JSON data in a decomposed binary format. Unlike the JSON type, it's processed for storage, which makes it slower to insert but much faster to query. You can index JSONB columns and use operators to query nested data. I use JSONB when I have semi-structured data that doesn't fit neatly into columns, like user preferences, product attributes that vary by type, or API responses. It's great for flexible schemas while still getting the benefits of SQL like transactions and joins. However, I'm careful not to abuse it - if the data has clear structure and is frequently queried in specific ways, traditional columns with proper indexes are usually better."],
    },
    {
        text: "What are PostgreSQL extensions?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.postgresql],
        answers: ["Extensions add functionality to PostgreSQL beyond the core features. Popular ones include PostGIS for geospatial data, pg_trgm for fuzzy text search and similarity matching, uuid-ossp for generating UUIDs, and pgcrypto for cryptographic functions. You enable them with CREATE EXTENSION. They're powerful because they extend the database with specialized capabilities without modifying the core system. For example, PostGIS adds geographic data types and spatial queries that would be complex to implement in application code. I use extensions when the database can handle certain operations more efficiently than the application layer, like full-text search with the built-in text search extension, or when I need specialized data types and functions that are already well-tested in an extension."],
    },
    {
        text: "What is full-text search in PostgreSQL?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.postgresql],
        answers: ["PostgreSQL's full-text search provides text search capabilities that are more sophisticated than LIKE or regex patterns. It handles stemming, ranking, stop words, and multiple languages. You use tsvector to store processed documents and tsquery for search queries. It can rank results by relevance and handle phrase searches. I use it for medium-scale search features where a dedicated search engine like Elasticsearch might be overkill. For example, searching product descriptions or article content. You can create GIN indexes on tsvector columns for performance. The advantage is it's built into PostgreSQL, so no additional infrastructure. The limitation is it doesn't scale to massive datasets or provide the advanced features of dedicated search engines, but for many applications it's perfectly sufficient."],
    },

    // MongoDB
    {
        text: "What is MongoDB and when would you choose it over SQL?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum.sql],
        answers: ["MongoDB is a document-based NoSQL database that stores data as JSON-like documents rather than rows and tables. I'd choose it when I have rapidly evolving schemas where the structure isn't fully defined upfront, or when different records need different fields. It's great for content management systems, catalogs with varied product types, or when you need to scale horizontally across many servers. The flexible schema makes development faster when requirements are changing. However, I'd stick with SQL for complex relational data with many joins, financial systems requiring strict ACID guarantees, or when the team is more familiar with SQL. MongoDB has added transactions and better consistency, but SQL databases still excel at complex relational queries and enforcing strict data integrity."],
    },
    {
        text: "What are documents and collections?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb],
        answers: ["In MongoDB, a document is a single record stored as a JSON-like object with field-value pairs. It's analogous to a row in SQL, but more flexible because different documents can have different fields. A collection is a group of documents, similar to a table in SQL, but without enforcing a schema. For example, a users collection might contain user documents with fields like name, email, and preferences. Documents can have nested objects and arrays, allowing you to model complex hierarchical data naturally. This flexibility is powerful but requires discipline - you still need to think about your data structure, just differently than with rigid schemas. Collections are created implicitly when you insert the first document."],
    },
    {
        text: "What is the aggregation pipeline?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum.aggregation],
        answers: ["The aggregation pipeline is MongoDB's framework for data processing and transformation. You pass documents through a series of stages, where each stage performs an operation like filtering, grouping, sorting, or reshaping. Common stages include match for filtering, group for aggregation, project for selecting fields, sort for ordering, and lookup for joins. For example, you might match orders from the last month, group by customer to sum totals, then sort by total descending. Each stage's output becomes the next stage's input. It's similar to SQL's GROUP BY and aggregate functions but more composable and powerful. I use it for analytics, reporting, and complex data transformations that would be cumbersome with simple queries."],
    },
    {
        text: "How do you model relationships in MongoDB?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum["database-design"]],
        answers: ["You have two main approaches: embedding and referencing. Embedding puts related data directly in the document as nested objects or arrays. For example, storing a user's addresses as an array within the user document. This is fast for reads because everything is in one place, but can lead to duplication and large documents. Referencing stores related data in separate collections and links them with IDs, similar to foreign keys in SQL. You'd store order IDs on a user document and the actual orders in an orders collection. The choice depends on the relationship and access patterns. Embed when data is always accessed together and has a clear ownership, reference when data is large, shared across documents, or updated independently."],
    },
    {
        text: "What are indexes in MongoDB and how do they differ from SQL?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum.indexing],
        answers: ["MongoDB indexes work similarly to SQL - they're data structures that improve query performance by avoiding full collection scans. MongoDB uses B-tree indexes by default. The difference is that MongoDB can index fields within nested documents and arrays, which is powerful for document-based data. You can create compound indexes, unique indexes, and text indexes. MongoDB also has special index types like geospatial indexes for location queries and TTL indexes that automatically delete documents after a time period. Like SQL, every index speeds up reads but slows writes and uses memory. The principles are the same - index fields you query frequently, especially in filters and sorts. You use explain to see if queries use indexes effectively."],
    },
    {
        text: "What is Mongoose and what problems does it solve?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongoose, ValidTag.enum.mongodb],
        answers: ["Mongoose is an ODM - Object Data Modeling library - for MongoDB and Node.js. It solves the problem of MongoDB's schema-less nature by allowing you to define schemas and models with validation, type casting, and business logic. It provides structure and consistency that raw MongoDB doesn't enforce. Mongoose adds features like middleware hooks for lifecycle events, virtuals for computed properties, population for reference handling similar to joins, and built-in validation. It also simplifies common operations with a cleaner API. The tradeoff is added complexity and a slight performance overhead. I use Mongoose when I want schema validation and the developer experience benefits outweigh the flexibility of schema-less design, which is most production applications."],
    },
    {
        text: "What are Mongoose schemas and models?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongoose],
        answers: ["A Mongoose schema defines the structure of documents in a collection - what fields exist, their types, validation rules, and default values. It's like a blueprint. For example, you might define a User schema with fields for name, email, and password, specifying that email is required and unique. A model is a constructor compiled from the schema that lets you create, read, update, and delete documents. You create instances of the model representing individual documents. The schema provides the structure and rules, while the model provides the interface to interact with the database. This separation allows you to define your data structure once and reuse it throughout your application while getting validation and type safety."],
    },
    {
        text: "What are Mongoose middleware (pre and post hooks)?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongoose],
        answers: ["Mongoose middleware are functions that run at specific points in a document's lifecycle. Pre hooks run before an operation like save, validate, or remove, while post hooks run after. For example, a pre-save hook might hash a password before saving a user, or validate custom business rules. A post-save hook might log the operation or trigger notifications. You can also have hooks for queries like pre-find to add default filters. They're useful for cross-cutting concerns like auditing, validation, or maintaining computed fields. The key is they're tied to the model, so the logic runs regardless of where in your application you save a document. I use them for operations that should always happen, like timestamps or password hashing."],
    },
    {
        text: "How do you handle transactions in MongoDB?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum.transactions],
        answers: ["MongoDB added multi-document ACID transactions in version 4.0 for replica sets and 4.2 for sharded clusters. You start a session, begin a transaction, perform operations, then commit or abort. If any operation fails, you can roll back all changes. This is crucial when you need to update multiple documents atomically, like transferring inventory between warehouses. However, transactions have performance overhead and are only available on replica sets. MongoDB's document model often eliminates the need for transactions - if you embed related data in a single document, updates are naturally atomic. I use transactions sparingly, only when I truly need atomicity across multiple documents, and design schemas to minimize that need. For simple applications, careful schema design often avoids the need entirely."],
    },
    {
        text: "What is the difference between embedding and referencing?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum["database-design"]],
        answers: ["Embedding stores related data directly within a document as nested objects or arrays. Referencing stores related data in separate collections and links them by ID. Embedding gives better read performance since everything is in one document, reduces the need for joins, and makes updates atomic. But it can cause data duplication and documents can grow too large. Referencing normalizes data, avoiding duplication and keeping documents small, but requires multiple queries or lookup operations to get complete data. I embed when there's a one-to-few relationship, data is always accessed together, and is owned by the parent document - like blog post comments. I reference when there's a one-to-many or many-to-many relationship, data is large, or is shared across documents - like authors and books."],
    },
    {
        text: "What are MongoDB operators?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb],
        answers: ["MongoDB operators are special syntax for performing operations in queries and updates. Query operators like dollar-gt, dollar-lt, dollar-in, and dollar-exists filter documents based on conditions. Update operators like dollar-set, dollar-inc, dollar-push, and dollar-pull modify documents. Aggregation operators transform data in the pipeline. For example, dollar-gt finds documents where a field is greater than a value, dollar-set updates a field, and dollar-push adds an element to an array. They're prefixed with a dollar sign. These operators make MongoDB queries expressive and powerful, allowing you to perform complex operations without pulling data into your application. Learning the common operators is essential for working efficiently with MongoDB - they replace what would be WHERE clauses and UPDATE logic in SQL."],
    },
    {
        text: "How do you handle migrations in MongoDB?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.mongodb, ValidTag.enum.migrations],
        answers: ["MongoDB migrations are trickier than SQL because there's no enforced schema. Schema changes often happen gradually as you update documents. Common approaches include running migration scripts that update existing documents to add or remove fields, using version fields on documents to track their schema version, or handling multiple schema versions in application code. Tools like migrate-mongo or custom scripts help manage this. I write idempotent migration scripts that check if a document needs updating before modifying it. For large collections, I process documents in batches to avoid overwhelming the database. Unlike SQL where you must migrate all data before changing the schema, MongoDB lets you migrate lazily - updating documents as they're accessed and handling both old and new formats in your code during the transition period."],
    },

    // Prisma
    {
        text: "What is Prisma and how does it differ from other ORMs?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Prisma is a next-generation ORM for Node.js and TypeScript. Unlike traditional ORMs, it uses a declarative schema file and generates a fully type-safe client based on your database schema. The big difference is that you get auto-completion and compile-time type checking for all your queries, which catches errors before runtime. Other ORMs like TypeORM or Sequelize use decorators or models that can get out of sync with the database. Prisma's approach is schema-first - you define your data model in the Prisma schema, and it handles both the database and the TypeScript types. It also has great migration tooling and a visual database browser called Prisma Studio. The developer experience is significantly better than traditional ORMs, especially in TypeScript projects."],
    },
    {
        text: "What is the Prisma schema and how do you define models?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["The Prisma schema is a file, usually schema.prisma, where you define your data models, database connection, and generator configuration. You define models using a simple syntax - each model represents a database table. For example, a User model might have id, email, and name fields with their types. You specify the database provider like PostgreSQL or MySQL, and the client generator. The schema is the single source of truth for your database structure. When you run prisma generate, it creates the Prisma Client with TypeScript types matching your schema. When you run prisma migrate, it generates SQL migrations based on schema changes. This centralized schema prevents drift between your code and database, which is a common problem with traditional ORMs."],
    },
    {
        text: "What is Prisma Client and how is it generated?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Prisma Client is an auto-generated, type-safe database client based on your Prisma schema. You generate it by running prisma generate, which reads your schema file and creates custom code tailored to your exact data model. Every table becomes a property on the client with methods like findMany, create, update, delete. The TypeScript types are generated automatically, so you get full auto-completion and type checking. When you change your schema, you regenerate the client and your types update. This is different from traditional ORMs where types are loosely coupled. The client handles query building, connection pooling, and translates your method calls into optimized SQL. It's designed to be intuitive while maintaining the full power of SQL underneath."],
    },
    {
        text: "How do you handle migrations in Prisma?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma, ValidTag.enum.migrations],
        answers: ["Prisma has two migration workflows: Prisma Migrate for development and production, and db push for prototyping. With Prisma Migrate, you modify your schema file, then run prisma migrate dev which generates a SQL migration file and applies it. The migration files are stored in your project and version controlled. For production, you run prisma migrate deploy which applies pending migrations. Prisma tracks which migrations have run using a migrations table. You can also create empty migrations for custom SQL. The workflow is smooth - change your schema, run migrate, and the database and your client types update together. This is much cleaner than writing raw SQL migrations or using migration libraries separately from your ORM."],
    },
    {
        text: "What are relations in Prisma and how do you query them?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Relations in Prisma define relationships between models using relation fields. You can have one-to-one, one-to-many, and many-to-many relationships. For example, a User can have many Posts, and each Post belongs to a User. You define this with relation fields and the relation decorator. To query relations, you use the include or select options. Include fetches related data, like including a user's posts when querying users. This generates an efficient JOIN or multiple queries. You can also use nested selects to precisely choose which fields to return. Prisma handles the SQL complexity - you just specify what data you want. The relations are fully type-safe, so you can't include a relationship that doesn't exist. It's much cleaner than manually joining tables."],
    },
    {
        text: "How do you handle raw SQL queries in Prisma?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma, ValidTag.enum.sql],
        answers: ["Prisma provides raw database access for when you need to drop down to SQL. You use prisma.dollar-queryRaw for queries that return data, or prisma.dollar-executeRaw for operations like updates or deletes. You can use template literals for parameterized queries to prevent SQL injection. The results are returned as plain JavaScript objects, though you can specify a type. I use raw queries for complex operations that are difficult to express with Prisma's query API, like certain aggregations or database-specific features. However, you lose type safety and Prisma's query optimization, so I try to use the standard API when possible. Raw SQL is an escape hatch for when you need it, not the primary way to interact with the database."],
    },
    {
        text: "What are Prisma middleware?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Prisma middleware are functions that run before and after database queries, letting you intercept and modify operations. You use prisma.dollar-use to register middleware. They receive the query parameters and the next function, similar to Express middleware. Common use cases include logging all queries, adding soft delete filters automatically, or implementing row-level security. For example, you might intercept all delete operations and convert them to updates that set a deletedAt field. Or you might log query performance. Middleware runs at the Prisma Client level, so it applies to all queries regardless of where in your application they're called. It's powerful for cross-cutting concerns that should apply to all database operations."],
    },
    {
        text: "How do you handle soft deletes in Prisma?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Soft deletes mark records as deleted without actually removing them from the database. In Prisma, you add a deletedAt nullable DateTime field to your model. Then you use middleware to intercept delete operations and convert them to updates that set deletedAt. You also intercept find operations to filter out deleted records automatically. This way, delete calls appear to remove the record, but it's actually just marked. You can create explicit methods to find deleted records when needed, like for admin views or recovery features. The advantage is you never lose data and can implement undelete functionality. The downside is your database grows larger and you need to be careful about unique constraints on soft-deleted records. It's a common pattern for applications that need audit trails or data recovery."],
    },
    {
        text: "What is the difference between findUnique, findFirst, and findMany?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["FindUnique retrieves a single record by a unique identifier like an ID or email. It requires searching on a field with a unique constraint and returns null if not found. FindFirst retrieves the first record matching a filter, useful when you expect one result but don't have a unique field. It also returns null if nothing matches. FindMany retrieves all records matching a filter and returns an array, which might be empty. Use findUnique when you're looking up by a unique field - it's the most efficient and type-safe. Use findFirst when you want one record but are filtering on non-unique fields, like finding the most recent order. Use findMany when you expect multiple results. The choice affects both performance and the return type."],
    },
    {
        text: "How do you handle transactions in Prisma?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma, ValidTag.enum.transactions],
        answers: ["Prisma supports transactions in two ways: sequential operations with dollar-transaction, and interactive transactions. Sequential transactions take an array of Prisma operations and execute them in order - if any fail, they all roll back. This is simple for straightforward multi-step operations. Interactive transactions provide a transaction client that you use within a callback, giving you control flow and the ability to make decisions based on query results. For example, you might read a user's balance, check if it's sufficient, then perform a transfer. The transaction rolls back automatically if an error is thrown. Prisma handles the complexity of starting, committing, and rolling back. Transactions ensure data integrity when multiple operations must succeed or fail together."],
    },
    {
        text: "What are nested writes in Prisma?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.prisma],
        answers: ["Nested writes let you create, update, or delete related records in a single operation. For example, when creating a user, you can create their profile at the same time using the create nested option. Or when updating a post, you can connect it to different tags. Prisma handles the complexity of multiple operations and foreign keys automatically. You can nest creates, updates, connects, disconnects, and deletes. This is powerful for managing complex data structures in one call instead of multiple sequential operations. Nested writes are executed in a transaction, so either all operations succeed or none do. They make your code cleaner and ensure data consistency. The type system guides you through what nested operations are valid for each relationship."],
    },

    // Supabase
    {
        text: "What is Supabase and how does it differ from Firebase?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase],
        answers: ["Supabase is an open-source Firebase alternative built on PostgreSQL. The key difference is that Supabase uses a real SQL database, giving you the full power of PostgreSQL with relations, complex queries, and triggers, while Firebase uses a NoSQL document database. Supabase provides authentication, real-time subscriptions, storage, and edge functions similar to Firebase, but everything is built on standard technologies rather than proprietary ones. You can self-host Supabase or use their hosted service. I prefer Supabase when I want SQL capabilities, need complex queries or relationships, or want to avoid vendor lock-in. Firebase might be better for rapid prototyping with less structured data or when you need Firebase's deeper Google Cloud integration. Supabase feels more like traditional backend development with modern tooling."],
    },
    {
        text: "What is Supabase Auth and how do you implement authentication?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase, ValidTag.enum.auth],
        answers: ["Supabase Auth is a complete authentication system built into Supabase. It supports email/password, magic links, OAuth providers like Google and GitHub, and phone authentication. You use the Supabase JavaScript client to call methods like signUp, signIn, and signOut. Supabase handles password hashing, session management, and JWT tokens automatically. The user's session is stored locally and the JWT is included in requests. On the backend, you can use Row Level Security policies that check the JWT to enforce data access rules. Implementing it is straightforward - initialize the client with your project URL and API key, then call the auth methods. The benefit is you get production-ready authentication without building it yourself, and it integrates seamlessly with database security."],
    },
    {
        text: "What are Row Level Security policies?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase, ValidTag.enum.rbac, ValidTag.enum.security],
        answers: ["Row Level Security, or RLS, is a PostgreSQL feature that Supabase leverages to secure your data at the database level. You define policies that determine which rows a user can select, insert, update, or delete based on their authentication status. For example, a policy might allow users to only see their own posts by checking if the user_id column matches their authenticated user ID. The policies run in the database using the JWT claims from the user's session. This means security is enforced even if someone bypasses your application code and queries the database directly. I define policies for each table specifying what authenticated users, anonymous users, or specific roles can do. It's more secure than application-level checks because it's impossible to bypass."],
    },
    {
        text: "How do you use Supabase real-time subscriptions?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase, ValidTag.enum["pub-sub"]],
        answers: ["Supabase real-time subscriptions let you listen for changes to your database in real-time. You use the client to subscribe to inserts, updates, or deletes on a table or even specific rows. When the data changes, your callback fires with the new data. For example, you might subscribe to new messages in a chat application. Supabase uses PostgreSQL's replication system and WebSockets to push changes to connected clients. You can filter subscriptions to only receive relevant changes. It's great for collaborative features, live dashboards, or chat applications. The subscription respects Row Level Security policies, so users only receive updates for data they have access to. It's much simpler than setting up your own pub-sub system and integrates naturally with database operations."],
    },
    {
        text: "What is Supabase Storage and how do you handle file uploads?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase],
        answers: ["Supabase Storage is an S3-compatible object storage service for files and media. You create buckets, which can be public or private, then upload files using the JavaScript client. For uploads, you call the upload method with the file and path. Supabase returns a URL you can use to access the file. You can set up RLS policies on storage buckets just like database tables, controlling who can upload, view, or delete files. For example, users might only be able to upload to their own folder. Supabase handles things like image transformations on the fly, so you can request different sizes of an image. It's useful for user avatars, document uploads, or any file storage needs. The integration with the rest of Supabase makes it seamless compared to managing S3 separately."],
    },
    {
        text: "How do you handle migrations in Supabase?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase, ValidTag.enum.migrations],
        answers: ["Supabase uses SQL migration files managed through the Supabase CLI. You create migrations with supabase migration new, which creates a timestamped SQL file. You write your schema changes as SQL in this file - creating tables, adding columns, defining RLS policies, etc. Running supabase db push applies pending migrations to your remote database. For local development, you can run a local Supabase instance and apply migrations there first. The migration system tracks which migrations have run using a schema_migrations table. Best practice is to make migrations reversible when possible and test them on a staging environment. Since it's just SQL, you have full control and can do anything PostgreSQL supports. The workflow integrates well with version control and CI/CD pipelines."],
    },
    {
        text: "What are the limitations of Supabase?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.supabase],
        answers: ["Supabase's main limitations stem from being built on PostgreSQL. It inherits PostgreSQL's scaling limitations - eventually you need to shard or move to a different architecture for truly massive scale. Real-time subscriptions can struggle with very high connection counts. The free tier has resource limits that are fine for development but not production. Edge functions are relatively new and lack some features of mature serverless platforms. Being newer than Firebase, the ecosystem and community are smaller. There's also the operational overhead if you self-host, though the managed offering mitigates this. For most applications these aren't dealbreakers, but you should be aware of them. The limitations are mainly around extreme scale and the maturity of some features, not fundamental architectural issues."],
    },

    // Redis
    {
        text: "What is Redis and what are its primary use cases?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.caching],
        answers: ["Redis is an in-memory data structure store that's extremely fast because it keeps data in RAM. The primary use cases are caching to speed up database queries, session storage for user sessions across servers, pub/sub messaging for real-time features, rate limiting to control API usage, and as a message broker for queues. It's also great for leaderboards, real-time analytics, and temporary data that needs fast access. I use Redis whenever I need sub-millisecond response times and can tolerate potential data loss since it's in-memory. For example, caching expensive database queries or API responses. The key is understanding what belongs in Redis versus a persistent database - Redis is for temporary, fast-access data that enhances performance but isn't the source of truth."],
    },
    {
        text: "What data structures does Redis support?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis],
        answers: ["Redis supports strings for simple key-value pairs, hashes for objects with multiple fields, lists for ordered collections that you can push and pop from, sets for unordered unique collections, and sorted sets for ordered unique collections with scores. There are also more advanced types like bitmaps, hyperloglogs for cardinality estimation, and geospatial indexes. Each structure has specific commands optimized for it. For example, sorted sets are perfect for leaderboards because you can add scores and retrieve top players efficiently. Lists work well for message queues. Understanding which structure to use for your data is key to using Redis effectively. The rich data structures make Redis much more powerful than a simple key-value cache."],
    },
    {
        text: "What is Redis pub/sub and when would you use it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum["pub-sub"]],
        answers: ["Redis pub/sub is a messaging pattern where publishers send messages to channels and subscribers receive messages from channels they're subscribed to. It's fire-and-forget - if no one is subscribed, the message is lost. I use it for real-time notifications, chat applications, or broadcasting updates to connected clients. For example, when a user posts a comment, you publish to a channel and all subscribed clients receive it instantly. The limitation is messages aren't persisted, so if a subscriber is offline, they miss messages. For more reliable messaging, I'd use Redis Streams or a dedicated message queue. But for ephemeral real-time updates where missing a message occasionally is acceptable, pub/sub is simple and performant."],
    },
    {
        text: "How do you implement caching with Redis?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.caching],
        answers: ["The basic pattern is to check Redis before querying the database. If the data is in Redis, return it. If not, query the database, store the result in Redis with an expiration time, then return it. This is cache-aside or lazy loading. You set a TTL appropriate for your data - maybe 5 minutes for frequently changing data or an hour for more stable data. For writes, you can update or delete the cache entry to keep it fresh. I use Redis for caching expensive queries, API responses, or computed values. The key is choosing what to cache - high-read, low-write data benefits most. You also need a strategy for cache invalidation when the underlying data changes. Redis's speed makes it ideal for caching, dramatically reducing database load."],
    },
    {
        text: "What are Redis transactions?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.transactions],
        answers: ["Redis transactions let you execute multiple commands atomically using MULTI, EXEC, and optionally WATCH. You start with MULTI, queue up commands, then EXEC executes them all at once. However, Redis transactions are different from database transactions - they don't support rollback. If one command fails, the others still execute. WATCH lets you implement optimistic locking by watching keys and aborting the transaction if they change. I use transactions when I need multiple operations to execute atomically, like incrementing a counter and adding to a list together. For more complex transactional needs, I'd use Lua scripts which guarantee atomicity. Redis transactions are lighter weight than traditional database transactions and focused on atomicity rather than isolation."],
    },
    {
        text: "What is Redis Cluster and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.scalability],
        answers: ["Redis Cluster is Redis's native sharding solution that distributes data across multiple Redis nodes. It uses hash slots - there are 16,384 slots and each key is hashed to a slot. Each master node owns a range of slots. When you set a key, Redis determines which slot and therefore which node it belongs to. Clients need to be cluster-aware to route commands to the correct node. Cluster also provides automatic failover - if a master fails, one of its replicas is promoted. I'd use Redis Cluster when a single Redis instance can't hold all your data or handle the load. The tradeoff is increased operational complexity and some limitations - you can't do multi-key operations that span nodes. For most use cases, a single Redis with replicas is sufficient."],
    },
    {
        text: "How do you handle cache invalidation with Redis?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum["cache-invalidation"]],
        answers: ["Cache invalidation is one of the hardest problems in computing. Common strategies include time-based expiration with TTLs, where data expires after a set time. Event-based invalidation deletes or updates cache entries when the underlying data changes. You can also use versioned keys, where the cache key includes a version that changes when data updates. For complex dependencies, tag-based invalidation groups related cache entries. I typically combine TTLs with event-based invalidation - set a reasonable TTL as a safety net, but actively invalidate when data changes. For example, when updating a user, delete their cache entry. The challenge is handling relationships - when you update a post, you might need to invalidate user feeds. The strategy depends on your consistency requirements and update patterns."],
    },
    {
        text: "What is Redis persistence and what are the options (RDB vs AOF)?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis],
        answers: ["Redis persistence saves in-memory data to disk so it survives restarts. RDB creates point-in-time snapshots at intervals - it's compact and fast for backups but you can lose data between snapshots if Redis crashes. AOF logs every write operation, so you can replay them to rebuild state. It's more durable but creates larger files and is slower. You can use both together - RDB for backups and faster restarts, AOF for durability. For pure caching, I often disable persistence since cached data can be regenerated. For sessions or other important data, I use AOF with fsync every second, balancing durability and performance. Understanding the tradeoff between durability and performance is key - more frequent persistence is safer but slower."],
    },
    {
        text: "How do you handle session storage with Redis?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum["session-management"]],
        answers: ["Redis is perfect for session storage in distributed applications. You store session data as a hash or JSON string with the session ID as the key. Set a TTL matching your session timeout, and Redis automatically cleans up expired sessions. When a request comes in, look up the session by ID, extend the TTL to keep active sessions alive, and update it as needed. This allows any server to access any user's session, enabling stateless application servers and easy horizontal scaling. I use Redis hashes to store session fields efficiently, and set TTLs to automatically expire inactive sessions. It's much better than in-memory sessions on individual servers because it scales horizontally and survives server restarts. The fast access time keeps requests snappy."],
    },
    {
        text: "What are Redis TTL and expiration strategies?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.caching],
        answers: ["TTL, or Time To Live, specifies how long a key should exist before Redis automatically deletes it. You set it with EXPIRE or as part of SET. Redis uses two strategies: passive expiration checks TTL when you access a key, and active expiration periodically scans for and deletes expired keys. This means expired keys might not be deleted immediately if they're not accessed. For caching, I set TTLs based on how fresh data needs to be - maybe 5 minutes for dynamic data or a day for static data. For sessions, TTL matches the session timeout. You can also use sliding expiration by updating the TTL on each access. Understanding TTL is crucial for managing memory and keeping cached data reasonably fresh without manual cleanup."],
    },

    // Caching
    {
        text: "What are cache invalidation strategies and what are the tradeoffs?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching, ValidTag.enum["cache-invalidation"]],
        answers: ["The main strategies are TTL-based, where data expires after a set time, event-based where you invalidate when the source data changes, and version-based where you change the cache key when data updates. TTL is simple but data can be stale, and you might cache miss on popular items. Event-based keeps data fresh but requires complex logic to track what to invalidate and can fail if events are missed. Version-based avoids invalidation by making old cache entries irrelevant, but wastes memory on old versions. In practice, I combine approaches - use TTL as a safety net, invalidate on events when possible, and version static assets. The tradeoff is always between consistency, complexity, and cache hit rate. Perfect consistency requires aggressive invalidation, hurting performance."],
    },
    {
        text: "What is the difference between write-through, write-behind, and write-around caching?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching, ValidTag.enum["cache-strategies"]],
        answers: ["Write-through writes to the cache and database simultaneously, keeping them in sync but slowing writes. Write-behind writes to cache immediately and asynchronously writes to the database later, making writes fast but risking data loss if the cache fails. Write-around writes directly to the database and bypasses the cache, only caching on reads. Write-through guarantees consistency but adds latency to writes. Write-behind is fastest but complex and risky. Write-around is simple and works when written data isn't immediately re-read. I use write-through when consistency is critical and writes are infrequent, write-behind for high write throughput when some data loss is acceptable, and write-around for write-heavy workloads where reads are rare. The choice depends on your read-write patterns and consistency requirements."],
    },
    {
        text: "What is cache stampede and how do you prevent it?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching],
        answers: ["Cache stampede, or thundering herd, happens when a popular cache entry expires and many requests simultaneously try to regenerate it, overwhelming your database. Imagine a homepage cache expires and 1000 concurrent users all hit the database at once. To prevent it, you can use locking so only one request regenerates the cache while others wait. Another approach is probabilistic early expiration, where you refresh the cache before it expires based on load. You can also serve stale data while refreshing in the background. I typically use a combination - implement a lock so only one request regenerates, and use stale-while-revalidate so users get slightly old data instantly while the cache updates. The key is preventing simultaneous expensive operations when cache misses occur."],
    },
    {
        text: "What is the difference between in-memory cache and distributed cache?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching],
        answers: ["In-memory cache stores data in the application server's RAM, making it extremely fast but limited to that server. Each server has its own cache, so data isn't shared. Distributed cache like Redis is a separate service that multiple servers share, providing a consistent cache across your application. In-memory is fastest because there's no network hop, but you lose it when the server restarts and it doesn't help with horizontal scaling since each server caches independently. Distributed cache is slightly slower due to network latency but survives restarts, scales independently, and provides a single source of cached data. I use in-memory for request-scoped caching or data that's okay to be server-specific, and distributed cache for session data or anything that needs to be consistent across servers."],
    },
    {
        text: "When would you use Redis vs Memcached?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.redis, ValidTag.enum.caching],
        answers: ["I'd use Redis over Memcached in most cases because Redis supports richer data structures, persistence, pub/sub, and has more features. Redis can handle lists, sets, and sorted sets, while Memcached is just key-value. Memcached might be slightly faster for pure simple caching due to its simplicity and multi-threading, but the difference is usually negligible. Redis's persistence means it can survive restarts, which is valuable for sessions or important cached data. The only time I'd specifically choose Memcached is if I need the absolute simplest caching solution or want multi-threaded performance for a very high-throughput cache-only use case. For almost every modern application, Redis's extra features make it the better choice without meaningful performance sacrifice."],
    },
    {
        text: "What are caching headers and how do they work?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching, ValidTag.enum["cache-control"]],
        answers: ["HTTP caching headers control how browsers and CDNs cache responses. Cache-Control specifies directives like max-age for how long to cache, no-cache to always revalidate, or private for browser-only caching. ETag provides a version identifier so clients can ask if their cached version is still valid. Expires sets an absolute expiration time, though Cache-Control is preferred. Last-Modified lets clients do conditional requests. For example, Cache-Control: max-age=3600 tells the browser to cache for an hour. Static assets might have max-age of a year, while API responses might be no-cache. Proper caching headers dramatically reduce server load and improve performance. I set aggressive caching for static assets with versioned URLs, and more conservative caching for dynamic content."],
    },
    {
        text: "What is stale-while-revalidate?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching],
        answers: ["Stale-while-revalidate is a caching strategy where you serve cached content even after it's expired, while simultaneously fetching fresh data in the background. It's specified in the Cache-Control header. For example, max-age=60, stale-while-revalidate=300 means serve from cache for 60 seconds, then serve stale content for up to 300 more seconds while revalidating. This ensures users always get instant responses with reasonably fresh data. The next request gets the updated cache. It's a great middle ground between performance and freshness. I use it for content that changes occasionally but where serving slightly stale data is acceptable, like blog posts or product listings. It prevents the latency spike when cache expires while ensuring data stays relatively current."],
    },
    {
        text: "How do you decide what to cache and for how long?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.caching],
        answers: ["I look at several factors: how expensive is the operation to perform, how frequently is the data accessed, how quickly does it change, and what's the impact of serving stale data. Cache expensive operations with high read frequency and low change rate. For example, a homepage that's expensive to render but only updates hourly is perfect for caching with a 30-minute TTL. User-specific data might not cache well if every user's data is unique. Static assets cache forever with versioned URLs. API responses might cache for seconds to minutes depending on freshness requirements. I use monitoring to identify slow queries that are called frequently - those are prime caching candidates. The TTL should be short enough that staleness is acceptable but long enough to meaningfully reduce load."],
    },

    // CDN
    {
        text: "What is a CDN and how does it work?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn],
        answers: ["A CDN, or Content Delivery Network, is a distributed network of servers that cache and serve your content from locations close to users. When a user requests a file, the CDN serves it from the nearest edge server rather than your origin server. This reduces latency because data travels shorter distances, and reduces load on your origin. The CDN caches responses based on caching headers you set. The first user in a region hits the origin and the CDN caches the response, then subsequent users in that region get it from cache. CDNs are essential for global applications because they make your site fast worldwide regardless of where your origin server is located. I use CDNs for static assets, images, and sometimes API responses to improve performance globally."],
    },
    {
        text: "What is the difference between a push CDN and a pull CDN?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn],
        answers: ["Push CDNs require you to upload content to the CDN servers. You're responsible for keeping them updated. Pull CDNs automatically fetch content from your origin server when requested and cache it. With push, you have control over exactly what's on the CDN but need to manage uploads and updates. With pull, it's automatic but the first request to each edge location is slower because it fetches from origin. Pull is more common and easier to use - you just point the CDN at your origin and it handles everything. I'd use push for large files that rarely change, like video content, where you want to preload the CDN. For most web applications, pull CDNs are simpler and work better."],
    },
    {
        text: "How do you handle cache invalidation with a CDN?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn, ValidTag.enum["cache-invalidation"]],
        answers: ["CDN cache invalidation is tricky because content is distributed across many edge servers. The main approaches are cache purging, where you tell the CDN to delete specific content, and versioned URLs, where you change the URL when content updates so the old cached version becomes irrelevant. Most CDNs provide purge APIs to clear cache by URL or tag. However, purging takes time to propagate and costs money on some CDNs. Versioned URLs with query strings or filenames like app.v2.js are more reliable - the CDN caches indefinitely and you just change the URL in your HTML. I use versioned URLs for static assets and purge APIs for content like HTML pages. The best approach depends on your use case and how frequently content changes."],
    },
    {
        text: "What are edge locations?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn],
        answers: ["Edge locations are the physical data centers around the world where a CDN stores cached content. They're strategically placed in major cities and internet exchange points to be as close to end users as possible. When a user requests content, it's served from the nearest edge location rather than traveling all the way to your origin server. For example, Cloudflare has edge locations in hundreds of cities globally. More edge locations means better global performance because more users are close to a cache. The edge location caches content based on your caching rules and TTLs. Edge locations are the foundation of how CDNs reduce latency - by bringing content physically closer to users worldwide."],
    },
    {
        text: "How do you configure CDN caching rules?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn, ValidTag.enum.caching],
        answers: ["CDN caching is configured through HTTP headers from your origin and sometimes CDN-specific rules. The Cache-Control header is primary - it tells the CDN how long to cache and under what conditions. You might set max-age=31536000 for static assets to cache for a year, or no-cache for dynamic content. Many CDNs also let you override or supplement these headers with rules in their dashboard. You can configure cache behavior by path, like caching everything under /static/ for a long time. You might cache based on query strings or cookies. I typically set long cache times for versioned static assets, moderate times for images and fonts, and short or no caching for HTML and API responses. Testing and monitoring cache hit rates helps optimize the rules."],
    },
    {
        text: "What are the tradeoffs between different CDN providers?",
        level: Level.enum.mid,
        category: Category.enum.databases,
        tags: [ValidTag.enum.cdn],
        answers: ["The main factors are number and location of edge servers, pricing models, features, and reliability. Cloudflare has the most edge locations and is very affordable, with a generous free tier. AWS CloudFront integrates well with AWS services but is more expensive. Fastly is developer-friendly with instant purging and flexible configuration. Akamai is enterprise-focused and expensive but extremely reliable. Some CDNs charge for bandwidth, others for requests. Features like image optimization, edge computing, and DDoS protection vary. I'd choose Cloudflare for most projects due to its balance of features, performance, and price. CloudFront if you're heavily invested in AWS. Fastly for advanced edge computing needs. The choice depends on your budget, existing infrastructure, geographic user distribution, and specific feature requirements."],
    },
];
